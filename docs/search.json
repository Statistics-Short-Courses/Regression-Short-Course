[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Linear Regression in R",
    "section": "",
    "text": "Preface\nThis notebook provides an introduction (or refresher) to linear regression, a fundamental tool for modeling relationships between variables. We’ll explore the concepts, fit models in R, interpret results, and visualize fits.\nBy the end, you should be able to:\n\nUnderstand the linear regression model and its assumptions.\nFit and interpret simple and multiple regression models in R.\nDiagnose model fit using residual plots and summary statistics.\nUse model output to make predictions and assess uncertainty.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "LinearModels.html",
    "href": "LinearModels.html",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "1.1 A simple linear model\nHere \\(\\varepsilon\\) represents the random variation in \\(Y\\) that is not explained by \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#a-simple-linear-model",
    "href": "LinearModels.html#a-simple-linear-model",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "Linear prediction\n\nWe now choose \\(f(X)\\) to be a linear function:\n\n\\[\nf(X)= \\beta_0 + \\beta_1\\cdot X\n\\]\n\nPutting aside the random error term for the moment, we can think of the expected value of \\(Y\\), denoted \\(E[Y]\\), as being given by this linear relationship:\n\n\\[\nE[Y]= \\beta_0 + \\beta_1\\cdot X\n\\tag{1.2}\\]\n\nIn other words, we are representing the expected value of \\(Y\\) as a straight line with y-intercept \\(\\beta_0\\) and slope \\(\\beta_1\\):\n\n\n\nviewof b1 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β₁)\"})\nviewof b0 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (β₀)\"})\ntex.block`E[Y] = ${b1}X + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = [-10,10]\nlineData = xRange.map(x =&gt; ({x, y: (b1 * x) + b0}))\nPlot.plot({\n  x:{domain: [-10,10], label: \"X\", grid: true},\n  y:{domain: [-10,10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(lineData, { x: \"x\", y: \"y\" })]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\nFigure 1.1: Plot of the linear function E[Y] = β₀+ β₁X\n\n\n\n\n\n\n\n\nHere, \\(\\beta_0\\) represents the value of \\(Y\\) when \\(X=0\\), and \\(\\beta_1\\) represents the change in Y for a one-unit increase in X.\n\n\nAssumption 2Y and X have a linear relationship\n\n\n\nExample 1: Salary growth over timeYou’ve been offered a job at ~company A~, and want to predict your salary at this company after working there for 10 years. We know that, on average, the starting salary at this company is $50,000, and salaries increase by $5000 per year of employment.\nUsing this information, we can construct a simple linear predictor of salary from employment time. Given an employment time of \\(X=x\\) years, we can represent a employee’s expected salary (\\(E[Y]\\)) as\n\\[\nE[Y]= \\$50,000 + \\$5,000\\cdot x.\n\\]\nAfter 10 years of working at this company (i.e. \\(X=10\\)), we would expect a salary of \\[E[Y]=\\$50,000 + \\$5,000\\times10= \\$100,000\\]\n\n\n\nYour turn\nA second company also has a job available. The starting salary is higher here - $70,000 on average - but payrises are smaller. You are told that, on average, employees working for the company for 5 years earn $20,000 more per year than when they started.\nYou want to use a simple linear prediction to calculate your expected salary (\\(E[Y]\\)) after \\(X\\) years of employment.\n\\[\nE[Y] = \\beta_0 + \\beta_1 X\n\\]\n\nWhat are the coefficients \\(\\beta_0\\) and \\(beta_1\\) in this case?\n\n\\(\\beta_0 =\\) \\(\\beta_1 =\\)\nUsing R, calculate the expected salary after working for this company for 10 years\n\n\n\n\n\n\n\n\n\n\n\n\nComplete the r function that predicts \\(Y\\) from an input \\(X\\) for this linear model (alternatively - give function e.g. ‘this function predicts salary at company 2 with an input X years work’)\n\n\n\n\n\n\n\n\n\n\n\n\nUse this function to predict \\(Y\\) for the following values of \\(X\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nToDoadd plot comparing two (companies) linear salary growth?\n\n\n\n\n\nRandom Errors\n\nIn practice, data rarely fall perfectly on a straight line. Even if the underlying relationship between \\(X\\) and \\(Y\\) is approximately linear, individual observations tend to vary around that line.\nAs mentioned in Section 1.x, to account for this variation we add an error term, \\(\\varepsilon\\), to our model\n\n\\[\nY=\\beta_0 + \\beta_1X + \\varepsilon\n\\]\n\nMean Zero\n\nThe error term represents the *difference between the actual value of \\(Y\\) and the value predicted by the linear predictor, \\(E[Y]\\).\nOn average, we expect these error terms to balance out:\n\n\nAssumption 3The mean of the error term is zero:\n\\[\nE[\\varepsilon]=\\mu=0\n\\]\ni.e. The linear predictor gives the correct value of \\(Y\\) on average\n\n\n\n\nConstant variance\n\nWhile correct on average, we expect there to be some spread of data around the line (this is why we have the error term). The amount of spread is measured by the variance of the errors.\nWe assume that this variance is constant - like \\(mu=0\\), it is the same for all values of \\(X\\) however we dont specify which particular value it takes:\n\n\nAssumption 4The variance of the error term is constant for all values of X: \\[Var(\\varepsilon)=\\sigma^2\\]\n\n\n\n\nNormal distribution\nWhile the assumptions of mean 0 and constant variance describe the center and spread of the errors, they don’t fully specify the shape of their distribution. To model this more completely, we often assume that the errors follow a Normal distribution.\n\nAssumption 5The error is normally distributed (with mean \\(\\mu=0\\) and variance \\(\\sigma^2\\) . \\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\n\n\nviewof mu = Inputs.range([-5, 5], {\n  value: 0,\n  step: 0.1,\n  label: `Mean (μ):`\n})\n\nviewof sigma = Inputs.range([0.2, 5], {\n  value: 1,\n  step: 0.1,\n  label: 'Standard deviation (σ):'\n})\n\nSQRT2PI=Math.sqrt(2 * Math.PI)\n\nnormalDensity = (x, mean, sd) =&gt;\n  (1 / (sd * SQRT2PI)) * Math.exp(-0.5 * ((x - mean) / sd) ** 2);\n  \ndensityGrid_1 = d3.range((-4 * sigma)+mu, mu+(4 * sigma), sigma / 50).map(x =&gt; ({\n  x,\n  density: normalDensity(x,mu,sigma)\n}));\n\ntex.block`\\varepsilon \\sim \\text{Normal}(${mu}, ${sigma}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 280,\n  marginLeft: 48,\n  marginBottom: 40,\n  y: { label: \"Density\" },\n  x: {domain: [-10,10], label: \"ε\" },\n  marks: [\n    Plot.areaY(densityGrid_1, {\n      x: \"x\",\n      y: \"density\",\n      fillOpacity: 0.2,\n      stroke: \"#2a5599\",\n      fill: \"#2a5599\"\n    })\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1.2: Normal distribution with adjustable mean and standard deviation.\n\n\n\n\n\n\n\n\nTechnical-Point 1The Central Limit Theorem is a fundamental result of probability theory that provides some extra motivation for this assumption. According to this theorem, sums or averages of many small, independent random effects tend to follow a Normal distribution. Thus, assuming normally distributed errors is both a practical simplification and a reasonable approximation in many situations.\n\n\n\n\n\nThe Simple Linear Model\nPutting these pieces together we are left with:\n\\[\nY=\\beta_0+\\beta_1X+\\varepsilon, \\quad{\\varepsilon \\sim N(0,\\sigma^2)}\n\\]\nThis ‘simple linear model’ is the starting place for conducting linear regression - in which whe ‘fit’ (i.e. estimate the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\)) from data.\n\n\nviewof b1_2 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β₁)\"})\nviewof b0_2 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (β₀)\"})\nviewof sigma_2 = Inputs.range([0.2, 5], {step: 0.1, value: 2.5, label: \"Std. deviation (σ)\"})\n\nviewof n_cs = Inputs.range([10, 50], {step: 1, value: 50, label: \"Number of cross sections visualised\"})\n\ntex.block`Y = ${b0_2} + ${b1_2}X + \\varepsilon, \\quad \\varepsilon \\sim \\text{Normal}(0, ${sigma_2}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxMin = -10;\nxMax = 10;\nstep = (xMax - xMin) / (n_cs - 1);\nxSampleValues = d3.range(xMin, xMax + step/2, step);  \n\nySectionValues = d3.range(-10, 10.001, 0.1)\nwidthScale = Math.min(1.8, sigma_2 * 0.9)\n\ndensityCurveData = xSampleValues.flatMap(xVal =&gt; {\n  const mu = b0_2 + b1_2 * xVal;\n  const peakDensity = normalDensity(mu, mu, sigma_2);\n\n  const rightSide = ySectionValues.map(y =&gt; {\n    const density = normalDensity(y, mu, sigma_2);\n    const width = (density / peakDensity) * widthScale;\n    return {x: xVal + width, y, group: xVal};\n  });\n\n  return rightSide\n});\n\ncrossSectionTrendLine = xSampleValues.map(x =&gt; ({\n  x,\n  y: b0_2 + b1_2 * x\n}))\n\nPlot.plot({\n  x: {domain: [-10, 10], label: \"X\", grid: true},\n  y: {domain: [-10, 10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(densityCurveData, {\n      x: \"x\",\n      y: \"y\",\n      z: \"group\",\n      stroke: \"#2a5599\",\n      strokeWidth: 1.5,\n      curve: \"basis\"\n    }),\n    Plot.line(crossSectionTrendLine, {x: \"x\", y: \"y\", stroke: \"black\", strokeWidth: 2})\n  ]\n});\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\nFigure 1.3: Cross-sections of the simple linear model normal error density.\n\n\n\n\n\n\n\n\nExample 2\n\nSame model as above - given standard deviation\nExample Y values\n\nIllustrate error with normal overay\n\nExcersises:\n\nwhich variance is larger?\n(hard) probability of finding E[Y]+2sd observation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html",
    "href": "FittingSLR.html",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "",
    "text": "2.1 Estimating parameters\nin the previous section, you were given a linear model - we knew the values of \\(\\beta_0\\), \\(\\beta_1\\) and even \\(\\sigma\\). However, in most contexts we don’t know the true relationship between \\(X\\) and \\(Y\\) in advance. Instead, we are given data and try to infer the values of \\(\\beta\\), \\(\\sigma\\) based on this.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#estimating-parameters",
    "href": "FittingSLR.html#estimating-parameters",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "",
    "text": "Residuals\n\n\nEstimating \\(\\beta\\)\nWhich line fits the data best (how low can the RMSE go)?\n\nx = Array.from({length: 50}, d3.randomNormal(180, 50))\nnoise = Array.from({length: 50}, d3.randomNormal(0, 15))\ny = x.map((xi, i) =&gt; 0.5 * xi - 10 + noise[i])\n\nlinearData = x.map((xi, i) =&gt; ({ x: xi, y: y[i] }))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof b0adj = Inputs.range([-50, 50], {step: 1, label: html`${tex`\\beta_0`}: Intercept (adjustment)`})\nviewof b1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\\beta_1`}: Slope`})\n\nb0 = (90 - b1 * 180) + b0adj\n\nresidualData = linearData.map(d =&gt; {\n  const yhat = b1 * d.x + b0;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n\nRMSR = Math.sqrt(d3.sum(residualData, d =&gt; d.residual ** 2) / residualData.length)\n\ntex.block`y = ${b1}x + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\sqrt{\\frac{1}{n}\\sum_{i}r_i^{2}} = ${RMSR.toFixed(2)}`\n\n\n\n\n\n\n\n\n\n\n\nxRange = d3.extent(residualData,d =&gt; d.x)\nlineData = xRange.map(x =&gt; ({x, y: b1 * x + b0}))\n\nPlot.plot({\n  marks: [\n    Plot.ruleX(residualData, {x: \"x\",\n      y1: \"y\",\n      y2: \"yhat\",\n      stroke: \"sign\", \n      strokeOpacity: 0.75,\n      strokeWidth: d =&gt; 1 + d.absRes / 12,\n       title: d =&gt; `x=${d.x.toFixed(1)}\ny=${d.y.toFixed(1)}\nŷ=${d.yhat.toFixed(1)}\nres=${d.residual.toFixed(1)}\nres²=${(d.residual**2).toFixed(1)}`\n    }),\n\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(linearData, { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [0, 180], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a model using least squares - lm()\n\n\n\n\n\n\n\n\n\n\nEstimating \\(\\sigma\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#checking-assumptions",
    "href": "FittingSLR.html#checking-assumptions",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.2 Checking assumptions",
    "text": "2.2 Checking assumptions\n\nIndependence of observations\n\n\nLinear relationships\n\n\nConstant Variance\n\n\nNormality of error",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#inference-for-simple-linear-regression",
    "href": "FittingSLR.html#inference-for-simple-linear-regression",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.3 Inference for Simple Linear Regression",
    "text": "2.3 Inference for Simple Linear Regression\n\nInference about \\(\\sigma\\)\n\n\nInference about \\(\\beta_1\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#interpreting-summary-output",
    "href": "FittingSLR.html#interpreting-summary-output",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.4 Interpreting `summary()` output",
    "text": "2.4 Interpreting `summary()` output",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html",
    "href": "MultipleLinearRegression.html",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "",
    "text": "3.1 Continuous predictors",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#continuous-predictors",
    "href": "MultipleLinearRegression.html#continuous-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "",
    "text": "Multi-collinearity",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#categorical-predictors",
    "href": "MultipleLinearRegression.html#categorical-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "3.2 Categorical predictors",
    "text": "3.2 Categorical predictors",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#multiplying-predictors",
    "href": "MultipleLinearRegression.html#multiplying-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "3.3 Multiplying predictors",
    "text": "3.3 Multiplying predictors\n\nPolynomials\n\n\nInteractions\n\n\nTransformation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#using-the-model-for-prediction",
    "href": "FittingSLR.html#using-the-model-for-prediction",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.5 Using the model for prediction",
    "text": "2.5 Using the model for prediction",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#a-simple-linear-model-in-r",
    "href": "LinearModels.html#a-simple-linear-model-in-r",
    "title": "1  An introduction to linear statistical models",
    "section": "1.2 A simple Linear model in R",
    "text": "1.2 A simple Linear model in R\n\nfinish this section and lead into the next on fitting models to data.\nsimulate observations of Y from a specified linear model\nto begin, we start with a collection of X values (we might imagine we measure these values in the wild)\n\n\nX &lt;- runif(n=100, min=0, max= 50)\nX\n\n  [1] 10.8095758  9.4429772 22.8368842 20.1335687 15.7408187 47.7205195\n  [7] 19.4758789 24.4519084 30.3749922 14.5808976 40.0109772 18.2322900\n [13] 10.8093794  7.9702449 28.6491688 45.7752697  8.6055770 21.3150800\n [19] 27.2933333 47.0797844 23.6855624 16.2801493 16.8929851 15.0188315\n [25] 49.3257711 40.3750170  4.3534425 44.6472789 48.4316954 36.3375948\n [31] 42.9140004 28.3916193 32.7403006 28.5875266 48.3963899 31.2961646\n [37] 47.4875281 20.2840084 19.7501307 48.5627789 39.2492616  6.9694703\n [43] 28.4480483 24.3724199 49.8566165 11.0818907 30.0649226 37.3768026\n [49]  1.9746579 42.2456796 45.1044191 21.7206590 42.1237081  1.7255316\n [55] 10.1791292 20.2942165 39.4231758  7.7464026 37.8042037 42.4738021\n [61] 35.3768982 22.1299142 28.2307153 25.2550817 19.0784173 28.2630602\n [67] 10.3164125 44.3536958 45.5904823 40.1858715  9.5508725 41.7318709\n [73] 46.5106486 19.6842758 29.5306980 29.0790480 38.1883459 46.4230786\n [79] 10.2133694 47.5929748  6.1493425 12.7160308 33.3136434  1.5961243\n [85] 32.7359470 21.5623723 21.7237119 12.7778843 10.9249671 25.9317875\n [91] 39.8771390 25.5107344 20.5180015 24.5461606  0.5650067 21.0628963\n [97]  6.2072086 46.3892038 36.9192024 11.0116267\n\n\n\nThis code randomly chooses n = 100 numbers between (and including) 1 and 50 with equal probibility.\n\nNext, we construct our linear model\n\nSimple_linear_model &lt;- function(X){4 + 1.2*X + rnorm(length(X), mean=0, sd= 4)}\n\n\nThis is function takes X values and returns the specified linear function with normally distributed random noise added.\nNow we can simulate observations of \\(Y\\) given our list of \\(X\\) values and out linear model:\n\n\nY &lt;- Simple_linear_model(X)\nY\n\n  [1] 13.713127  9.774769 30.867817 23.524903 19.337835 65.795903 24.714123\n  [8] 35.982579 36.739388 31.651285 53.091205 26.149585 13.593687 10.252611\n [15] 31.966067 54.900441 13.222817 34.399584 38.878280 62.511408 34.669219\n [22] 21.925963 25.087426 23.460427 64.754146 61.744572 12.353266 49.293703\n [29] 59.660814 41.743172 56.320522 35.862517 37.992209 32.106435 63.486442\n [36] 43.722117 63.028589 31.621387 25.408459 64.868319 53.253000 10.155191\n [43] 44.303647 34.633114 62.855034 11.731489 39.040400 42.385560  6.031488\n [50] 51.096933 54.168226 31.625266 52.540671  2.954059 12.422937 32.503742\n [57] 53.788948 13.113428 42.500861 60.086823 41.971340 22.508818 30.738297\n [64] 34.491442 27.778987 35.616865 12.611930 63.336623 61.803003 52.944817\n [71] 14.816893 55.612758 56.348106 23.299519 46.112228 42.905210 55.480206\n [78] 61.188130 17.488105 57.268908 18.621361 23.623832 38.434159  1.654610\n [85] 39.371639 33.325153 25.662269 22.312920 22.669396 40.026773 54.027193\n [92] 34.285229 22.523932 37.347428  3.080413 23.132694  9.573746 65.967575\n [99] 46.566438 17.863087\n\n\nlets look at the joint distribution of X and Y:\n\nggplot(cbind(X,Y), aes(X,Y))+\n  geom_point()\n\n\n\n\n\n\n\nFigure 1.4: Data generated from the simple linear model \\(Y=4+1.2\\times X + \\varepsilon\\), with \\(\\varepsilon\\sim N(0,16)\\)\n\n\n\n\n\nHeres the same code running in webR - try adjusting some of the parameters (e.g. the number of samples) and running to plot a different random sample!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  }
]