[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Linear Regression in R",
    "section": "",
    "text": "Preface\nThis notebook provides an introduction (or refresher) to linear regression, a fundamental tool for modeling relationships between variables. We’ll explore the concepts, fit models in R, interpret results, and visualize fits.\nBy the end, you should be able to:\n\nUnderstand the linear regression model and its assumptions.\nFit and interpret simple and multiple regression models in R.\nDiagnose model fit using residual plots and summary statistics.\nUse model output to make predictions and assess uncertainty.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "LinearModels.html",
    "href": "LinearModels.html",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "1.1 Linear prediction\nHere \\(\\varepsilon\\) represents the random variation in \\(Y\\) that is not explained by \\(X\\).\n\\[\nf(X)= \\beta_0 + \\beta_1\\cdot X\n\\]\n\\[\nE[Y]= \\beta_0 + \\beta_1\\cdot X\n\\tag{1.2}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#a-simple-linear-model",
    "href": "LinearModels.html#a-simple-linear-model",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "Linear prediction\n\nWe now choose \\(f(X)\\) to be a linear function:\n\n\\[\nf(X)= \\beta_0 + \\beta_1\\cdot X\n\\]\n\nPutting aside the random error term for the moment, we can think of the expected value of \\(Y\\), denoted \\(E[Y]\\), as being given by this linear relationship:\n\n\\[\nE[Y]= \\beta_0 + \\beta_1\\cdot X\n\\tag{1.2}\\]\n\nIn other words, we are representing the expected value of \\(Y\\) as a straight line with y-intercept \\(\\beta_0\\) and slope \\(\\beta_1\\):\n\n\n\nviewof b1 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β₁)\"})\nviewof b0 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (β₀)\"})\ntex.block`E[Y] = ${b1}X + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = [-10,10]\nlineData = xRange.map(x =&gt; ({x, y: (b1 * x) + b0}))\nPlot.plot({\n  x:{domain: [-10,10], label: \"X\", grid: true},\n  y:{domain: [-10,10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(lineData, { x: \"x\", y: \"y\" })]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\nFigure 1.1: Plot of the linear function E[Y] = β₀+ β₁X\n\n\n\n\n\n\n\n\nHere, \\(\\beta_0\\) represents the value of \\(Y\\) when \\(X=0\\), and \\(\\beta_1\\) represents the change in Y for a one-unit increase in X.\n\n\nAssumption 2Y and X have a linear relationship\n\n\n\nExample 1: Salary growth over timeYou’ve been offered a job at ~company A~, and want to predict your salary at this company after working there for 10 years. We know that, on average, the starting salary at this company is $50,000, and salaries increase by $5000 per year of employment.\nUsing this information, we can construct a simple linear predictor of salary from employment time. Given an employment time of \\(X=x\\) years, we can represent a employee’s expected salary (\\(E[Y]\\)) as\n\\[\nE[Y]= \\$50,000 + \\$5,000\\cdot x.\n\\]\nAfter 10 years of working at this company (i.e. \\(X=10\\)), we would expect a salary of \\[E[Y]=\\$50,000 + \\$5,000\\times10= \\$100,000\\]\n\n\n\n\nA competing offer\nA second company also has a job available. The starting salary is higher here - $70,000 on average - but payrises are smaller. You are told that, on average, employees working for the company for 5 years earn $20,000 more per year than when they started.\nYou want to use a simple linear prediction to calculate your expected salary (\\(E[Y]\\)) after \\(X\\) years of employment.\n\\[\nE[Y] = \\beta_0 + \\beta_1 X\n\\]\n\nWhat are the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in this case?\nAssign the relevant values to variables in R\n\n\n\n\n\n\n\n\n\nUse the distinct() function from the {dplyr} package.\n\n\nbeta_0 &lt;- 70000\nbeta_1 &lt;- 4000\nbeta_0 &lt;- 70000\nbeta_1 &lt;- 4000\n\n\n\n\n\n\nGreat, so our linear predictor is \\[\nE[Y]= \\$70,000 + \\$4,000\\times X\n\\]\nUsing R, calculate the expected salary after working for this company for 10 years\n\n\n\n\n\n\n\n\n\n\n\n\nComplete the r function that predicts \\(Y\\) from an input \\(X\\) for this linear model (alternatively - give function e.g. ‘this function predicts salary at company 2 with an input X years work’)\n\n\n\n\n\n\n\n\n\n\n\n\nUse this function to predict \\(Y\\) for the following values of \\(X\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Errors\n\nIn practice, data rarely fall perfectly on a straight line. Even if the underlying relationship between \\(X\\) and \\(Y\\) is approximately linear, individual observations tend to vary around that line.\nAs mentioned in Section 1.x, to account for this variation we add an error term, \\(\\varepsilon\\), to our model\n\n\\[\nY=\\beta_0 + \\beta_1X + \\varepsilon\n\\]\n\nMean Zero\n\nThe error term represents the *difference between the actual value of \\(Y\\) and the value predicted by the linear predictor, \\(E[Y]\\).\nOn average, we expect these error terms to balance out:\n\n\nAssumption 3The mean of the error term is zero:\n\\[\nE[\\varepsilon]=\\mu=0\n\\]\ni.e. The linear predictor gives the correct value of \\(Y\\) on average\n\n\n\n\nConstant variance\n\nWhile correct on average, we expect there to be some spread of data around the line (this is why we have the error term). The amount of spread is measured by the variance of the errors.\nWe assume that this variance is constant - like \\(mu=0\\), it is the same for all values of \\(X\\) however we dont specify which particular value it takes:\n\n\nAssumption 4The variance of the error term is constant for all values of X: \\[Var(\\varepsilon)=\\sigma^2\\]\n\n\n\n\nNormal distribution\nWhile the assumptions of mean 0 and constant variance describe the center and spread of the errors, they don’t fully specify the shape of their distribution. To model this more completely, we often assume that the errors follow a Normal distribution.\n\nAssumption 5The error is normally distributed (with mean \\(\\mu=0\\) and variance \\(\\sigma^2\\) . \\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\n\n\nviewof mu = Inputs.range([-5, 5], {\n  value: 0,\n  step: 0.1,\n  label: `Mean (μ):`\n})\n\nviewof sigma = Inputs.range([0.2, 5], {\n  value: 1,\n  step: 0.1,\n  label: 'Standard deviation (σ):'\n})\n\nSQRT2PI=Math.sqrt(2 * Math.PI)\n\nnormalDensity = (x, mean, sd) =&gt;\n  (1 / (sd * SQRT2PI)) * Math.exp(-0.5 * ((x - mean) / sd) ** 2);\n  \ndensityGrid_1 = d3.range((-4 * sigma)+mu, mu+(4 * sigma), sigma / 50).map(x =&gt; ({\n  x,\n  density: normalDensity(x,mu,sigma)\n}));\n\ntex.block`\\varepsilon \\sim \\text{Normal}(${mu}, ${sigma}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 280,\n  marginLeft: 48,\n  marginBottom: 40,\n  y: { label: \"Density\" },\n  x: {domain: [-10,10], label: \"ε\" },\n  marks: [\n    Plot.areaY(densityGrid_1, {\n      x: \"x\",\n      y: \"density\",\n      fillOpacity: 0.2,\n      stroke: \"#2a5599\",\n      fill: \"#2a5599\"\n    })\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1.2: Normal distribution with adjustable mean and standard deviation.\n\n\n\n\n\n\n\n\nTechnical-Point 1The Central Limit Theorem is a fundamental result of probability theory that provides some extra motivation for this assumption. According to this theorem, sums or averages of many small, independent random effects tend to follow a Normal distribution. Thus, assuming normally distributed errors is both a practical simplification and a reasonable approximation in many situations.\n\n\n\nExample 2\n\nlets return to our simple linear model of salary at ~company A~,\n\n\\[\nE[Salary] = 50,000 + 5,000\\times Years\n\\] This expresses how salary tends to increase with experience (i.e. on averaage). But in practice, not every employee with the same number of years earns the same amount — some earn a bit more, some a bit less. Suppose we know that most employees — about two-thirds of them — earn within roughly $4,000 of the average salary for their experience level. In other words, if the average salary after five years is $75,000, then about two-thirds of employees earn between $71,000 and $79,000, and almost everyone (about 95%) earns between $67,000 and $83,000.\nWe can capture this variability with a random error term, \\(\\varepsilon\\), assumed to follow a Normal distribution with mean 0 and standard deviation \\(\\sigma = 4,000\\).\n\\[\nSalary = 50,000 + 5,000\\times Years + \\varepsilon, \\quad{\\varepsilon \\sim \\mathcal{N}(0,4000^2)}\n\\]\nThis means that for a given number of years \\(X\\): - The expected salary is \\(50,000 + 5,000\\cdot X\\) - Actual salaries will vary around that average, typically within about ±$4,000\nFor example, after 5 years (X=5): \\[\nE[Y]=\\$50,000 + \\$5,000 \\times 5 = \\$75,000\n\\]\nThe distribution of salaries for employees with 5 years’ experience is\n\\[\nY\\sim \\mathcal{N}(75,000, 4,000^2)\n\\]\nSince we have a probability distribution over \\(Y\\), we can use R to evaluate the probability of any given salary after X years at the company.\nFor example, we want to know if employed at company A, what is the probabity after working there for 10 years I will have a salary of at least $110,000?\nFirst lets calculate the average salary after 10 years\n\n50000+(5000*10)\n\n[1] 1e+05\n\n\n$100,000.\nNow\n\npnorm(11e4,1e5, 4e3, lower.tail = FALSE)\n\n[1] 0.006209665\n\n\nThe following diagram tells us roughly the probability of observing a \\(Y\\) value in the given range:\n\n\nmu_salary = 75000\nsigma_salary = 4000\n\nviewof k = Inputs.range([0, 3], {step: 0.1, value: 1, label: \"Half-width k (so interval is μ ± k·σ)\"})\nviewof offset_z = Inputs.range([-5, 5], {step: 0.1, value: 0, label: \"Centre offset (in σ units)\"})\n\ncentre = mu_salary + offset_z * sigma_salary\na = centre - k * sigma_salary\nb = centre + k * sigma_salary\n\n\n// --- Numerical helpers ---\nerf = x =&gt; {\n  const a1 = 0.254829592, a2 = -0.284496736, a3 = 1.421413741,\n  a4 = -1.453152027, a5 = 1.061405429, p = 0.3275911;\n  const sign = Math.sign(x) || 1;\n  x = Math.abs(x);\n  const t = 1 / (1 + p * x);\n  const y = 1 - (((((a5*t + a4)*t + a3)*t + a2)*t + a1)*t) * Math.exp(-x*x);\n  return sign * y;\n}\nnormalCDF = (x, mean, sd) =&gt; 0.5 * (1 + erf((x - mean) / (sd * Math.SQRT2)))\n\n// Probability mass between a and b\nprob = Math.max(0, Math.min(1, normalCDF(b, mu_salary, sigma_salary) - normalCDF(a, mu_salary, sigma_salary)))\n\n// Density grid and shaded interval\ndensityGrid_salary = d3.range(mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary, sigma_salary/100)\n.map(y =&gt; ({ y, density: normalDensity(y, mu_salary, sigma_salary) }))\n\nshaded = densityGrid_salary.filter(d =&gt; d.y &gt;= a && d.y &lt;= b)\n\n// Display text summary\ntex.block`P(${Math.round(a).toLocaleString()} \\le Y \\le ${Math.round(b).toLocaleString()}) = ${prob.toFixed(3)} \\;\\;(\\approx ${(prob*100).toFixed(1)}\\%)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 300,\n  marginLeft: 56,\n  marginBottom: 40,\n  x: { label: \"Salary ($)\", grid: true, domain: [mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary] },\n  y: { label: \"Density\",  },\n  marks: [\n    // Full curve (light)\n    Plot.areaY(densityGrid_salary, {x:\"y\", y:\"density\", fill:\"#2a5599\", fillOpacity:0.12, stroke:\"#2a5599\"}),\n    // Shaded probability region\n    Plot.areaY(shaded, {x:\"y\", y:\"density\", fill:\"#FFD54F\", fillOpacity:0.35}),\n    // Vertical rules\n    Plot.ruleX([mu_salary], {stroke: \"black\", strokeDash: [4,4]}),\n    Plot.ruleX([a], {y1:0, y2:normalDensity(a, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    Plot.ruleX([b], {y1:0, y2:normalDensity(b, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    // Baseline\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1.3: Probability of salary falling within a chosen interval.\n\n\n\n\n\n\n\nHere \\(\\varepsilon\\) represents random deviations from the expected (average) salary for a given number of years X. - Same model as above - given standard deviation - Example Y values - Illustrate error with normal overay - Excersises: - which variance is larger? - (hard) probability of finding E[Y]+2sd observation\n\n\n\n\n\nThe Simple Linear Model\nPutting these pieces together we are left with:\n\\[\nY=\\beta_0+\\beta_1X+\\varepsilon, \\quad{\\varepsilon \\sim N(0,\\sigma^2)}\n\\]\nThis ‘simple linear model’ is the starting place for conducting linear regression - in which whe ‘fit’ (i.e. estimate the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\)) from data.\n\n\nviewof b1_2 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β₁)\"})\nviewof b0_2 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (β₀)\"})\nviewof sigma_2 = Inputs.range([0.2, 5], {step: 0.1, value: 2.5, label: \"Std. deviation (σ)\"})\n\nviewof n_cs = Inputs.range([10, 50], {step: 1, value: 50, label: \"Number of cross sections visualised\"})\n\ntex.block`Y = ${b0_2} + ${b1_2}X + \\varepsilon, \\quad \\varepsilon \\sim \\text{Normal}(0, ${sigma_2}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxMin = -10;\nxMax = 10;\nstep = (xMax - xMin) / (n_cs - 1);\nxSampleValues = d3.range(xMin, xMax + step/2, step);  \n\nySectionValues = d3.range(-10, 10.001, 0.1)\nwidthScale = Math.min(1.8, sigma_2 * 0.9)\n\ndensityCurveData = xSampleValues.flatMap(xVal =&gt; {\n  const mu = b0_2 + b1_2 * xVal;\n  const peakDensity = normalDensity(mu, mu, sigma_2);\n\n  const rightSide = ySectionValues.map(y =&gt; {\n    const density = normalDensity(y, mu, sigma_2);\n    const width = (density / peakDensity) * widthScale;\n    return {x: xVal + width, y, group: xVal};\n  });\n\n  return rightSide\n});\n\ncrossSectionTrendLine = xSampleValues.map(x =&gt; ({\n  x,\n  y: b0_2 + b1_2 * x\n}))\n\nPlot.plot({\n  x: {domain: [-10, 10], label: \"X\", grid: true},\n  y: {domain: [-10, 10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(densityCurveData, {\n      x: \"x\",\n      y: \"y\",\n      z: \"group\",\n      stroke: \"#2a5599\",\n      strokeWidth: 1.5,\n      curve: \"basis\"\n    }),\n    Plot.line(crossSectionTrendLine, {x: \"x\", y: \"y\", stroke: \"black\", strokeWidth: 2})\n  ]\n});\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\nFigure 1.4: Cross-sections of the simple linear model normal error density.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#a-simple-linear-model-in-r",
    "href": "LinearModels.html#a-simple-linear-model-in-r",
    "title": "1  An introduction to linear statistical models",
    "section": "1.3 A simple Linear model in R",
    "text": "1.3 A simple Linear model in R\n\nfinish this section and lead into the next on fitting models to data.\nsimulate observations of Y from a specified linear model\n\n\nto begin, we start with a collection of X values (we might imagine we measure these values in the wild)\n\n\nn &lt;- 100\nX &lt;- runif(n=100, min=0, max= 50)\nhead(X)\n\n[1]  8.7017513 12.8652617 22.6738993 48.5435575 37.6681501  0.2450643\n\n\n\nThis code randomly chooses n = 100 values uniformy at random from the interval \\([0,50]\\).\n\n\nDefine the model\n\nNext, we construct our simple linear model\n\nbeta_0 &lt;- 4\nbeta_1 &lt;- 1.2\nsigma &lt;- 4\n\nsimple_linear_model &lt;- function(X, beta_0, beta_1, sigma) {\n  mu &lt;- beta_0 + (beta_1 * X) \n  mu + rnorm(length(X), mean = 0, sd = sigma)\n}\n\n\nThis is function takes X values and returns the specified linear function with normally distributed random noise added.\nNow we can simulate observations of \\(Y\\) given our list of \\(X\\) values and out linear model:\n\n\nY &lt;- simple_linear_model(X, beta_0, beta_1, sigma)\nhead(Y)\n\n[1] 18.164732 16.592125 28.500632 61.984854 45.915293  8.985226\n\n\nlets look at the joint distribution of X and Y:\n\nlibrary(ggplot2)\n\ndf &lt;- data.frame(X = X, Y = Y)\n\nggplot(df, aes(X, Y)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(intercept = beta_0, slope = beta_1, linetype = \"dashed\") +\n  labs(x = \"X\", y = \"Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 1.5: Data generated from the simple linear model \\(Y=4+1.2\\times X + \\varepsilon\\), with \\(\\varepsilon\\sim N(0,16)\\). Dashed line shows E[Y|X] = β0 + β1X\n\n\n\n\n\nHeres the same code running in webR - try adjusting some of the parameters (e.g. the number of samples) and running to plot a different random sample!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html",
    "href": "FittingSLR.html",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "",
    "text": "2.1 Estimating parameters\nin the previous section, you were given a linear model - we knew the values of \\(\\beta_0\\), \\(\\beta_1\\) and even \\(\\sigma\\). However, in most contexts we don’t know the true relationship between \\(X\\) and \\(Y\\) in advance. Instead, we are given data and try to infer the values of \\(\\beta\\), \\(\\sigma\\) based on this.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#estimating-parameters",
    "href": "FittingSLR.html#estimating-parameters",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "",
    "text": "Residuals\n\n\nEstimating \\(\\beta\\)\nWhich line fits the data best (how low can the RMSE go)?\n\nx = Array.from({length: 50}, d3.randomNormal(180, 50))\nnoise = Array.from({length: 50}, d3.randomNormal(0, 15))\ny = x.map((xi, i) =&gt; 0.5 * xi - 10 + noise[i])\n\nlinearData = x.map((xi, i) =&gt; ({ x: xi, y: y[i] }))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof b0adj = Inputs.range([-50, 50], {step: 1, label: html`${tex`\\beta_0`}: Intercept (adjustment)`})\nviewof b1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\\beta_1`}: Slope`})\n\nb0 = (90 - b1 * 180) + b0adj\n\nresidualData = linearData.map(d =&gt; {\n  const yhat = b1 * d.x + b0;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n\nRMSR = Math.sqrt(d3.sum(residualData, d =&gt; d.residual ** 2) / residualData.length)\n\ntex.block`y = ${b1}x + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\sqrt{\\frac{1}{n}\\sum_{i}r_i^{2}} = ${RMSR.toFixed(2)}`\n\n\n\n\n\n\n\n\n\n\n\nxRange = d3.extent(residualData,d =&gt; d.x)\nlineData = xRange.map(x =&gt; ({x, y: b1 * x + b0}))\n\nPlot.plot({\n  marks: [\n    Plot.ruleX(residualData, {x: \"x\",\n      y1: \"y\",\n      y2: \"yhat\",\n      stroke: \"sign\", \n      strokeOpacity: 0.75,\n      strokeWidth: d =&gt; 1 + d.absRes / 12,\n       title: d =&gt; `x=${d.x.toFixed(1)}\ny=${d.y.toFixed(1)}\nŷ=${d.yhat.toFixed(1)}\nres=${d.residual.toFixed(1)}\nres²=${(d.residual**2).toFixed(1)}`\n    }),\n\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(linearData, { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [0, 180], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a model using least squares - lm()\n\n\n\n\n\n\n\n\n\n\nEstimating \\(\\sigma\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#checking-assumptions",
    "href": "FittingSLR.html#checking-assumptions",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.2 Checking assumptions",
    "text": "2.2 Checking assumptions\n\nIndependence of observations\n\n\nLinear relationships\n\n\nConstant Variance\n\n\nNormality of error",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#inference-for-simple-linear-regression",
    "href": "FittingSLR.html#inference-for-simple-linear-regression",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.3 Inference for Simple Linear Regression",
    "text": "2.3 Inference for Simple Linear Regression\n\nInference about \\(\\sigma\\)\n\n\nInference about \\(\\beta_1\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#interpreting-summary-output",
    "href": "FittingSLR.html#interpreting-summary-output",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.4 Interpreting `summary()` output",
    "text": "2.4 Interpreting `summary()` output",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#using-the-model-for-prediction",
    "href": "FittingSLR.html#using-the-model-for-prediction",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.5 Using the model for prediction",
    "text": "2.5 Using the model for prediction",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html",
    "href": "MultipleLinearRegression.html",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "",
    "text": "3.1 Continuous predictors",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#continuous-predictors",
    "href": "MultipleLinearRegression.html#continuous-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "",
    "text": "Multi-collinearity",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#categorical-predictors",
    "href": "MultipleLinearRegression.html#categorical-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "3.2 Categorical predictors",
    "text": "3.2 Categorical predictors",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#multiplying-predictors",
    "href": "MultipleLinearRegression.html#multiplying-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "3.3 Multiplying predictors",
    "text": "3.3 Multiplying predictors\n\nPolynomials\n\n\nInteractions\n\n\nTransformation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#linear-prediction",
    "href": "LinearModels.html#linear-prediction",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "We now choose \\(f(X)\\) to be a linear function:\n\n\n\nPutting aside the random error term for the moment, we can think of the expected value of \\(Y\\), denoted \\(E[Y]\\), as being given by this linear relationship:\n\n\n\nIn other words, we are representing the expected value of \\(Y\\) as a straight line with y-intercept \\(\\beta_0\\) and slope \\(\\beta_1\\):\n\n\n\nviewof b1 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β₁)\"})\nviewof b0 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (β₀)\"})\ntex.block`E[Y] = ${b1}X + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = [-10,10]\nlineData = xRange.map(x =&gt; ({x, y: (b1 * x) + b0}))\nPlot.plot({\n  x:{domain: [-10,10], label: \"X\", grid: true},\n  y:{domain: [-10,10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(lineData, { x: \"x\", y: \"y\" })]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\nFigure 1.1: Plot of the linear function E[Y] = β₀+ β₁X\n\n\n\n\n\n\n\n\nHere, \\(\\beta_0\\) represents the value of \\(Y\\) when \\(X=0\\), and \\(\\beta_1\\) represents the change in Y for a one-unit increase in X.\n\n\nAssumption 2Y and X have a linear relationship\n\n\n\nExample 1: Salary growth over timeYou’ve been offered a job at ~company A~, and want to predict your salary at this company after working there for 10 years. We know that, on average, the starting salary at this company is $50,000, and salaries increase by $5000 per year of employment.\nUsing this information, we can construct a simple linear predictor of salary from employment time. Given an employment time of \\(X=x\\) years, we can represent a employee’s expected salary (\\(E[Y]\\)) as\n\\[\nE[Y]= \\$50,000 + \\$5,000\\cdot x.\n\\]\nAfter 10 years of working at this company (i.e. \\(X=10\\)), we would expect a salary of \\[E[Y]=\\$50,000 + \\$5,000\\times10= \\$100,000\\]\n\n\n\nExercise: A competing offer\nA second company also has a job available. The starting salary is higher here - $70,000 on average - but payrises are smaller. You are told that, on average, employees working for the company for 5 years earn $20,000 more per year than when they started.\nYou want to use a simple linear prediction to calculate your expected salary (\\(E[Y]\\)) after \\(X\\) years of employment.\n\\[\nE[Y] = \\beta_0 + \\beta_1 X\n\\]\n\nChoosing parameters\nWhat are the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in this case?\nAssign the relevant values to variables in R\n\n\n\n\n\n\n\n\n\n\n\nbeta_0 &lt;- 70000\nbeta_1 &lt;- 4000\nbeta_0 &lt;- 70000\nbeta_1 &lt;- 4000\n\n\n\n\n\n\n\n\nLinear prediction\nGreat, so our linear predictor of the expected salary looks something like: \\[\nE[Y]= \\$70,000 + \\$4,000\\cdot X\n\\]\nUsing the parameter variables you just defined, calculate the expected salary after working for this companyfor 10 years\n\n\n\n\n\n\n\n\n\n\n\nEY &lt;- beta_0 + beta_1*10\nEY &lt;- beta_0 + beta_1*10\n\n\n\n\n\n\n\n\nUsing R functions\nGreat, so our expected salary after working at the company for 10 years is\n\n\n\n\n\n\n\n\nComplete the r function that predicts \\(Y\\) from an input \\(X\\) for this linear model (alternatively - give function e.g. ‘this function predicts salary at company 2 with an input X years work’)\n\n\n\n\n\n\n\n\n\n\n\n70000+4000*10\n70000+4000*10\n\n\n\n\n\n\nUse this function to predict \\(Y\\) for the following values of \\(X\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n70000+4000*10\n70000+4000*10\n\n\n\n\n\n\n\nToDo 1add plot comparing two (companies) linear salary growth?\n\n\n\nContinue to the next section",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#random-errors",
    "href": "LinearModels.html#random-errors",
    "title": "1  An introduction to linear statistical models",
    "section": "1.2 Random Errors",
    "text": "1.2 Random Errors\n\nIn practice, data rarely fall perfectly on a straight line. Even if the underlying relationship between \\(X\\) and \\(Y\\) is approximately linear, individual observations tend to vary around that line.\nAs mentioned in Section 1.x, to account for this variation we add an error term, \\(\\varepsilon\\), to our model\n\n\\[\nY=\\beta_0 + \\beta_1X + \\varepsilon\n\\]\n\nMean Zero\n\nThe error term represents the *difference between the actual value of \\(Y\\) and the value predicted by the linear predictor, \\(E[Y]\\).\nOn average, we expect these error terms to balance out:\n\n\nAssumption 3The mean of the error term is zero:\n\\[\nE[\\varepsilon]=\\mu=0\n\\]\ni.e. The linear predictor gives the correct value of \\(Y\\) on average\n\n\n\n\nConstant variance\n\nWhile correct on average, we expect there to be some spread of data around the line (this is why we have the error term). The amount of spread is measured by the variance of the errors.\nWe assume that this variance is constant - like \\(mu=0\\), it is the same for all values of \\(X\\) however we dont specify which particular value it takes:\n\n\nAssumption 4The variance of the error term is constant for all values of X: \\[Var(\\varepsilon)=\\sigma^2\\]\n\n\n\n\nNormal distribution\nWhile the assumptions of mean 0 and constant variance describe the center and spread of the errors, they don’t fully specify the shape of their distribution. To model this more completely, we often assume that the errors follow a Normal distribution.\n\nAssumption 5The error is normally distributed (with mean \\(\\mu=0\\) and variance \\(\\sigma^2\\) . \\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\n\n\nviewof mu = Inputs.range([-5, 5], {\n  value: 0,\n  step: 0.1,\n  label: `Mean (μ):`\n})\n\nviewof sigma = Inputs.range([0.2, 5], {\n  value: 1,\n  step: 0.1,\n  label: 'Standard deviation (σ):'\n})\n\nSQRT2PI=Math.sqrt(2 * Math.PI)\n\nnormalDensity = (x, mean, sd) =&gt;\n  (1 / (sd * SQRT2PI)) * Math.exp(-0.5 * ((x - mean) / sd) ** 2);\n  \ndensityGrid_1 = d3.range((-4 * sigma)+mu, mu+(4 * sigma), sigma / 50).map(x =&gt; ({\n  x,\n  density: normalDensity(x,mu,sigma)\n}));\n\ntex.block`\\varepsilon \\sim \\text{Normal}(${mu}, ${sigma}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 280,\n  marginLeft: 48,\n  marginBottom: 40,\n  y: { label: \"Density\" },\n  x: {domain: [-10,10], label: \"ε\" },\n  marks: [\n    Plot.areaY(densityGrid_1, {\n      x: \"x\",\n      y: \"density\",\n      fillOpacity: 0.2,\n      stroke: \"#2a5599\",\n      fill: \"#2a5599\"\n    })\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1.2: Normal distribution with adjustable mean and standard deviation.\n\n\n\n\n\n\n\n\nTechnical-Point 1The Central Limit Theorem is a fundamental result of probability theory that provides some extra motivation for this assumption. According to this theorem, sums or averages of many small, independent random effects tend to follow a Normal distribution. Thus, assuming normally distributed errors is both a practical simplification and a reasonable approximation in many situations.\n\n\n\nExample 2\n\nlets return to our simple linear model of salary at ~company A~,\n\n\\[\nE[Salary] = 50,000 + 5,000\\times Years\n\\] This expresses how salary tends to increase with experience (i.e. on averaage). But in practice, not every employee with the same number of years earns the same amount — some earn a bit more, some a bit less. Suppose we know that most employees — about two-thirds of them — earn within roughly $4,000 of the average salary for their experience level. In other words, if the average salary after five years is $75,000, then about two-thirds of employees earn between $71,000 and $79,000, and almost everyone (about 95%) earns between $67,000 and $83,000.\nWe can capture this variability with a random error term, \\(\\varepsilon\\), assumed to follow a Normal distribution with mean 0 and standard deviation \\(\\sigma = 4,000\\).\n\\[\nSalary = 50,000 + 5,000\\times Years + \\varepsilon, \\quad{\\varepsilon \\sim \\mathcal{N}(0,4000^2)}\n\\]\nThis means that for a given number of years \\(X\\): - The expected salary is \\(50,000 + 5,000\\cdot X\\) - Actual salaries will vary around that average, typically within about ±$4,000\nFor example, after 5 years (X=5): \\[\nE[Y]=\\$50,000 + \\$5,000 \\times 5 = \\$75,000\n\\]\nThe distribution of salaries for employees with 5 years’ experience is\n\\[\nY\\sim \\mathcal{N}(75,000, 4,000^2)\n\\]\nSince we have a probability distribution over \\(Y\\), we can use R to evaluate the probability of any given salary after X years at the company.\nFor example, we want to know if employed at company A, what is the probabity after working there for 10 years I will have a salary of at least $110,000?\nFirst lets calculate the average salary after 10 years\n\n50000+(5000*10)\n\n[1] 1e+05\n\n\n$100,000.\nNow\n\npnorm(11e4,1e5, 4e3, lower.tail = FALSE)\n\n[1] 0.006209665\n\n\nThe following diagram tells us roughly the probability of observing a \\(Y\\) value in the given range:\n\n\nmu_salary = 75000\nsigma_salary = 4000\n\nviewof k = Inputs.range([0, 3], {step: 0.1, value: 1, label: \"Half-width k (so interval is μ ± k·σ)\"})\nviewof offset_z = Inputs.range([-5, 5], {step: 0.1, value: 0, label: \"Centre offset (in σ units)\"})\n\ncentre = mu_salary + offset_z * sigma_salary\na = centre - k * sigma_salary\nb = centre + k * sigma_salary\n\n\n// --- Numerical helpers ---\nerf = x =&gt; {\n  const a1 = 0.254829592, a2 = -0.284496736, a3 = 1.421413741,\n  a4 = -1.453152027, a5 = 1.061405429, p = 0.3275911;\n  const sign = Math.sign(x) || 1;\n  x = Math.abs(x);\n  const t = 1 / (1 + p * x);\n  const y = 1 - (((((a5*t + a4)*t + a3)*t + a2)*t + a1)*t) * Math.exp(-x*x);\n  return sign * y;\n}\nnormalCDF = (x, mean, sd) =&gt; 0.5 * (1 + erf((x - mean) / (sd * Math.SQRT2)))\n\n// Probability mass between a and b\nprob = Math.max(0, Math.min(1, normalCDF(b, mu_salary, sigma_salary) - normalCDF(a, mu_salary, sigma_salary)))\n\n// Density grid and shaded interval\ndensityGrid_salary = d3.range(mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary, sigma_salary/100)\n.map(y =&gt; ({ y, density: normalDensity(y, mu_salary, sigma_salary) }))\n\nshaded = densityGrid_salary.filter(d =&gt; d.y &gt;= a && d.y &lt;= b)\n\n// Display text summary\ntex.block`P(${Math.round(a).toLocaleString()} \\le Y \\le ${Math.round(b).toLocaleString()}) = ${prob.toFixed(3)} \\;\\;(\\approx ${(prob*100).toFixed(1)}\\%)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 300,\n  marginLeft: 56,\n  marginBottom: 40,\n  x: { label: \"Salary ($)\", grid: true, domain: [mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary] },\n  y: { label: \"Density\",  },\n  marks: [\n    // Full curve (light)\n    Plot.areaY(densityGrid_salary, {x:\"y\", y:\"density\", fill:\"#2a5599\", fillOpacity:0.12, stroke:\"#2a5599\"}),\n    // Shaded probability region\n    Plot.areaY(shaded, {x:\"y\", y:\"density\", fill:\"#FFD54F\", fillOpacity:0.35}),\n    // Vertical rules\n    Plot.ruleX([mu_salary], {stroke: \"black\", strokeDash: [4,4]}),\n    Plot.ruleX([a], {y1:0, y2:normalDensity(a, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    Plot.ruleX([b], {y1:0, y2:normalDensity(b, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    // Baseline\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1.3: Probability of salary falling within a chosen interval.\n\n\n\n\n\n\n\nHere \\(\\varepsilon\\) represents random deviations from the expected (average) salary for a given number of years X. - Same model as above - given standard deviation - Example Y values - Illustrate error with normal overay - Excersises: - which variance is larger? - (hard) probability of finding E[Y]+2sd observation\n\n\n\n\nThe Simple Linear Model\nPutting these pieces together we are left with:\n\\[\nY=\\beta_0+\\beta_1X+\\varepsilon, \\quad{\\varepsilon \\sim N(0,\\sigma^2)}\n\\]\nThis ‘simple linear model’ is the starting place for conducting linear regression - in which whe ‘fit’ (i.e. estimate the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\)) from data.\n\n\nviewof b1_2 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β₁)\"})\nviewof b0_2 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (β₀)\"})\nviewof sigma_2 = Inputs.range([0.2, 5], {step: 0.1, value: 2.5, label: \"Std. deviation (σ)\"})\n\nviewof n_cs = Inputs.range([10, 50], {step: 1, value: 50, label: \"Number of cross sections visualised\"})\n\ntex.block`Y = ${b0_2} + ${b1_2}X + \\varepsilon, \\quad \\varepsilon \\sim \\text{Normal}(0, ${sigma_2}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxMin = -10;\nxMax = 10;\nstep = (xMax - xMin) / (n_cs - 1);\nxSampleValues = d3.range(xMin, xMax + step/2, step);  \n\nySectionValues = d3.range(-10, 10.001, 0.1)\nwidthScale = Math.min(1.8, sigma_2 * 0.9)\n\ndensityCurveData = xSampleValues.flatMap(xVal =&gt; {\n  const mu = b0_2 + b1_2 * xVal;\n  const peakDensity = normalDensity(mu, mu, sigma_2);\n\n  const rightSide = ySectionValues.map(y =&gt; {\n    const density = normalDensity(y, mu, sigma_2);\n    const width = (density / peakDensity) * widthScale;\n    return {x: xVal + width, y, group: xVal};\n  });\n\n  return rightSide\n});\n\ncrossSectionTrendLine = xSampleValues.map(x =&gt; ({\n  x,\n  y: b0_2 + b1_2 * x\n}))\n\nPlot.plot({\n  x: {domain: [-10, 10], label: \"X\", grid: true},\n  y: {domain: [-10, 10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(densityCurveData, {\n      x: \"x\",\n      y: \"y\",\n      z: \"group\",\n      stroke: \"#2a5599\",\n      strokeWidth: 1.5,\n      curve: \"basis\"\n    }),\n    Plot.line(crossSectionTrendLine, {x: \"x\", y: \"y\", stroke: \"black\", strokeWidth: 2})\n  ]\n});\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\nFigure 1.4: Cross-sections of the simple linear model normal error density.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  }
]