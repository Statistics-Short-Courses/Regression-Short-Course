[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Linear Regression in R",
    "section": "",
    "text": "Preface\nThis notebook provides an introduction (or refresher) to linear regression, a fundamental tool for modeling relationships between variables. We’ll explore the concepts, fit models in R, interpret results, and visualize fits.\nBy the end, you should be able to:\n\nUnderstand the linear regression model and its assumptions.\nFit and interpret simple and multiple regression models in R.\nDiagnose model fit using residual plots and summary statistics.\nUse model output to make predictions and assess uncertainty.\nPresent the results of a linear regression analysis for publication.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "LinearModels.html",
    "href": "LinearModels.html",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "1.1 Statistical models\nHere \\(\\varepsilon\\) represents the random variation in \\(Y\\) that is not explained by \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#statistical-models",
    "href": "LinearModels.html#statistical-models",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "A primary goal of statistical modelling is to characterise the relationship between variables, for now lets call these variables \\(X\\) and \\(Y\\).\nIn this context, we designate one variable, \\(Y\\) as the outcome or dependent variable, and attempt to construct a model describing how it depends on the other variable(s), \\(X\\), which we call predictors or independent variables.\nWe can express the idea that \\(Y\\) depends on \\(X\\) mathematically as \\[\nY=f(X)\n\\]\nThat is, \\(Y\\) is some function of \\(X\\) (we have not yet specified what kind of function). Given values of \\(X\\), we can use this function to predict or explain corresponding values of \\(Y\\).\nThis is similar to deterministic models you may know from physics - for example \\(E=MC^2\\), where \\(E\\) (the outcome), depends on \\(M\\) (a predictor) through the function \\(f(M)=MC^2\\)).\nIn statistical modelling we take another step and assume that \\(Y\\) is not a purely determinsitic function of \\(X\\), but that it varies somewhat randomly around such a relationship. We capture this by introducing a random error term , \\(\\varepsilon\\): \\[\nY=f(X)+\\varepsilon\n\\tag{1.1}\\]\n\n\n\nThis means that \\(Y\\) is not perfectly determined by \\(X\\): even if we know the values of X, the outcome Y can still vary due to random noise.\nThis is the setup for our statisical model, and while it may seem very general it makes a key assumption: that it is possible to model our outcome \\(Y\\) just using the information from \\(X\\) (along with random noise). We will discuss the relevance of this assumption more in chapter 2, but lets put a pin in it for now:\n\n\nAssumption 1\\(Y\\) depends only on a deterministic function of \\(X\\) and a random noise component\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#linear-prediction",
    "href": "LinearModels.html#linear-prediction",
    "title": "1  An introduction to linear statistical models",
    "section": "1.2 Linear prediction",
    "text": "1.2 Linear prediction\n\nWe now choose \\(f(X)\\) to be a linear function:\n\n\\[\nf(X)= \\beta_0 + \\beta_1\\cdot X\n\\]\n\nPutting aside the random error term for the moment, we can think of the expected value of \\(Y\\), denoted \\(E[Y]\\), as being given by this linear relationship:\n\n\\[\nE[Y]= \\beta_0 + \\beta_1\\cdot X\n\\tag{1.2}\\]\n\nIn other words, we are representing the expected value of \\(Y\\) as a straight line with y-intercept \\(\\beta_0\\) and slope \\(\\beta_1\\):\n\n\n\nviewof b1 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β₁)\"})\nviewof b0 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (β₀)\"})\ntex.block`E[Y] = ${b1}X + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = [-10,10]\nlineData = xRange.map(x =&gt; ({x, y: (b1 * x) + b0}))\nPlot.plot({\n  x:{domain: [-10,10], label: \"X\", grid: true},\n  y:{domain: [-10,10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(lineData, { x: \"x\", y: \"y\" })]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\nFigure 1.1: Plot of the linear function E[Y] = β₀+ β₁X\n\n\n\n\n\n\n\n\nHere, \\(\\beta_0\\) represents the expected value of \\(Y\\) (\\(E[Y]\\)) when \\(X=0\\), and \\(\\beta_1\\) represents the change in \\(E[Y]\\) for a one-unit increase in \\(X\\).\n\n\nAssumption 2Y and X have a linear relationship\n\n\n\nContinue\n\n\nExample 1: Salary growth over timeYou’ve been offered a job at ~company A~, and want to predict your salary at this company after working there for 10 years. We know that, on average, the starting salary at this company is $50,000, and salaries increase by $5000 per year of employment.\nUsing this information, we can construct a simple linear predictor of salary from employment time. Given an employment time of \\(X=x\\) years, we can represent a employee’s expected salary (\\(E[Y]\\)) as\n\\[\nE[Y]= \\$50,000 + \\$5,000\\cdot x.\n\\]\n\nPlotCode\n\n\n\n\n\n\n\ntest\n\n\n\n\n\n\n\nggplot()+\n  geom_abline(intercept=5e4, slope=5e3, colour='#2196F3')+\n  lims(x=c(0,15),y=c(4e4,13e4))+\n  labs(x=\"Years Employment\", y= \"Expecteted Salary\")\n\n\n\n\nAfter 10 years of working at this company (i.e. \\(X=10\\)), we would expect a salary of \\[E[Y]=\\$50,000 + \\$5,000\\times10= \\$100,000\\]\n\n\n\nExercise 1: A competing offer\nA second company also has a job available. The starting salary ishigher here - $70,000 on average - but payrises are smaller. You are told that, on average, employees working for the company for 6 years earn $18,000 more per year than when they started.\nYou want to use a simple linear prediction to calculate your expectedsalary (\\(E[Y]\\)) after \\(X\\) years of employment.\n\\[\nE[Y] = \\beta_0 + \\beta_1 X\n\\]\n\nChoosing parameters\nWhat are the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in this case?\nAssign the relevant values to variables in R\n\n\n\n\n\n\n\n\n\n\n\nbeta_0 &lt;- 70000\nbeta_1 &lt;- 3000\nbeta_0 &lt;- 70000\nbeta_1 &lt;- 3000\n\n\n\n\n\n\n\n\nLinear prediction\nGreat, so our linear predictor of the expected salary looks something like: \\[\nE[Y]= \\$70,000 + \\$3,000\\cdot X\n\\]\nhere is a plot of that linear relationship alongside that of company 1:\n\nPlotCode\n\n\n\n\n\n\n\nLinear trend of salary and years employed at two different companies\n\n\n\n\n\n\n\nggplot()+\n  geom_abline(aes(intercept=7e4, slope=3e3, colour=\"1\"))+\n  geom_abline(aes(intercept=5e4, slope=5e3, colour=\"2\"))+\n  lims(x=c(0,15),y=c(4e4,13e4))+\n  labs(x=\"Years Employment\",\n       y= \"Expecteted Salary ($)\",\n       colour=\"Company\")+\n  scale_color_manual(values= c(\"1\"=\"#4CAF50\",\"2\"='#2196F3'))\n\n\n\n\nUsing the parameter variables you just defined, calculate the expected salary after working for this companyfor 10 years\n\n\n\n\n\n\n\n\n\n\n\nE_Y &lt;- beta_0 + (beta_1*10)\nE_Y &lt;- beta_0 + (beta_1*10)\n\n\n\n\n\n\n\n\nUsing R functions\nGreat, so our expected salary after working at the company for 10 years is\n\n\n\n\n\n\n\n\n\\[\nE[Y]= \\$70,000+\\$3,000\\times10 = \\$100,000\n\\]\nSounds alright.\nNow to finish this section lets turn this linear prediction into our own R function that takes an input \\(X\\) and returns the \\(E[Y]\\)\n\n\n\n\n\n\n\n\nUse this function to predict \\(Y\\) for the following values of \\(X\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsimple_linear_prediction(X)\nsimple_linear_prediction(X)\n\n\n\n\n\n\n\n\nGood work!\nNext we’ll add the second main component to our linear predictor to make it a linear statistical model\n\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#random-errors",
    "href": "LinearModels.html#random-errors",
    "title": "1  An introduction to linear statistical models",
    "section": "1.3 Random Errors",
    "text": "1.3 Random Errors\n\nIn practice, data rarely fall perfectly on a straight line. Even if the underlying relationship between \\(X\\) and \\(Y\\) is approximately linear, individual observations tend to vary around that line.\nAs mentioned in Section 1.x, to account for this variation we add an error term, \\(\\varepsilon\\), to our model\n\n\\[\nY=\\beta_0 + \\beta_1X + \\varepsilon\n\\]\n\nMean Zero\n\nThe error term represents the *difference between the actual value of \\(Y\\) and the value predicted by the linear predictor, \\(E[Y]\\).\nOn average, we expect these error terms to balance out:\n\n\nAssumption 3The mean of the error term is zero:\n\\[\nE[\\varepsilon]=\\mu=0\n\\]\ni.e. The linear predictor gives the correct value of \\(Y\\) on average\n\n\n\n\nConstant variance\n\nWhile correct on average, we expect there to be some spread of data around the line (this is why we have the error term). The amount of spread is measured by the variance of the errors.\nWe assume that this variance is constant - like \\(mu=0\\), it is the same for all values of \\(X\\) however we dont specify which particular value it takes:\n\n\nAssumption 4The variance of the error term is constant for all values of X: \\[Var(\\varepsilon)=\\sigma^2\\]\n\n\n\n\nNormal distribution\nWhile the assumptions of mean 0 and constant variance describe the center and spread of the errors, they don’t fully specify the shape of their distribution. To model this more completely, we often assume that the errors follow a Normal distribution.\n\nAssumption 5The error is normally distributed (with mean \\(\\mu=0\\) and variance \\(\\sigma^2\\) . \\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\n\n\nviewof mu = Inputs.range([-5, 5], {\n  value: 0,\n  step: 0.1,\n  label: `Mean (μ):`\n})\n\nviewof sigma = Inputs.range([0.2, 5], {\n  value: 1,\n  step: 0.1,\n  label: 'Standard deviation (σ):'\n})\n\nSQRT2PI=Math.sqrt(2 * Math.PI)\n\nnormalDensity = (x, mean, sd) =&gt;\n  (1 / (sd * SQRT2PI)) * Math.exp(-0.5 * ((x - mean) / sd) ** 2);\n  \ndensityGrid_1 = d3.range((-4 * sigma)+mu, mu+(4 * sigma), sigma / 50).map(x =&gt; ({\n  x,\n  density: normalDensity(x,mu,sigma)\n}));\n\ntex.block`\\varepsilon \\sim \\text{Normal}(${mu}, ${sigma}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 280,\n  marginLeft: 48,\n  marginBottom: 40,\n  y: { label: \"Density\" },\n  x: {domain: [-10,10], label: \"ε\" },\n  marks: [\n    Plot.areaY(densityGrid_1, {\n      x: \"x\",\n      y: \"density\",\n      fillOpacity: 0.2,\n      stroke: \"#2a5599\",\n      fill: \"#2a5599\"\n    })\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1.2: Normal distribution with adjustable mean and standard deviation.\n\n\n\n\n\n\n\n\nTechnical-point 1: Central limit theorem\n\n\n\nExample 2: Variation in salaryLets return to our simple linear model of salary at ~company A~,\n\\[\nE[Salary] = 50,000 + 5,000\\times Years\n\\]\nThis expresses how salary tends to increase with experience (i.e. on averaage). But in practice, not every employee with the same number of years earns the same amount — some earn a bit more, some a bit less. Suppose we know that most employees — about two-thirds of them — earn within roughly $4,000 of the average salary for their experience level. In other words, if the average salary after five years is $75,000, then about two-thirds of employees earn between $71,000 and $79,000, and almost everyone (about 95%) earns between $67,000 and $83,000.\nWe can capture this variability with a random error term, \\(\\varepsilon\\), assumed to follow a Normal distribution with mean 0 and standard deviation \\(\\sigma = 4,000\\).\n\\[\nSalary = 50,000 + 5,000\\times Years + \\varepsilon, \\quad{\\varepsilon \\sim \\mathcal{N}(0,4000^2)}\n\\]\nThis means that for a given number of years \\(X\\): - The expected salary is \\(50,000 + 5,000\\cdot X\\) - Actual salaries will vary around that average, typically within about ±$4,000\nFor example, after 5 years (X=5): \\[\nE[Y]=\\$50,000 + \\$5,000 \\times 5 = \\$75,000\n\\]\nThe distribution of salaries for employees with 5 years’ experience is\n\\[\nY\\sim \\mathcal{N}(75,000, 4,000^2)\n\\]\nSince we have a probability distribution over \\(Y\\), we can use R to evaluate the probability of any given salary after X years at the company.\nFor example, we want to know if employed at company A, what is the probabity after working there for 10 years I will have a salary of at least $110,000?\nFirst lets calculate the average salary after 10 years\n\n50000+(5000*10)\n\n[1] 1e+05\n\n\n$100,000.\nNow\n\npnorm(11e4,1e5, 4e3, lower.tail = FALSE)\n\n[1] 0.006209665\n\n\nThe following diagram tells us roughly the probability of observing a \\(Y\\) value in the given range:\n\n\nmu_salary = 75000\nsigma_salary = 4000\n\nviewof k = Inputs.range([0, 3], {step: 0.1, value: 1, label: \"Half-width k (so interval is μ ± k·σ)\"})\nviewof offset_z = Inputs.range([-5, 5], {step: 0.1, value: 0, label: \"Centre offset (in σ units)\"})\n\ncentre = mu_salary + offset_z * sigma_salary\na = centre - k * sigma_salary\nb = centre + k * sigma_salary\n\n\n// --- Numerical helpers ---\nerf = x =&gt; {\n  const a1 = 0.254829592, a2 = -0.284496736, a3 = 1.421413741,\n  a4 = -1.453152027, a5 = 1.061405429, p = 0.3275911;\n  const sign = Math.sign(x) || 1;\n  x = Math.abs(x);\n  const t = 1 / (1 + p * x);\n  const y = 1 - (((((a5*t + a4)*t + a3)*t + a2)*t + a1)*t) * Math.exp(-x*x);\n  return sign * y;\n}\nnormalCDF = (x, mean, sd) =&gt; 0.5 * (1 + erf((x - mean) / (sd * Math.SQRT2)))\n\n// Probability mass between a and b\nprob = Math.max(0, Math.min(1, normalCDF(b, mu_salary, sigma_salary) - normalCDF(a, mu_salary, sigma_salary)))\n\n// Density grid and shaded interval\ndensityGrid_salary = d3.range(mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary, sigma_salary/100)\n.map(y =&gt; ({ y, density: normalDensity(y, mu_salary, sigma_salary) }))\n\nshaded = densityGrid_salary.filter(d =&gt; d.y &gt;= a && d.y &lt;= b)\n\n// Display text summary\ntex.block`P(${Math.round(a).toLocaleString()} \\le Y \\le ${Math.round(b).toLocaleString()}) = ${prob.toFixed(3)} \\;\\;(\\approx ${(prob*100).toFixed(1)}\\%)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 300,\n  marginLeft: 56,\n  marginBottom: 40,\n  x: { label: \"Salary ($)\", grid: true, domain: [mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary] },\n  y: { label: \"Density\",  },\n  marks: [\n    // Full curve (light)\n    Plot.areaY(densityGrid_salary, {x:\"y\", y:\"density\", fill:\"#2a5599\", fillOpacity:0.12, stroke:\"#2a5599\"}),\n    // Shaded probability region\n    Plot.areaY(shaded, {x:\"y\", y:\"density\", fill:\"#FFD54F\", fillOpacity:0.35}),\n    // Vertical rules\n    Plot.ruleX([mu_salary], {stroke: \"black\", strokeDash: [4,4]}),\n    Plot.ruleX([a], {y1:0, y2:normalDensity(a, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    Plot.ruleX([b], {y1:0, y2:normalDensity(b, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    // Baseline\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1.3: Probability of salary falling within a chosen interval.\n\n\n\n\n\n\n\nHere \\(\\varepsilon\\) represents random deviations from the expected (average) salary for a given number of years X. - Same model as above - given standard deviation - Example Y values - Illustrate error with normal overay - Excersises: - which variance is larger? - (hard) probability of finding E[Y]+2sd observation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#the-simple-linear-model",
    "href": "LinearModels.html#the-simple-linear-model",
    "title": "1  An introduction to linear statistical models",
    "section": "1.4 The Simple Linear Model",
    "text": "1.4 The Simple Linear Model\nPutting these pieces together we are left with:\n\\[\nY=\\beta_0+\\beta_1X+\\varepsilon, \\quad{\\varepsilon \\sim N(0,\\sigma^2)}\n\\]\nThis ‘simple linear model’ is the starting place for conducting linear regression - in which we ‘fit’ (i.e. estimate the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\)) from data.\n\n\nviewof b1_2 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β₁)\"})\nviewof b0_2 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (β₀)\"})\nviewof sigma_2 = Inputs.range([0.2, 5], {step: 0.1, value: 2.5, label: \"Std. deviation (σ)\"})\n\nviewof n_cs = Inputs.range([10, 50], {step: 1, value: 50, label: \"Number of cross sections visualised\"})\n\ntex.block`Y = ${b0_2} + ${b1_2}X + \\varepsilon, \\quad \\varepsilon \\sim \\text{Normal}(0, ${sigma_2}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxMin = -10;\nxMax = 10;\nstep = (xMax - xMin) / (n_cs - 1);\nxSampleValues = d3.range(xMin, xMax + step/2, step);  \n\nySectionValues = d3.range(-10, 10.001, 0.1)\nwidthScale = Math.min(1.8, sigma_2 * 0.9)\n\ndensityCurveData = xSampleValues.flatMap(xVal =&gt; {\n  const mu = b0_2 + b1_2 * xVal;\n  const peakDensity = normalDensity(mu, mu, sigma_2);\n\n  const rightSide = ySectionValues.map(y =&gt; {\n    const density = normalDensity(y, mu, sigma_2);\n    const width = (density / peakDensity) * widthScale;\n    return {x: xVal + width, y, group: xVal};\n  });\n\n  return rightSide\n});\n\ncrossSectionTrendLine = xSampleValues.map(x =&gt; ({\n  x,\n  y: b0_2 + b1_2 * x\n}))\n\nPlot.plot({\n  x: {domain: [-10, 10], label: \"X\", grid: true},\n  y: {domain: [-10, 10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(densityCurveData, {\n      x: \"x\",\n      y: \"y\",\n      z: \"group\",\n      stroke: \"#2a5599\",\n      strokeWidth: 1.5,\n      curve: \"basis\"\n    }),\n    Plot.line(crossSectionTrendLine, {x: \"x\", y: \"y\", stroke: \"black\", strokeWidth: 2})\n  ]\n});\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\nFigure 1.4: Cross-sections of the simple linear model normal error density.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#a-simple-linear-model-in-r",
    "href": "LinearModels.html#a-simple-linear-model-in-r",
    "title": "1  An introduction to linear statistical models",
    "section": "1.5 A simple Linear model in R",
    "text": "1.5 A simple Linear model in R\n\nfinish this section and lead into the next on fitting models to data.\nsimulate observations of Y from a specified linear model\n\n\nto begin, we start with a collection of X values (we might imagine we measure these values in the wild)\n\n\nn &lt;- 100\nX &lt;- runif(n=100, min=0, max= 50)\nhead(X)\n\n[1] 38.417800 34.430966  6.117309 21.941640 43.100729 18.519673\n\n\n\nThis code randomly chooses n = 100 values uniformy at random from the interval \\([0,50]\\).\n\n\nDefine the model\n\nNext, we construct our simple linear model\n\nbeta_0 &lt;- 4\nbeta_1 &lt;- 1.2\nsigma &lt;- 4\n\nsimple_linear_model &lt;- function(X, beta_0, beta_1, sigma) {\n  mu &lt;- beta_0 + (beta_1 * X) \n  mu + rnorm(length(X), mean = 0, sd = sigma)\n}\n\n\nThis is function takes X values and returns the specified linear function with normally distributed random noise added.\nNow we can simulate observations of \\(Y\\) given our list of \\(X\\) values and out linear model:\n\n\nY &lt;- simple_linear_model(X, beta_0, beta_1, sigma)\nhead(Y)\n\n[1] 46.81969 40.09232 19.85133 29.00861 57.38458 22.84689\n\n\nlets look at the joint distribution of X and Y:\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nFigure 1.5: Data generated from the simple linear model \\(Y=4+1.2\\times X + \\varepsilon\\), with \\(\\varepsilon\\sim N(0,16)\\). Dashed line shows E[Y|X] = β0 + β1X\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\ndf &lt;- data.frame(X = X, Y = Y)\n\nggplot(df, aes(X, Y)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(intercept = beta_0, slope = beta_1, linetype = \"dashed\") +\n  labs(x = \"X\", y = \"Y\") +\n  theme_minimal()\n\n\n\n\nHeres the same code running in webR - try adjusting some of the parameters (e.g. the number of samples) and running to plot a different random sample!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "SimpleLinearRegression.html",
    "href": "SimpleLinearRegression.html",
    "title": "2  Simple Linear Regression",
    "section": "",
    "text": "Fitting models to data\nin the previous section, you were given a linear model - we knew the values of \\(\\beta_0\\), \\(\\beta_1\\) and even \\(\\sigma\\). However, in most contexts we don’t know the true relationship between \\(X\\) and \\(Y\\) in advance. Instead, we are given data and try to infer the values of \\(\\beta\\) and \\(\\sigma\\) by analysing the data.\nIndeed, at the end of the last session we generated data from a linear model. In preparation for this chapter, I simulated data from a similar such linear model - your task here (and in regression generally) is to try an make a (educated) guess about what linear model (summarised by it’s values for \\(\\beta\\) and \\(\\sigma\\)) produced this data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "SimpleLinearRegression.html#fitting-models-to-data",
    "href": "SimpleLinearRegression.html#fitting-models-to-data",
    "title": "2  Simple Linear Regression",
    "section": "",
    "text": "Continue",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "SimpleLinearRegression.html#estimating-parameters",
    "href": "SimpleLinearRegression.html#estimating-parameters",
    "title": "2  Simple Linear Regression",
    "section": "2.1 Estimating parameters",
    "text": "2.1 Estimating parameters\n\nHats\nlets distinguish our estimates from the ‘true’, underlying, parameter values by giving them \\(\\hat{\\text{hats}}\\). so, we have\n\\[\n\\hat{\\beta_0}: \\text{Estimated intercept}\n\\] \\[\n\\hat{\\beta_1}: \\text{Estimated slope}\n\\] \\[\n\\hat{\\sigma}: \\text{Estimated standard deviation}.\n\\]\nWe also define \\(\\hat{Y}\\) as the predicted value of \\(Y\\) given our estimated model:\n\\[\n\\hat{Y}=\\hat{\\beta_0}+\\hat{\\beta_1}X\n\\] Looking at some data, we try to find the values of our parameters that best ‘fit’ its distribution. Adjust the sliders below to find a line that you think fits the data:\n\n\nviewof b0adj_1 = Inputs.range([-5, 5], {step: 1, label: html`${tex`\\hat{\\beta_0}`}: Estimated Intercept (asjust)`})\nviewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\\hat{\\beta_1}`}: Estimated Slope`})\n\nb0_1 = (-3 - 0.5*b1_1) + b0adj_1\n\ntex.block`\\hat{Y} = ${b0_1.toFixed(2)} + ${b1_1.toFixed(2)}x`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = d3.extent(linearData.x)\n\nlineData_1 = xRange.map(x =&gt; ({x, y: b1_1 * x + b0_1}))\n\nPlot.plot({\n  marks: [\n    Plot.line(lineData_1, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(transpose(linearData), { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [-10, 5], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\nWhat is your best estimate for the values of \\(\\beta_0\\) and \\(\\beta_1\\) that best fit the given data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOk great, so we have an estimate for our line-of-best-fit:\n\ntex.block`\\hat{Y} = ${my_beta_0_hat} + ${my_beta_1_hat}\\cdot X`\n\n\n\n\n\n\nbut what do we mean by ‘best’ fit here? How do we evaluate this estimate and maybe compare it to other estimates?\n\nContinue\n\n\n\nResiduals\nResiduals are the difference between observed values of \\(Y\\), and the value predicted by our estimated linear predictor \\(\\hat{Y}\\). We denote residuals with the letter \\(e\\): \\[\ne=Y-\\hat{Y} = Y - \\hat{\\beta_0}+\\hat{\\beta_1}X.\n\\]\nResiduals, \\(e\\), are similar to the errors, \\(\\varepsilon\\), that we encountered in chapter 1 - but they are distinct. In many situations, we do not know the ‘acutual’ values of \\(\\beta_0\\) an \\(\\beta_1\\) - so we cannot calculate the errors, \\(\\varepsilon= Y- E[Y]= Y-\\beta_0 + \\beta_1 X\\). However, we do have our estimates, \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\), so we can calculate residuals.\nIndeed - calculating residuals gives us a way to assess how well our estimated model fits the data. We can think of the residuals as being the ‘mismatch’ between our estimated linear predictor and each data point. By minimizing the size of residuals (minimisiong the mismatch of our model to the data), we can get a better fit of our line to data.\n\nContinue\n\n\n\nEstimating \\(\\beta\\) by minimising residuals\n\nhow low can the (squared) residuals go?\nBelow, the plot now also displays residuals for each data point. Underneath the model equation is the Sum of Squared Residuals (SSR), which gives a measure of the absolute difference between our linear predictor and the observed outcomes.\n\\[\nSSR=\\sum_{i=1}^{n} e_i^2\n\\]\nwhere \\(e_i = Y_i - \\hat{Y}_i\\). This gives us a numerical measure of how well the line fits the data - see how low you can get the SSR by adjusting slope and intercept.\n\n\nviewof b0adj = Inputs.range([-5, 5], {step: 1, label: html`${tex`\\hat{\\beta_0}`}: Estimated Intercept`})\nviewof b1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\\hat{\\beta_1}`}: Estimated Slope`})\n\nb0 = -3 + b0adj\n\nresidualData = transpose(linearData).map(d =&gt; {\n  const yhat = b1 * d.x + b0;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n  \nSSRes = d3.sum(residualData, d =&gt; d.residual ** 2)\n\ntex.block`\\hat{Y} = ${b1}X + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\sum_{i}r_i^{2} = ${SSRes.toFixed(2)}`\n\n\n\n\n\n\n\n\n\n\n\nlineData = xRange.map(x =&gt; ({x, y: b1 * x + b0}))\n\nPlot.plot({\n  marks: [\n    Plot.ruleX(residualData, {x: \"x\",\n      y1: \"y\",\n      y2: \"yhat\",\n      stroke: \"sign\", \n      strokeOpacity: 0.75,\n      strokeWidth: d =&gt; 1 + d.absRes}),\n\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(transpose(linearData), { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [-10,5], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\nBased on this visualisation, what do you think is the minimum possible \\(SSR\\) achievable?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want you can update your \\(\\hat{\\beta}\\) estimates too (otherwise just proceed):\n\n\n\n\n\n\n\n\n\n\n\nTechnical-point 1: Why squares?The least squares estimator chooses \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) so that the sum of squared residuals \\(\\sum_{i=1}^{n} e_i^2\\) is as small as possible. But why take squares? As mentioned, squaring numbers returns a positive result. Therefore the square gives some idea of the ‘absolute’ value of the residuals (and doesn’t allow negative residuals to cancel out positve ones). But why not just take the absolute value? Because the square function penalises large deviations heavily, least squares prefers lines that keep every point reasonably close to the fitted value rather than letting a few points drift far away.\n\n\n\n\n\nFitting a model using least squares\nWe call the estimates \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) which minimise the sum of squares the least squares estimates. Some nice mathematics tells us that the least squares esimates for a simple linear model are unique - that is, there is one set of values for \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) which satisfy this property. Moreover, we don’t have to manually adjust our parameters to keep lowering SSR - there is a convenient closed form expression which allows us to compute \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) very efficiently. As a statistical programming language, this is something R does very easily..\n\nThe lm() function\nIn R the lm() function computes the least squares estimates \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) for our simple linear model (among other things) in a single command:\n\n\n\n\n\n\n\n\nWe can extract the coefficients from the lm by indexing:\n\n\n\n\n\n\n\n\nor by the coef() function:\n\n\n\n\n\n\n\n\n\nHow do these estimates compare with yours?\n\nLets compare the estimated linear fits graphically:\n\n\n\n\n\n\n\n\n\nContinue\n\n\n\n\nEstimating \\(\\sigma\\)\nOnce the line has been fitted (i.e. \\(\\beta\\) has been estimated as \\(\\hat{\\beta}\\)), we also have to estimate the distribution of error terms (remember, as per ?sec-varAssumption, the error distribution has a constant value defined by \\(\\sigma\\)). As you might expect, we utilise the residuals from our least squares linear predictor to estimate sigma. The spread of the error terms by measuring the spread of the residuals around our line of best fit.\nWe can extract the individual residual values from the fitted lm object by indexing the lm object (e.g. my_lm$residuals) or with the residuals() function.\n\nExercise 3: Extracting residuals from an lm object\nExtract the residuals by indexing our least squares fitted model lm_1 and assign them to the variable e\n\n\n\n\n\n\n\n\n\n\n\ne &lt;- lm_1$residuals\ne &lt;- lm_1$residuals\n\n\n\n\n\n\nor, if desired we can calculate them ourselves:\nComplete the code below to calculate the residuals for the least-squares model:\n\n\n\n\n\n\n\n\n\n\n\n\nResidual Standard Error\nDividing the residual sum of squares by \\(n-2\\) (one degree of freedom lost per fitted coefficient) gives the mean squared error, and taking the square root yields the residual standard deviation (aka. the Residual-Standard-Error) \\[\n\\hat \\sigma = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^{n} r_i^2}.\n\\] In R output, this value is often called the Standard error. Interpreting \\(\\hat \\sigma\\) is often easier on the original \\(y\\) scale: a typical observation falls about \\(\\hat \\sigma\\) units away from the fitted line.\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "SimpleLinearRegression.html#checking-assumptions",
    "href": "SimpleLinearRegression.html#checking-assumptions",
    "title": "2  Simple Linear Regression",
    "section": "2.2 Checking assumptions",
    "text": "2.2 Checking assumptions\n\nIndependence of observations\n\nIn chapter 1, we layed out the assumptions we made as we constructed a simple linear model. The first and most foundational assumption we made was that our outcome \\(Y\\) only depended on the predictor \\(X\\) (along with random error). This means that no other variables in our dataset influence \\(Y\\), even through an influence on \\(X\\). Since we are dealing with the simple case of one predictor variable \\(X\\), it may seem that this assumtpion is garaunteed. However, a more subtle dependence may still be present: there may be some relationship between the observations themselves \\(X_i {{\\perp \\!\\!\\! \\perp} X_j\\) meaning that our observation \\(Y_i\\) is depends on more than just \\(X_i\\).\nlets look at an example of how this might arise:\n\n\nExample 1\nResearchers are interested in the relationship between IQ measurement and alcohol consumption. To this end they design a study wherein they visit a nearby pub over several days and measure individual’s IQ and the amount of alcohol they’ve consumed.\nHere is a plot of the data they collected:\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnonindependence_df |&gt;\n  ggplot() +\n  aes(x = IQ, y = intake) +\n  geom_point()\n\n\n\n\nFitting a simple linear model of IQ~Alcohol intake, would suggest that higher alcohol intake is associated with higher IQ scores. indeed, we can visualise a simple linear fit with the geom_smooth(method=\"lm\") function\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnonindependence_df |&gt;\n  ggplot() +\n  aes(x = IQ, y = intake) +\n  geom_point()+\n  geom_smooth(method=\"lm\", se=FALSE)\n\n\n\n\nwhich shows this positive trend. In some sense this is what the data shows, however it is not the whole story. Consider the following plot, with each observation coloured by participant:\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnonindependence_df |&gt;\n  ggplot() +\n  aes(x = IQ, y = intake, colour = participant) +\n  geom_point()\n\n\n\n\nWe can see that multiple observations (points) originate from the same participant - and therefore are likely to associated with one another. Moreover, if we focus on linear trends within each individual, our model(s) tell a different story:\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnonindependence_df |&gt;\n  ggplot() +\n  aes(x = IQ, y = intake, colour = participant) +\n  geom_point()+\n  geom_smooth(method=\"lm\", se=FALSE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "SimpleLinearRegression.html#inference-for-simple-linear-regression",
    "href": "SimpleLinearRegression.html#inference-for-simple-linear-regression",
    "title": "2  Simple Linear Regression",
    "section": "2.4 Inference for Simple Linear Regression",
    "text": "2.4 Inference for Simple Linear Regression\n\nConnection to simulation: We have been given data sampled from a linear modelâ€”consider whether the parameter estimates obtained so far represent the underlying model well.\nRepeat sampling experiment: Since we can simulate data, take another sample from the same model to explore variability in the estimates.\nComparison visuals: Overlay lines from multiple samples to build intuition for the sampling distribution of \\(\\hat\\beta\\).\nKey takeaway: Even with the same generating process, estimates vary; inference quantifies this uncertainty.\nsince we have the opportunity, lets take another sample from the same model:\n\n\n\n\n\n\n\n\n\nAnd fit a linear regression via least squares\n\n\n\n\n\n\n\n\nlets compare out two datasets (remember, these come from the same underlying linear model with \\(\\beta_0=-3\\) and \\(\\beta_1=0.5\\) and their lines of best fit:\n\n\n\n\n\n\n\n\n\nRepeating this process produces the sampling distribution of our parameters.\nin the real world we wont have the luxury of generating new samples by running R commands, so we have to make do with the data we have. Thankfully, if our assumptions of the underlying model are true then we can expect the same process of convergence to a stable sampling distribution to hold true and our parameter estimates for beta to have a t-distribution.\n\n\nInference about \\(\\beta\\)\n\nObjective: Derive sampling distributions for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) under the model assumptions.\nKey results: Emphasise unbiasedness, variance formulas, and the joint normality of the estimators.\nConfidence intervals: Outline the structure \\(\\hat{\\beta} \\pm t_{n-2,\\alpha/2} \\cdot \\operatorname{SE}(\\hat{\\beta})\\).\nHypothesis tests: Summarise how to test linear contrasts of the coefficients using t-statistics.\n\n\n\nInference about \\(\\sigma\\)\n\nDistributional result: With the assumptions above, the scaled residual sum of squares follows a \\(\\chi^2_{n-2}\\) distribution.\nConfidence interval: Show how this leads to bounds for \\(\\sigma\\) using chi-squared quantiles.\nHypothesis test: Note the form of tests comparing error variance claims.\nSoftware link: Point to the sigma output and degrees of freedom in R for practical calculation. ### Inference about \\(\\beta_1\\)\nHypothesis: To assess association, test \\(H_0: \\beta_1 = 0\\) versus an appropriate alternative.\nTest statistic: Introduce the t-statistic \\(t = \\frac{\\hat \\beta_1 - 0}{\\operatorname{SE}(\\hat \\beta_1)}\\) and its \\(t_{n-2}\\) reference distribution.\nSoftware output: Interpret R’s t-statistic, p-value, and confidence interval for \\(\\beta_1\\).\nPractical interpretation: Translate statistical significance into statements about direction and strength of association.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "SimpleLinearRegression.html#interpreting-summary-output",
    "href": "SimpleLinearRegression.html#interpreting-summary-output",
    "title": "2  Simple Linear Regression",
    "section": "2.5 Interpreting `summary()` output",
    "text": "2.5 Interpreting `summary()` output\n\nPurpose: summary() wraps the model fit, assumptions, and inference into a single report.\nCoefficients table: Highlight estimates, standard errors, t-values, and p-values for \\(\\beta_0\\) and \\(\\beta_1\\).\nResidual standard error: Connect this to \\(\\hat \\sigma\\) and the degrees of freedom shown in the output.\nModel fit metrics: Explain multiple \\(R^2\\) and adjusted \\(R^2\\) as measures of explained variation.\nOverall test: Describe how the F-statistic in simple regression aligns with the \\(\\beta_1\\) t-test.\nWorkflow tip: Encourage students to read the table line by line, linking each quantity back to the modelling steps above.\n\nInterpreting each component in context helps translate the statistical output into practical insight about the data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "SimpleLinearRegression.html#using-the-model-for-prediction",
    "href": "SimpleLinearRegression.html#using-the-model-for-prediction",
    "title": "2  Simple Linear Regression",
    "section": "2.6 Using the model for prediction",
    "text": "2.6 Using the model for prediction\n\nPrerequisite: Only use the model for prediction after diagnostics suggest the assumptions hold.\nPoint prediction: Use predict() to obtain fitted values for new \\(x\\).\nInterval estimates: Emphasise the difference between confidence intervals for the mean response and prediction intervals for individual outcomes.\nCommunicating uncertainty: Always report the uncertainty associated with predictions.\nScope of application: Warn about extrapolating beyond the observed range of \\(x\\) and discuss potential pitfalls.\n\ntest.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html",
    "href": "MultipleLinearRegression.html",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "",
    "text": "3.1 Session overview",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#session-overview",
    "href": "MultipleLinearRegression.html#session-overview",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "",
    "text": "Revisit the assumptions that underlie multiple linear regression and how they extend the simple linear case.\nMotivate the need for multiple predictors with short stories from real-world analyses (e.g., predicting house prices or patient outcomes).\nHighlight how model selection, diagnostics, and interpretation change as models become more complex.\nEncourage learners to connect ideas across chapters by comparing model summaries, visual diagnostics, and inferential statements.\n\n\nPotential exercises\n\nAsk learners to brainstorm scenarios where a single predictor is insufficient and list potential additional variables.\nProvide a model summary output and have learners identify which regression assumptions they would check first.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#continuous-predictors",
    "href": "MultipleLinearRegression.html#continuous-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "3.2 Continuous predictors",
    "text": "3.2 Continuous predictors\n\nKey topics to emphasise\n\nClarify the meaning of partial regression coefficients as “holding other variables constant.”\nDiscuss the role of centred and standardised predictors for interpretability and numerical stability.\nUse added-variable plots or component-plus-residual plots to visualise the unique contribution of each predictor.\n\n\n\nPotential examples\n\nFit a model predicting fuel efficiency (e.g., mtcars data) from engine size, weight, and horsepower to show competing effects.\nContrast models with raw vs. centred predictors to illustrate interpretational differences.\n\n\n\nPotential exercises\n\nProvide learners with a dataset (such as mtcars) and have them interpret the coefficient of weight before and after centring.\nAsk learners to build a scatterplot matrix to hypothesise relationships among continuous predictors before modelling.\n\n\n\nMulti-collinearity\n\nKey topics to emphasise\n\nDefine variance inflation factors (VIFs) and tolerance as diagnostic tools.\nExplain why multicollinearity inflates standard errors and complicates inference.\nHighlight remedial strategies: collecting more data, combining variables, or using regularisation methods.\n\n\n\nPotential examples\n\nDemonstrate high correlation between horsepower and displacement in the mtcars dataset and show the impact on coefficient estimates.\nCompare model outputs before and after removing a redundant predictor.\n\n\n\nPotential exercises\n\nProvide correlation matrices and ask learners to flag problematic pairs of predictors.\nHave learners compute VIFs for a fitted model and interpret which predictors require attention.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#categorical-predictors",
    "href": "MultipleLinearRegression.html#categorical-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "3.3 Categorical predictors",
    "text": "3.3 Categorical predictors\n\nKey topics to emphasise\n\nReview dummy (indicator) coding and how the choice of reference category affects interpretation.\nExplain how to include categorical predictors with more than two levels using treatment or sum coding.\nIntroduce the concept of adjusted means when controlling for other predictors.\n\n\n\nPotential examples\n\nModel exam scores using study hours (continuous) and teaching method (categorical) to illustrate contrasts.\nShow how to interpret coefficients when switching the reference category.\n\n\n\nPotential exercises\n\nAsk learners to encode a three-level categorical variable manually and verify with software output.\nProvide regression results and have learners translate coefficients into comparisons between categories.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#combining-predictors",
    "href": "MultipleLinearRegression.html#combining-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "3.4 Combining predictors",
    "text": "3.4 Combining predictors\n\nKey topics to emphasise\n\nFrame constructed terms (products, ratios, smooth functions) as tools to capture curvature or conditional relationships.\nStress the importance of centring predictors before forming interaction or polynomial terms to reduce multicollinearity.\nConnect constructed terms to model comparison using nested F-tests or information criteria.\n\n\n\nPotential examples\n\nCompare linear and quadratic fits for age vs. blood pressure to illustrate polynomial curvature.\nIntroduce interaction between advertising budget and market type to show varying slopes.\n\n\n\nPotential exercises\n\nHave learners expand a design matrix to include squared terms and interpret the resulting coefficients.\nAsk learners to perform an F-test comparing a model with and without interaction terms.\n\n\n\nPolynomials\n\nKey topics to emphasise\n\nEmphasise that polynomial terms model smooth curvature and require careful interpretation at different predictor levels.\nDemonstrate how polynomial degree affects model flexibility and risk of overfitting.\nInclude a note on the bias–variance trade-off to frame how adding higher-degree terms can reduce bias but increase variance.\nEncourage plotting predicted values to visualise curvature.\n\n\n\nPotential examples\n\nUse a dataset with age vs. lung capacity to illustrate diminishing returns via quadratic terms.\nContrast polynomial regression with piecewise linear fits to show alternative approaches.\n\n\n\nPotential exercises\n\nAsk learners to fit linear, quadratic, and cubic models and compare adjusted \\(R^2\\) values.\nProvide predicted curves and have learners match them to the appropriate polynomial degree.\n\n\n\n\nInteractions\n\nKey topics to emphasise\n\nInterpret interaction coefficients as changes in slopes across levels of another predictor.\nHighlight the necessity of plotting interactions (e.g., interaction plots, marginal effects).\nDiscuss the difference between interactions involving continuous vs. categorical variables.\nNote that interactions need not be simple products; they can combine predictors through indicator-triggered slopes, smooth-varying functions, or other structured contrasts.\n\n\n\nPotential examples\n\nFit a model predicting salary from years of experience, gender, and their interaction to discuss equity analyses.\nShow how to compute and interpret simple slopes at specific predictor values.\n\n\n\nPotential exercises\n\nHave learners calculate predicted outcomes at combinations of predictor values to interpret interaction effects.\nAsk learners to describe how an interaction would appear in a contour or surface plot.\n\n\n\n\nTransformation\n\nKey topics to emphasise\n\nExplain when transforming the response or predictors can stabilise variance or linearise relationships.\nCompare log, square-root, and Box–Cox transformations and their interpretational consequences.\nEncourage residual diagnostics to confirm whether transformations improved model fit.\n\n\n\nPotential examples\n\nTransform skewed income data with a log transformation and interpret coefficients in percentage terms.\nIllustrate how transforming predictors (e.g., log-weight) can linearise nonlinear relationships with the response.\n\n\n\nPotential exercises\n\nProvide residual plots before and after a transformation for learners to evaluate improvement.\nAssign a short activity where learners propose appropriate transformations based on distributional summaries.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "RegressionCaseStudy.html",
    "href": "RegressionCaseStudy.html",
    "title": "4  Regression case study",
    "section": "",
    "text": "Exercise 1a new test",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression case study</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "5  Glossary",
    "section": "",
    "text": "Term  Definition",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "SimpleLinearRegression.html#linearity",
    "href": "SimpleLinearRegression.html#linearity",
    "title": "2  Simple Linear Regression",
    "section": "2.3 ### Linearity",
    "text": "2.3 ### Linearity\n\nWhat it means: The expected response should change linearly with the predictor.\nDiagnostics: Scatterplots of \\((x_i, y_i)\\) and residual-versus-fitted plots are useful checks.\nWarning signs: Curved patterns or systematic structure in these plots suggest the linear form is inadequate.\nPossible remedies: Try transforming variables, adding polynomial terms, or introducing additional predictors.\n\n\nConstant Variance\n\nWhat it means: Also called homoscedasticity, the variability of residuals should stay roughly constant across fitted values.\nDiagnostics: Residual-versus-fitted plots should show an even vertical spread.\nWarning signs: Funnel or fan shapes (narrow for small \\(\\hat y\\) but wide for large \\(\\hat y\\)) reveal heteroscedasticity.\nPossible remedies: Transform the response, use weighted least squares, or model the variance explicitly.\n\n\n\nNormality of error\n\nWhat it means: Least squares inference (confidence intervals, hypothesis tests) assumes residuals are approximately normal.\nDiagnostics: Use a normal Q-Q plot to compare residual quantiles with the theoretical normal line.\nWarning signs: Systematic bends or heavy tail departures from the diagonal indicate skewness or heavy tails.\nPossible remedies: Transform the response, refit with robust methods, or collect more data to stabilise the distribution.\n\n\n\nExamples of assumption violations\n\nSimulated example: In this case the assumptions holdâ€”as expected because the data were sampled from an actual linear model.\nReal-world contrasts: Add case studies where independence fails (e.g., repeated measures), the relationship is curved, the variance changes, or the residuals are non-normal.\nTeaching tip: For each violation, include visuals showing the problematic residual pattern and a suggested fix.\n\n\nExample 2\nLinearity\nIf the true relationship between the predictor and response is curved, the fitted straight line misses systematic structure. Here a quadratic mean trend leaves a clear arc in the scatterplot and in the dashed least-squares fit.\n\n\n\n\n\n\n\n\n\n\n\nExample 3\nConstant variance\nWhen variability grows with the fitted value, the residuals form a funnel shape. In this simulated data the spread of the errors increases with the predictor, so the residual-versus-fitted plot fans out instead of forming an even band.\n\n\n\n\n\n\n\n\n\n\n\nExample 4\nNormality\nHeavy-tailed errors lead to extreme residuals that stray from the reference line on a Q-Q plot. The simulated model below uses Student-\\(t\\) noise with only two degrees of freedom, producing thicker tails than the normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  }
]