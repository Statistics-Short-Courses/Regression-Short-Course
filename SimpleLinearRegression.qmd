---
format: 
  live-html:
    toc: true

execute:
  echo: true
  warning: false
  message: false

embed-resources: true

filters:
  - custom-numbered-blocks

custom-numbered-blocks:
  classes:
    Assumption:
      colors: [FFCDD2, F44336]
      boxstyle: foldbox.simple
      collapse: false
    Example:
      colors: [BBDEFB, 2196F3]
      boxstyle: foldbox.simple
      collapse: false
    Exercise:
      colors: [C8E6C9, 4CAF50]
      boxstyle: foldbox.simple
      collapse: false
    Technical-point:
      colors: [FFF9C4, FFEB3B]
      boxstyle: foldbox.simple
      collapse: true
webr:
  render-df: gt-interactive
---
{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}
{{< include ./_extensions/r-wasm/live/_gradethis.qmd >}}

```{webr}
#| include: false
library(tidyverse)
set_theme(theme_bw())
```

```{r}
#| include: false
library(tidyverse)
set_theme(theme_bw())
```

```{webr}
#| include: false
#| define:
#|  - linearData
n <- 20
x <- rnorm(n,0,4)
y <- 0.5 * x - 3 + rnorm(n,0, 2)

linearData <- data.frame(x,y)
```

# Simple Linear Regression
##  Fitting models to data {.unnumbered}
in the previous section, you were given a linear model - we knew the values of $\beta_0$, $\beta_1$ and even $\sigma$. However, in most contexts we don't know the true relationship between $X$ and $Y$ in advance. Instead, we are given *data* and try to *infer* the values of $\beta$ and $\sigma$ by analysing the data.

Indeed, at the end of the last session we generated data from a linear model. In preparation for this chapter, we've generated data from a similar such linear model. Here it is:

::: {.panel-tabset}
### Plot
```{webr}
#| echo: false
ggplot(linearData)+
  aes(x=x, y=y)+
  geom_point()
```

### Data

```{webr}
#| echo: false
linearData
```
:::

 your task for the remainder of the chapter (and in regression generally) is to try an make a (educated) guess about what model produced this data. 

- [ ] Continue

## Estimating parameters

We call the process of trying to guess the parameters in the data generating model (i.e. $\beta$ and $\sigma$), *estimation*, and our guesses are *estimates*. To avoid confusion, we distinguish our estimates from the 'true' parameter values by giving them $\hat{\text{hats}}$. So, we have

$$
\hat{\beta_0}: \text{Estimated intercept}
$$
$$
\hat{\beta_1}: \text{Estimated slope}
$$
$$
\hat{\sigma}: \text{Estimated standard deviation}.
$$
We also define $\hat{Y}$ as the predicted value of $Y$ given our estimated model:
$$
\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X
$$
Looking at some data, we try to find the values of our parameters that best 'fit' its distribution. Adjust the sliders below to find a line that you think fits the data:

```{ojs}
//| panel: sidebar
//| echo: false
viewof b0adj_1 = Inputs.range([-5, 5], {step: 1, label: html`${tex`\hat{\beta_0}`}: Estimated Intercept (asjust)`})
viewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\hat{\beta_1}`}: Estimated Slope`})

b0_1 = (-3 - 0.5*b1_1) + b0adj_1

tex.block`\hat{Y} = ${b0_1.toFixed(2)} + ${b1_1.toFixed(2)}x`
```

```{ojs}
//| panel: fill
//| echo: false

xRange = d3.extent(linearData.x)

lineData_1 = xRange.map(x => ({x, y: b1_1 * x + b0_1}))

Plot.plot({
  marks: [
    Plot.line(lineData_1, { x: "x", y: "y" }),

    Plot.dot(transpose(linearData), { x: "x", y: "y", r: 3 })
  ],
  x: { domain: xRange },
  y: { domain: [-10, 5], label: "y" },
})
```

::: Exercise
What is your best estimate for the values of $\beta_0$ and $\beta_1$ that best fit the given data?

```{webr}
#| exercise: ex_2.1
#| envir: Ex2
#| define:
#|   - my_beta_0_hat
#|   - my_beta_1_hat
my_beta_0_hat <- ______
my_beta_1_hat <- ______
```
:::: {.solution exercise="ex_1"}
```{webr}
#| exercise: ex_2.1
#| solution: true
```
::::
```{webr}
#| exercise: ex_2.1
#| envir: Ex2
#| check: true
#| class: wait
grade_this({
  if (is.numeric(get("my_beta_0_hat", envir = .envir_result)) && is.numeric(get("my_beta_1_hat", envir = .envir_result))){pass("Good guess!")}
  fail("both \\$\\beta_1\\$ and beta_0 should be numbers")
})
```
:::

###

Ok great, so we have an estimate for our line-of-best-fit:
```{ojs}
//| echo: false
tex.block`\hat{Y} = ${my_beta_0_hat} + ${my_beta_1_hat}\cdot X`
```
but what do we mean by 'best' fit here? How do we evaluate this estimate and maybe compare it to other estimates?

-   [ ] Continue

### Residuals

Residuals are the difference between observed values of $Y$, and the value predicted by our estimated linear predictor $\hat{Y}$. We denote residuals with the letter $e$:
$$
e=Y-\hat{Y} = Y - \hat{\beta_0}+\hat{\beta_1}X. 
$$

Residuals, $e$, are similar to the errors, $\varepsilon$, that we encountered in chapter 1 - but they are distinct. In many situations, we do not know the 'acutual' values of $\beta_0$ an $\beta_1$ - so we cannot calculate the errors, $\varepsilon= Y- E[Y]= Y-\beta_0 + \beta_1 X$. However, we do  have our estimates, $\hat{\beta_0}$ and $\hat{\beta_1}$, so we can calculate residuals. 

Indeed - calculating residuals gives us a way to assess how well our estimated model fits the data.  We can think of the residuals as being the 'mismatch' between our estimated linear predictor and each data point. By *minimizing* the size of residuals (minimisiong the mismatch of our model to the data), we can get a better fit of our line to data.

- [ ] Continue 

### Estimating $\beta$ by minimising residuals

#### how low can the (squared) residuals go?

Below, the plot now also displays residuals for each data point.
Underneath the model equation is the *Sum of Squared Residuals* (SSR), which gives a measure of the absolute difference between our linear predictor and the observed outcomes. 

$$
SSR=\sum_{i=1}^{n} e_i^2
$$

where $e_i = Y_i - \hat{Y}_i$.  This gives us a numerical measure of how well the line fits the data - see how low you can get the SSR by adjusting slope and intercept.


```{ojs}
//| panel: sidebar
//| echo: false
viewof b0adj = Inputs.range([-5, 5], {step: 1, label: html`${tex`\hat{\beta_0}`}: Estimated Intercept`})
viewof b1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\hat{\beta_1}`}: Estimated Slope`})

b0 = -3 + b0adj

residualData = transpose(linearData).map(d => {
  const yhat = b1 * d.x + b0;
  const res  = d.y - yhat;
  return {
    ...d,
    yhat,
    residual: res,
    sign: res >= 0 ? "positive" : "negative",
    absRes: Math.abs(res)
  };
})
  
SSRes = d3.sum(residualData, d => d.residual ** 2)

tex.block`\hat{Y} = ${b1}X + ${b0}`

tex.block`\sum_{i}r_i^{2} = ${SSRes.toFixed(2)}`
```

```{ojs}
//| panel: fill
//| echo: false
lineData = xRange.map(x => ({x, y: b1 * x + b0}))

Plot.plot({
  marks: [
    Plot.ruleX(residualData, {x: "x",
      y1: "y",
      y2: "yhat",
      stroke: "sign", 
      strokeOpacity: 0.75,
      strokeWidth: d => 1 + d.absRes}),

    Plot.line(lineData, { x: "x", y: "y" }),

    Plot.dot(transpose(linearData), { x: "x", y: "y", r: 3 })
  ],
  x: { domain: xRange },
  y: { domain: [-10,5], label: "y" },
})
```

::: Exercise
Based on this visualisation, what do you think is the minimum possible $SSR$ achievable?
```{webr}
#| exercise: Ex_2.2
#| envir: Ex2
#| define:
#|   - SSR
SSR <- ________
```
:::: {.solution exercise="Ex_2.2"}
```{webr}
#| exercise: Ex_2.2
#| solution: true
```
::::
```{webr}
#| exercise: Ex_2.2
#| check: true
#| class: wait
grade_this({
  if (is.numeric(get("SSR", envir = .envir_result))){
    pass("Good guess!")}
  fail("SSR should be a number!")
})
```
If you want you can update your $\hat{\beta}$ estimates too (otherwise just proceed):

```{webr}
#| exercise: ex_2.3
#| envir: Ex2
my_beta_0_hat <- ______

my_beta_1_hat <- ______
```

:::
::: Technical-point
#### Why squares?
The least squares estimator chooses $\hat \beta_0$ and $\hat \beta_1$ so that the sum of squared residuals $\sum_{i=1}^{n} e_i^2$ is as small as possible. But why take squares?
As mentioned, squaring numbers returns a positive result. Therefore the square gives some idea of the 'absolute' value of the residuals (and doesn't allow negative residuals to cancel out positve ones). But why not just take the absolute value?
Because the square function penalises large deviations heavily, least squares prefers lines that keep every point reasonably close to the fitted value rather than letting a few points drift far away.
:::

### Fitting a model using least squares

We call the estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ which minimise the sum of squares the *least squares estimates*. Some nice mathematics tells us that the least squares esimates for a simple linear model are *unique* - that is, there is one set of values for $\hat{\beta_0}$ and $\hat{\beta_1}$ which satisfy this property. Moreover, we don't have to manually adjust our parameters to keep lowering SSR - there is a convenient closed form expression which allows us to compute $\hat{\beta_0}$ and $\hat{\beta_1}$ very efficiently. As a statistical programming language, this is something R does very easily..


#### The `lm()` function

In R the `lm()` function computes the least squares estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ for our simple linear model (among other things) in a single command: 

```{webr}
#| input:
#|   - linearData
lm_1 <- lm(y~x,linearData)
```
We can  extract the coefficients from the `lm` by indexing:

```{webr}
lm_1$coefficients
```
or by the `coef()` function:

```{webr}
coef(lm_1)
```
-  [ ] How do these estimates compare with yours? 

Lets compare the estimated linear fits graphically:

::: {.panel-tabset}
### Plot
```{webr}
#| exercise: comparePlot
#| envir: Ex2
#| echo: false
#| edit: false
ggplot(linearData)+
  aes(x=x,y=y)+
  geom_point()+
  geom_abline( # the least squares estimate
    aes(intercept = lm_1$coefficients[1],
    slope = lm_1$coefficients[2],
    colour = "Least squares fit")) +
  geom_abline( # Your guess
    aes(intercept = my_beta_0_hat,
    slope = my_beta_1_hat,
    colour = "Your guess"))+
  scale_colour_manual(values= c("Least squares fit"="#4CAF50","Your guess"='#2196F3'))
```
### Code
```{webr}
#| eval: false
ggplot(linearData)+
  aes(x=x,y=y)+
  geom_point()+
  geom_abline( # the least squares estimate
    aes(intercept = lm_1$coefficients[1],
    slope = lm_1$coefficients[2],
    colour = "Least squares fit")) +
  geom_abline( # Your guess
    aes(intercept = my_beta_0_hat,
    slope = my_beta_1_hat,
    colour = "Your guess"))+
  scale_colour_manual(values= c("Least squares fit"="#4CAF50","Your guess"='#2196F3'))
```
:::
-   [ ] Continue


### Estimating $\sigma$

Once the line has been fitted (i.e. $\beta$ has been estimated as $\hat{\beta}$), we also have to estimate the distribution of error terms (remember, as per @sec-varAssumption, the error distribution has a constant variance defined by $\sigma^2$). As you might expect, we again utilise the residuals from our least squares linear predictor to estimate $sigma$.  The spread of the error terms will be estimated by the spread of the residuals around our line of best fit.

We can extract the individual residual values from the fitted `lm` object by indexing (e.g. `my_lm$residuals`) or with the `residuals()` function.

::: Exercise
#### Extracting residuals from an `lm` object
Extract the residuals by indexing our least squares fitted model `lm_1` and assign them to the variable `e`

```{webr}
#| exercise: ex_2.2.1
#| envir: ex_2.2
e <- ______
```
:::: {.solution exercise="ex_2.2.1"}
#### Solution 
```{webr}
#| exercise: ex_2.2.1
#| solution: true
#| envir: ex_2.2
e <- lm_1$residuals
```
::::

```{webr}
#| exercise: ex_2.2.1
#| envir: ex_2.2
#| check: true
#| class: wait
grade_this_code()
```
:::

### Residual Standard Error

Dividing the residual sum of squares by $n-2$ (one degree of freedom lost per fitted coefficient) gives the mean squared error, and taking the square root yields the residual standard deviation (aka. the {{< glossary Residual-Standard-Error >}})
$$
\hat \sigma = \sqrt{\frac{1}{n-2}\sum_{i=1}^{n} r_i^2}.
$$
Interpreting $\hat \sigma$ is often easier on the original $y$ scale: a typical observation falls about $\hat \sigma$ units away from the fitted line.

::: Exercise
#### Calculate the Residual Standard Error
using your `e` object defined in the previous exercise, calculate the residual standard error of the least squares model (hint - there are 20 observations in the linearData dataset)
```{webr}
#| exercise: ex_2.2.3
#| envir: ex_2.2
rse <- ______
```
:::: {.solution exercise="ex_2.2.1"}
#### Solution 
```{webr}
#| exercise: ex_2.2.3
#| solution: true
#| envir: ex_2.2
rse <- sqrt(sum(e^2)/(nrow(linearData)-2))
```
::::

```{webr}
#| exercise: ex_2.2.1
#| envir: ex_2.2.1
#| check: true
#| class: wait
grade_this_code()
```
:::

- [ ] Continue

## Checking assumptions 

### Independence of observations
- In @sec-chap1, we layed out the assumptions we made as we constructed a simple linear model. The first and most foundational assumption we made was that our outcome $Y$ only depended on the predictor $X$ (along with random error). This means that no other variables in our dataset influence $Y$, even through an influence on $X$. Since we are dealing with the simple case of one predictor variable $X$, it may seem that this assumtpion is garaunteed. However, a more subtle dependence may still be present: there may be some relationship between the observations themselves $X_i {{\perp \!\!\! \perp} X_j$ meaning that our observation $Y_i$ is depends on more than just $X_i$. 

- lets look at an example of how this might arise:

::: Example
Researchers are interested in the relationship between IQ measurement and alcohol consumption. To this end they design a study wherein they visit a nearby pub over several days and measure individual's IQ and the amount of alcohol they've consumed. 

```{r}
#| include: false
n_part <- 5
n_obs <- 10
mu_00 <- 100
mu_01 <- 5 #between-person effect
mu_10 <- -2 #withinperson effect

nonindependence_df <- tibble(
  participant = c("Anne", "John", "Laura", "Mary", "Steve"),
  intake_mean = seq(from = 1, to = 10, by = 2),
  u0 = rnorm(n_part, 0, 2)
) |>
  mutate(
    IQ_baseline = mu_00 + mu_01 * intake_mean + u0,
    β_within = mu_10
  )|>
  mutate(obs = list(1:n_obs)) |>
  unnest(obs) |>
  mutate(
    intake = intake_mean + rnorm(n_obs, 0, 2), 
    dev_intake = intake - intake_mean,
    IQ = IQ_baseline + β_within * dev_intake + rnorm(n(), 0, 3)
  )
```
Here is a plot of the data they collected:

::: {.panel-tabset}
## Plot
```{r}
#| echo: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake) +
  geom_point()
```
## Code

```{r}
#| eval: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake) +
  geom_point()
```
:::
Fitting a simple linear model of IQ~Alcohol intake, would suggest that higher alcohol intake is associated with higher IQ scores. indeed, we can visualise a simple linear fit with the `geom_smooth(method="lm")` function

::: {.panel-tabset}
## Plot
```{r}
#| echo: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake) +
  geom_point()+
  geom_smooth(method="lm", se=FALSE)
```
## Code

```{r}
#| eval: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake) +
  geom_point()+
  geom_smooth(method="lm", se=FALSE)
```
:::
which shows this positive trend. In some sense this is what the data shows, however it is not the whole story. Consider the following plot, with each observation coloured by participant:

::: {.panel-tabset}
## Plot
```{r}
#| echo: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake, colour = participant) +
  geom_point()
```
## Code

```{r}
#| eval: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake, colour = participant) +
  geom_point()
```
:::

We can see that multiple observations (points) originate from the same participant - and therefore are likely to associated with one another. Moreover, if we focus on linear trends within each individual, our model(s) tell a different story:

::: {.panel-tabset}
## Plot
```{r}
#| echo: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake, colour = participant) +
  geom_point()+
  geom_smooth(method="lm", se=FALSE)
```
## Code

```{r}
#| eval: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake, colour = participant) +
  geom_point()+
  geom_smooth(method="lm", se=FALSE)
```
:::
:::

### Linearity 
-  in @assumption-linearity
-   **What it means:** The expected response should change linearly with the predictor.
-   **Diagnostics:** Scatterplots of $(x_i, y_i)$ and residual-versus-fitted plots are useful checks.
-   **Warning signs:** Curved patterns or systematic structure in these plots suggest the linear form is inadequate.
-   **Possible remedies:** Try transforming variables, adding polynomial terms, or introducing additional predictors.

### Constant Variance
-   **What it means:** Also called homoscedasticity, the variability of residuals should stay roughly constant across fitted values.
-   **Diagnostics:** Residual-versus-fitted plots should show an even vertical spread.
-   **Warning signs:** Funnel or fan shapes (narrow for small $\hat y$ but wide for large $\hat y$) reveal heteroscedasticity.
-   **Possible remedies:** Transform the response, use weighted least squares, or model the variance explicitly.

### Normality of error
-   **What it means:** Least squares inference (confidence intervals, hypothesis tests) assumes residuals are approximately normal.
-   **Diagnostics:** Use a normal Q-Q plot to compare residual quantiles with the theoretical normal line.
-   **Warning signs:** Systematic bends or heavy tail departures from the diagonal indicate skewness or heavy tails.
-   **Possible remedies:** Transform the response, refit with robust methods, or collect more data to stabilise the distribution.

### Examples of assumption violations
-   **Simulated example:** In this case the assumptions holdâ€”as expected because the data were sampled from an actual linear model.
-   **Real-world contrasts:** Add case studies where independence fails (e.g., repeated measures), the relationship is curved, the variance changes, or the residuals are non-normal.
-   **Teaching tip:** For each violation, include visuals showing the problematic residual pattern and a suggested fix.

::: Example
**Linearity**

If the true relationship between the predictor and response is curved, the fitted straight line misses systematic structure. Here a quadratic mean trend leaves a clear arc in the scatterplot and in the dashed least-squares fit.

```{webr}
set.seed(2024)
nonlinear_example <- tibble(
  dosage = seq(0, 10, length.out = 60),
  response = 5 + 1.4 * dosage - 0.18 * dosage^2 + rnorm(60, 0, 1.2)
)

ggplot(nonlinear_example, aes(dosage, response)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "#d73027", linetype = "dashed") +
  geom_smooth(method = "loess", se = FALSE, colour = "#4575b4") +
  labs(x = "Dosage (mg)", y = "Response score") +
  theme(legend.position = "none")
```
:::

::: Example
**Constant variance**

When variability grows with the fitted value, the residuals form a funnel shape. In this simulated data the spread of the errors increases with the predictor, so the residual-versus-fitted plot fans out instead of forming an even band.

```{webr}
set.seed(2024)
hetero_example <- tibble(
  x = seq(0, 10, length.out = 80),
  y = 2 + 0.5 * x + rnorm(80, 0, 0.4 + 0.25 * x)
)

hetero_model <- lm(y ~ x, data = hetero_example)
hetero_diag <- hetero_example |>
  mutate(
    fitted = fitted(hetero_model),
    residuals = resid(hetero_model)
  )

ggplot(hetero_diag, aes(fitted, residuals)) +
  geom_hline(yintercept = 0, colour = "grey70") +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, colour = "#542788") +
  labs(x = "Fitted value", y = "Residual")
```
:::

::: Example
**Normality**

Heavy-tailed errors lead to extreme residuals that stray from the reference line on a Q-Q plot. The simulated model below uses Student-$t$ noise with only two degrees of freedom, producing thicker tails than the normal distribution.

```{webr}
set.seed(2024)
heavy_tail_example <- tibble(
  hours = runif(100, 0, 8),
  satisfaction = 4 + 0.8 * hours + rt(100, df = 2)
)

heavy_tail_model <- lm(satisfaction ~ hours, data = heavy_tail_example)
heavy_tail_resid <- tibble(residuals = resid(heavy_tail_model))

ggplot(heavy_tail_resid, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(colour = "#1b9e77") +
  labs(x = "Theoretical quantiles", y = "Sample quantiles")
```
:::


## Inference for Simple Linear Regression


-   **Connection to simulation:** We have been given data sampled from a linear modelâ€”consider whether the parameter estimates obtained so far represent the underlying model well.
-   **Repeat sampling experiment:** Since we can simulate data, take another sample from the same model to explore variability in the estimates.
-   **Comparison visuals:** Overlay lines from multiple samples to build intuition for the sampling distribution of $\hat\beta$.
-   **Key takeaway:** Even with the same generating process, estimates vary; inference quantifies this uncertainty.

- since we have the opportunity, lets take another sample from the same model:

```{webr}
n <- 20
x <- rnorm(n,0,4)
error <-  rnorm(n,0, 2)
y <- 0.5 * x - 3 + error
linearData_new <- data.frame(x,y)
```

And fit a linear regression via least squares
```{webr}
lm_2 <- lm(y~x,linearData_new)
lm
```
lets compare out two datasets (remember, these come from the same underlying linear model with $\beta_0=-3$ and $\beta_1=0.5$ and their lines of best fit:
```{webr}
ggplot()+
  geom_point(data=linearData, aes())
```
- Repeating this process produces the sampling distribution of our parameters.

- in the real world we wont have the luxury of generating new samples by running R commands, so we have to make do with the data we have. Thankfully, if our assumptions of the underlying model are true then we can expect the same process of convergence to a stable sampling distribution to hold true and our parameter estimates for beta to have a t-distribution.

### Inference about $\beta$

-   **Objective:** Derive sampling distributions for $\hat{\beta}_0$ and $\hat{\beta}_1$ under the model assumptions.
-   **Key results:** Emphasise unbiasedness, variance formulas, and the joint normality of the estimators.
-   **Confidence intervals:** Outline the structure $\hat{\beta} \pm t_{n-2,\alpha/2} \cdot \operatorname{SE}(\hat{\beta})$.
-   **Hypothesis tests:** Summarise how to test linear contrasts of the coefficients using t-statistics.

### Inference about $\sigma$

-   **Distributional result:** With the assumptions above, the scaled residual sum of squares follows a $\chi^2_{n-2}$ distribution.
-   **Confidence interval:** Show how this leads to bounds for $\sigma$ using chi-squared quantiles.
-   **Hypothesis test:** Note the form of tests comparing error variance claims.
-   **Software link:** Point to the `sigma` output and degrees of freedom in R for practical calculation.
### Inference about $\beta_1$

-   **Hypothesis:** To assess association, test $H_0: \beta_1 = 0$ versus an appropriate alternative.
-   **Test statistic:** Introduce the t-statistic $t = \frac{\hat \beta_1 - 0}{\operatorname{SE}(\hat \beta_1)}$ and its $t_{n-2}$ reference distribution.
-   **Software output:** Interpret R's t-statistic, p-value, and confidence interval for $\beta_1$.
-   **Practical interpretation:** Translate statistical significance into statements about direction and strength of association.

## Interpreting \`summary()\` output

-   **Purpose:** `summary()` wraps the model fit, assumptions, and inference into a single report.
-   **Coefficients table:** Highlight estimates, standard errors, t-values, and p-values for $\beta_0$ and $\beta_1$.
-   **Residual standard error:** Connect this to $\hat \sigma$ and the degrees of freedom shown in the output.
-   **Model fit metrics:** Explain multiple $R^2$ and adjusted $R^2$ as measures of explained variation.
-   **Overall test:** Describe how the F-statistic in simple regression aligns with the $\beta_1$ t-test.
-   **Workflow tip:** Encourage students to read the table line by line, linking each quantity back to the modelling steps above.

Interpreting each component in context helps translate the statistical output into practical insight about the data.

## Using the model for prediction

-   **Prerequisite:** Only use the model for prediction after diagnostics suggest the assumptions hold.
-   **Point prediction:** Use `predict()` to obtain fitted values for new $x$.
-   **Interval estimates:** Emphasise the difference between confidence intervals for the mean response and prediction intervals for individual outcomes.
-   **Communicating uncertainty:** Always report the uncertainty associated with predictions.
-   **Scope of application:** Warn about extrapolating beyond the observed range of $x$ and discuss potential pitfalls.

test. 


## possible exercises
- calculate residuals manually from beta coefficients 