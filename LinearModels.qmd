---
format: 
  live-html:
    toc: true

execute:
  echo: true
  warning: false
  message: false

editor: 
  markdown: 
    wrap: 72
    
embed-resources: true

custom-numbered-blocks:
  classes:
    Assumption: default
    Example: default
    
filters:
- custom-numbered-blocks
---

{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}

```{r}
#| include: false
library(ggplot2)
```


# An introduction to linear statistical models

-   A primary goal of statistical modelling is to characterise the relationship between variables, for now lets call them $X$ and $Y$.
-   In this context, we designate one variable, $Y$ as the *outcome* or *dependent variable*, and attempt to construct a *model* describing how it depends on the other variable(s), $X$, which we call *predictors* or *independent variables*. 
-   We can express the idea that $Y$ depends on $X$ mathematically as
$$
Y=f(X)
$$
-   That is, $Y$ is some function of $X$ (we have not yet specified what kind of function). Given values of $X$, we can use this function to *predict* or *explain* corresponding values of $Y$.

- This is similar to *deterministic models* you may know from physics - for example 
 $$E=MC^2$$,
 where $E$ (the outcome), depends on $M$ (a predictor) through the function $f(M)=MC^2$). 
 
- In statistical modelling we take another step and assume that $Y$ is not a purely determinsitic function of $X$, but that it varies somewhat randomly around such a relationship. We capture this by introducing a *random error term *, $\varepsilon$:
$$
Y=f(X)+\varepsilon
$$

Here $\varepsilon$ represents the random variation in $Y$ that is not explained by $X$. 

-This means that $Y$ is not a perfectly determined by of $X$: even if we know the values of X, the outcome Y can still vary due to random noise. 

::: Assumption 
$Y$ depends only on a deterministic function of $X$ and a random noise component
:::

## A simple linear model

### Linear prediction

-   We choose $f(X)$ to be a linear function

    -   $$
        f(X)= \beta_0 + \beta_1\cdot X
        $$



-   putting aside the error term for now, the approximate value - lets
    call it the 'expected value' of Y, $E[Y]$, for now - can be
    represented by a linear equation,

    -   $$
        E[Y]= \beta_0 + \beta_1\cdot X
        $$

-   or alternatively as a line with y-intercept $\beta_0$ and slope
    $\beta_1$:


```{ojs}
//| panel: sidebar
//| echo: false

viewof b1 = Inputs.range([-2, 2], {step: 0.1, label: "slope"})
viewof b0 = Inputs.range([-10, 10], {step: 1, label: "intercept"})

tex.block`Y = ${b1}X + ${b0}`
```

```{ojs}
//| panel: fill
//| echo: false

xRange = [-10,10]
lineData = xRange.map(x => ({x, y: (b1 * x) + b0}))

Plot.plot({
  x:{domain: [-10,10], label: "X", grid: true},
  y:{domain: [-10,10], label: "Y", grid: true},
  marks: [
    Plot.line(lineData, { x: "x", y: "y" })]
})
```

-   That is, $\beta_0$ tells us what Y looks like when $X=0$, and
    $\beta_1$ tells us how much Y changes when X increases by 1.
    
::: Assumption 
Y and X have a linear relationship
:::

::: Example
#### Salary growth over time

You've been offered a job at ~company A~, and want to predict your salary at this company after working there for 10 years. 
We know that, on average, the starting salary at this company is \$50,000 and salaries increase by \$5000 each year someone stays employed there. 

Using this information, we can construct a simple linear model of salary from employment time. Given an employment time of $X=x$ years, we can represent a employee's expected salary ($E[Y]$) mathematically as

$$
E[Y]= \$50,000 + \$5,000\cdot x.
$$

When we have been working at this company for 10 years (i.e. $X=10$), we therefore expect a salary of $E[Y]=\$50,000 + \$5,000\cdot 10= \$100,000$
:::

::: {.webex-check .webex-box}
#### Your turn 

A second company also has a job available. Base salary on start is higher here at \$70,000 on average, but payrises are less  - you are told that on average employees working for the company for 5 years have \$20,000 higher salary than when they started. 
You want to use a simple linear model to calculate your expected salary ($E[Y]$) at company 2 after $X$ years of employment. 

$$
E[Y] = \beta_0 + \beta_1 X
$$

1. What are the coefficients $\beta_0$ and $beta_1$ in this case?


```{r}
#| echo: false
#| results: 'asis'
library(webexercises)
beta0_quiz <- fitb(answer= 70000, num=TRUE)
beta1_quiz <- fitb(answer=4000, num=TRUE)


# When using `{r} ...` language-bracketed inline code, functions like `mqc()` and `torf()` need to be enclosed by `I()`. This tells knitr to treat the output as markdown, thereby avoiding any character escaping of HTML code these functions generate.
```

$\beta_0 =$`{r} I(beta0_quiz)`
$\beta_1 =$`{r} I(beta1_quiz)`

Using R, calculate the expected salary after working for this company for 10 years

```{webr}
#| exercise: example_2

```

```{webr}
#| exercise: example_2
#| check: true
if (identical(.result, 70000+(4000*10))) {
  list(correct = TRUE, message = "Nice work!")
} else {
  list(correct = FALSE, message = "That's incorrect, sorry.")
}
```
2. complete the r function that predicts Y from an input X value for this linear model
```{webr}
#| exercise: r_function
#| envir: linear_predictor_function
linear_prediction <- function(X){__ + (__*__)}
```

```{webr}
#| exercise: example_2
#| check: true
if (identical(.result, 50000+(5000*10))) {
  list(correct = TRUE, message = "Nice work!")
} else {
  list(correct = FALSE, message = "That's incorrect, sorry.")
}
```
use your function to predict Y for the following collection of Xs
```{webr}
#| edit: false
X <- c(9,4,5,2,6)
```

```{webr}
#| exercise: r_function_call
#| envir: linear_predictor_function



```

```{webr}
#| exercise: r_function_call
#| check: true
if (identical(.result, 50000+(5000*X))) {
  list(correct = TRUE, message = "Nice work!")
} else {
  list(correct = FALSE, message = "That's incorrect, sorry.")
}
```

::::


### Random Errors

-   Data rarely lie perfectly on a line 
    -   Add 'error' term: $\varepsilon$
-   on average, we expect our linear predictor to be correct. 

::: Assumption 
The mean of $\varepsilon$, $\mu=0$
:::

-   however, we also expect there to be variance around the linear predictor. 

::: Assumption 
Variance of Y around the linear predictor according constant value $Var(\varepsilon)=\sigma^2$
:::



::: Assumption 
The error is normally distributed (with mean $\mu=0$ and variance $\sigma^2$ .
$$\varepsilon \sim \mathcal{N}(0, \sigma^2)$$
:::

::: Example
-   Same model as above - given standard deviation
-   Example Y values
    -   Illustrate error with normal overay
-   Excersises:
    -   which variance is larger?
    -   (hard) probability of finding E\[Y\]+2sd observation
:::
