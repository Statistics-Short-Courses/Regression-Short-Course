[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Linear Regression in R",
    "section": "",
    "text": "Preface\nThis notebook provides an introduction (or refresher) to linear regression, a fundamental tool for modeling relationships between variables. We’ll explore the concepts, fit models in R, interpret results, and visualize fits.\nBy the end, you should be able to:\n\nUnderstand the linear regression model and its assumptions.\nFit and interpret simple and multiple regression models in R.\nDiagnose model fit using residual plots and summary statistics.\nUse model output to make predictions and assess uncertainty.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "LinearModels.html",
    "href": "LinearModels.html",
    "title": "1  Working with linear models",
    "section": "",
    "text": "1.1 Linear relationships between variables\nRegression analysis models how a response variable (\\(Y\\)) depends on one or more predictors (X). Linear regression assumes that there is a linear relationship between \\(X\\) and \\(Y\\) That is, that we can express Y as X multiplied by and added to some constant values (\\(\\beta\\)). For example, with one predictor variable we have a ‘simple linear regression’:\n\\[Y \\approx \\beta_0 + \\beta_1 X\\].\nHere the two constants, \\(\\beta_0\\) and \\(\\beta_1\\), tell us about the relationship between X and Y. Specifically, \\(\\beta_0\\) tells us what Y looks like when \\(X=0\\), and \\(\\beta_1\\) tells us how much Y changes when X increases by 1.\nto make this more concrete, lets look at a basic example.\nEvaluating our linear equation across the range of X, gives us a straigth line with Y-intercept $50k and slope of $5k/year.\nggplot(cbind('Years employed'=X, \"Predicted salary($k)\"= E_Y), aes(`Years employed`, `Predicted salary($k)`))+ \n  geom_point()+\n  geom_abline(intercept= 50, slope=5)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Working with linear models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#linear-relationships-between-variables",
    "href": "LinearModels.html#linear-relationships-between-variables",
    "title": "1  Working with linear models",
    "section": "",
    "text": "Example 1: A simple linear model\nYou’ve been offered a job at ~company A~, and want to predict your salary at this company after working there for 10 years. Say we have records for 10 employees at a company, including their time working at the company. These values, in years are:\n\nX &lt;- sample(1:15, 30, replace=2)\nX\n\n [1]  4  1 14 10 12 12 14 12  6 10 12 15  1  1  6  2  3  7 11 11  9  4  8  5  5\n[26] 15 15 14  7  1\n\n\nOur aim in this example is to predict the salary of each participant \\(Y_i\\).\nWe know that everybody starts at this company at a base salary of $50,000, and every year they stay employed they get a pay rise of - on average - $5000.\nUsing this information, we can construct a simple linear model of salary from employment time. Given an employment time of \\(X_i\\) years, we can represent a employee’s expected salary (\\(\\hat{Y}_i\\)) mathematically as\n\\[\nE[Y]= \\$50,000 + \\$5,000\\cdot X_i.\n\\]\nlets compute these expected values for our 10 employees\n\nE_Y &lt;- 50 + 5*X\nE_Y\n\n [1]  70  55 120 100 110 110 120 110  80 100 110 125  55  55  80  60  65  85 105\n[20] 105  95  70  90  75  75 125 125 120  85  55\n\n\nand look at a plot of this data\n\nlibrary(ggplot2)\nggplot(cbind('Years employed'=X, \"Predicted salary($k)\"= E_Y), aes(`Years employed`, `Predicted salary($k)`))+ \n  geom_point()\n\n\n\n\n\n\n\n\nThe relationship between \\(X\\) and \\(\\hat{Y}\\) is perfectly linear - for an employee of \\(X_i\\) years, we can calulate \\(\\hat{Y}_i\\) using the linear equation above. Note that this applies not just to our sample of 10 employees, but to any other employee at the company - like yourself.\nFor example, an 11th employee has been working at the company for 8 years, his expected salary will therefore be \\(\\hat{Y}_{11}= \\$50,000 + \\$5,000\\cdot8 = \\$90,000\\).\n\n\nExcersise:\nUsing R, calculate the expected salary after working for this company for 10 years\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\nA second company also has a job available. Base salary on start is higher here at $70,000, but payrises are less - you are told that on average employees working for the company for 5 years have $20,000 higher salary than when they started. We want to construct a simple linear model to estimate Salary at company 2 after X years of employment.\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]\n\nWhat are the coefficients \\(\\beta_0\\) and \\(beta_1\\) in this case?\n\n\\(\\beta_0 =\\) \\(\\beta_1 =\\)\n\ncomplete the r function that predicts Y from an input X value for this linear model\n\n\n\n\n\n\n\n\n\n\n\n\nuse your function to predict Y for the following collection of X",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Working with linear models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#random-error",
    "href": "LinearModels.html#random-error",
    "title": "1  Working with linear models",
    "section": "1.2 Random error",
    "text": "1.2 Random error\nBut what is \\(E Y\\)? it is the predicted salary based on the ‘average’ rate of pay rise each year. The actual pay rise varies from individual to individual, year to year, based on factors we don’t have information about (e.g. job performance, time-off, good luck).\nThankfully, we have data on the actual salary of these employees too. Lets have a look:\n\nY &lt;- (50 + 5*X + rnorm(30, mean=0, sd=4)) |&gt; round(1)\nY\n\n [1]  64.7  54.8 122.9 103.1 110.3 113.5 116.1 106.0  78.0 107.3 102.6 129.5\n[13]  50.7  58.1  85.6  67.0  67.1  89.9 103.1 101.6  97.4  67.8  88.0  80.7\n[25]  72.9 119.6 122.1 117.3  87.1  55.4\n\n\n\nggplot(cbind('Years'=X, \"Salary($k)\"= Y), aes(Years, `Salary($k)`))+ \n  geom_point()\n\n\n\n\n\n\n\n\nOverlaying our linear model from before,\n\nggplot(cbind('Years'=X, \"Salary($k)\"= Y), aes(Years, `Salary($k)`))+ \n  geom_point()+\n  geom_abline(intercept= 50, slope=5)\n\n\n\n\n\n\n\n\nWe can see that our predictions were not always (in fact, never exactly) correct, however our simple linear equation does fairly well at characterising the general relationship between the length of employment and (actual) salary.\nThe difference between the linear predictor (\\(\\beta_0+\\beta_1\\cdot X_i\\)) and the actual values of the outcome, \\(Y_i\\) is called the ‘error’ and represented by \\(\\varepsilon\\). i.e.  \\[\n\\varepsilon_i = Y_i- \\beta_0+\\beta_1\\cdot X_i\n\\]\nlets calculate the errors for our linear model of the actual outcomes \\(Y_i\\):\n\ne &lt;-  Y-E_Y\ne\n\n [1] -5.3 -0.2  2.9  3.1  0.3  3.5 -3.9 -4.0 -2.0  7.3 -7.4  4.5 -4.3  3.1  5.6\n[16]  7.0  2.1  4.9 -1.9 -3.4  2.4 -2.2 -2.0  5.7 -2.1 -5.4 -2.9 -2.7  2.1  0.4\n\n\nIt can help to look at a plot\n\nggplot(data.frame(e),aes(e))+\n  geom_histogram(bins=5)\n\n\n\n\n\n\n\n\nWe can see that most errors are close to zero (this is good), however there is of course some variation.\nWe want to give \\(\\epsilon\\) some concrete form so we can use our model for prediction and inference, so make the assumption (for a number of reasons which we will not go into detail) that \\(\\epsilon\\) is a normally distributed random variable with mean,\\(\\mu=0\\) and some fixed standard deviation \\(\\sigma\\) i.e. \\[\n\\varepsilon \\sim Normal(0,\\sigma^2)\n\\]\n\nggplot(data.frame(e),aes(e))+\n  geom_histogram(bins=5)+\n  geom_function(fun= \\(x)dnorm(x,0,4)*100, xlim=c(-10,10), colour= \"red\")\n\n\n\n\n\n\n\n\nWith this assumption in place we can rearrange the equation for \\(\\epsilon\\) to give us the cannonical form of the simple linear regression model:\n\\[\nY= \\beta_0 + \\beta_1X +\\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2)\n\\]\n\nlibrary(purrr)\nlibrary(dplyr)\nggplot(cbind('Years employed'=X, \"Predicted salary($k)\"= E_Y), aes(`Years employed`, `Predicted salary($k)`))+ \n  geom_point()+\n  geom_path(\n    data = min(X):max(X) |&gt; \n      map(\\(x) tibble(x=x,\n                      y=seq(min(E_Y),max(E_Y), length.out=200),\n                      z=x+dnorm(y, 50+5*x, 4)*10)) |&gt; \n      list_rbind(),\n    aes(z, y, group = x),\n    linewidth = 0.4, alpha = 0.9) +\n  theme_minimal(base_size = 12)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Working with linear models</span>"
    ]
  },
  {
    "objectID": "Fitting.html",
    "href": "Fitting.html",
    "title": "2  Fitting a linear model to data",
    "section": "",
    "text": "2.1 Estimating parameters\nin the previous section, you were given a linear model - we knew the values of \\(\\beta_0\\), \\(\\beta_1\\) and even \\(\\sigma\\). However, in most contexts we don’t know the true relationship between \\(X\\) and \\(Y\\) in advance. Instead, we are given data and try to infer the values of \\(\\beta\\), \\(\\sigma\\) based on this.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data</span>"
    ]
  },
  {
    "objectID": "Fitting.html#estimating-parameters",
    "href": "Fitting.html#estimating-parameters",
    "title": "2  Fitting a linear model to data",
    "section": "",
    "text": "residuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating \\(\\beta\\)\nWhich line fits the data best (how low can the RMSE go)?\n\n\nviewof b1 = Inputs.range([-2, 2], {step: 0.1, label: \"slope\"})\nviewof b0adj = Inputs.range([-20, 20], {step: 1, label: \"intercept adustment\"})\nb0 = (90 - b1 * 180) + b0adj\n\nresidualData = data.map(d =&gt; {\n  const yhat = b1 * d.x + b0;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n\nRMSR = Math.sqrt(d3.sum(residualData, d =&gt; d.residual ** 2) / residualData.length)\n\ntex.block`y = ${b1}x + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\sqrt{\\frac{1}{n}\\sum_{i}r_i^{2}} = ${RMSR.toFixed(2)}`\n\n\n\n\n\n\n\n\n\n\n\nxRange = d3.extent(residualData,d =&gt; d.x)\nlineData = xRange.map(x =&gt; ({x, y: b1 * x + b0}))\n\nPlot.plot({\n  marks: [\n    Plot.ruleX(residualData, {x: \"x\",\n      y1: \"y\",\n      y2: \"yhat\",\n      stroke: \"sign\", \n      strokeOpacity: 0.75,\n      strokeWidth: d =&gt; 1 + d.absRes / 12,\n       title: d =&gt; `x=${d.x.toFixed(1)}\ny=${d.y.toFixed(1)}\nŷ=${d.yhat.toFixed(1)}\nres=${d.residual.toFixed(1)}\nres²=${(d.residual**2).toFixed(1)}`\n    }),\n\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(data, { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [0, 180], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating \\(\\sigma\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data</span>"
    ]
  },
  {
    "objectID": "Fitting.html#fitting-a-model-using-least-squares---lm",
    "href": "Fitting.html#fitting-a-model-using-least-squares---lm",
    "title": "2  Fitting a linear model to data",
    "section": "2.2 Fitting a model using least squares - lm()",
    "text": "2.2 Fitting a model using least squares - lm()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data</span>"
    ]
  },
  {
    "objectID": "Fitting.html#interpreting-summary-output",
    "href": "Fitting.html#interpreting-summary-output",
    "title": "2  Fitting a linear model to data",
    "section": "2.3 Interpreting summary() output",
    "text": "2.3 Interpreting summary() output",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data</span>"
    ]
  },
  {
    "objectID": "Fitting.html#inference-for-simple-linear-regression",
    "href": "Fitting.html#inference-for-simple-linear-regression",
    "title": "2  Fitting a linear model to data",
    "section": "2.4 Inference for simple linear regression",
    "text": "2.4 Inference for simple linear regression",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data</span>"
    ]
  },
  {
    "objectID": "Diagnostics.html",
    "href": "Diagnostics.html",
    "title": "3  Checking model assumptions",
    "section": "",
    "text": "3.1 Assumptions of the linear model",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Checking model assumptions</span>"
    ]
  },
  {
    "objectID": "Diagnostics.html#assumptions-of-the-linear-model",
    "href": "Diagnostics.html#assumptions-of-the-linear-model",
    "title": "3  Checking model assumptions",
    "section": "",
    "text": "Independence of observations\n\n\nLinear relationship\n\n\nConstant Variance\n\n\nNormally distributed error",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Checking model assumptions</span>"
    ]
  },
  {
    "objectID": "Diagnostics.html#using-residuals-to-check-assumptions",
    "href": "Diagnostics.html#using-residuals-to-check-assumptions",
    "title": "3  Checking model assumptions",
    "section": "3.2 Using residuals to check assumptions",
    "text": "3.2 Using residuals to check assumptions",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Checking model assumptions</span>"
    ]
  },
  {
    "objectID": "Diagnostics.html#common-assumption-violations",
    "href": "Diagnostics.html#common-assumption-violations",
    "title": "3  Checking model assumptions",
    "section": "3.3 Common assumption violations",
    "text": "3.3 Common assumption violations\n\nNon-independence of observations\n\n\nNon-linear relationships\n\n\nNon-constant Variance\n\n\nNon-normality of error",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Checking model assumptions</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html",
    "href": "MultipleLinearRegression.html",
    "title": "4  Multiple Linear Regression",
    "section": "",
    "text": "4.1 Adding extra predictors",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#adding-extra-beta-s",
    "href": "MultipleLinearRegression.html#adding-extra-beta-s",
    "title": "4  Multiple Linear Regression",
    "section": "4.2 adding extra \\(\\beta s\\)",
    "text": "4.2 adding extra \\(\\beta s\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#fitting-a-multiple-regression-model",
    "href": "MultipleLinearRegression.html#fitting-a-multiple-regression-model",
    "title": "4  Multiple Linear Regression",
    "section": "4.3 fitting a multiple regression model",
    "text": "4.3 fitting a multiple regression model",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#interpreting-output",
    "href": "MultipleLinearRegression.html#interpreting-output",
    "title": "4  Multiple Linear Regression",
    "section": "4.4 Interpreting output",
    "text": "4.4 Interpreting output",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#diagnostics-for-multiple-regression",
    "href": "MultipleLinearRegression.html#diagnostics-for-multiple-regression",
    "title": "4  Multiple Linear Regression",
    "section": "4.5 diagnostics for multiple regression",
    "text": "4.5 diagnostics for multiple regression\n\nmulticollinearity",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  }
]