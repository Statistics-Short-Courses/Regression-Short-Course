---
format: live-html
engine: knitr
---
{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}

# Multiple Linear Regression: Building more complex models

## Session overview

- Revisit the assumptions that underlie multiple linear regression and how they extend the simple linear case.
- Motivate the need for multiple predictors with short stories from real-world analyses (e.g., predicting house prices or patient outcomes).
- Highlight how model selection, diagnostics, and interpretation change as models become more complex.
- Encourage learners to connect ideas across chapters by comparing model summaries, visual diagnostics, and inferential statements.

### Potential exercises

- Ask learners to brainstorm scenarios where a single predictor is insufficient and list potential additional variables.
- Provide a model summary output and have learners identify which regression assumptions they would check first.

## Continuous predictors

### Key topics to emphasise

- Clarify the meaning of partial regression coefficients as “holding other variables constant.”
- Discuss the role of centred and standardised predictors for interpretability and numerical stability.
- Use added-variable plots or component-plus-residual plots to visualise the unique contribution of each predictor.

### Potential examples

- Fit a model predicting fuel efficiency (e.g., `mtcars` data) from engine size, weight, and horsepower to show competing effects.
- Contrast models with raw vs. centred predictors to illustrate interpretational differences.

### Potential exercises

- Provide learners with a dataset (such as `mtcars`) and have them interpret the coefficient of weight before and after centring.
- Ask learners to build a scatterplot matrix to hypothesise relationships among continuous predictors before modelling.

### Multi-collinearity

#### Key topics to emphasise

- Define variance inflation factors (VIFs) and tolerance as diagnostic tools.
- Explain why multicollinearity inflates standard errors and complicates inference.
- Highlight remedial strategies: collecting more data, combining variables, or using regularisation methods.

#### Potential examples

- Demonstrate high correlation between horsepower and displacement in the `mtcars` dataset and show the impact on coefficient estimates.
- Compare model outputs before and after removing a redundant predictor.

#### Potential exercises

- Provide correlation matrices and ask learners to flag problematic pairs of predictors.
- Have learners compute VIFs for a fitted model and interpret which predictors require attention.

## Categorical predictors

### Key topics to emphasise

- Review dummy (indicator) coding and how the choice of reference category affects interpretation.
- Explain how to include categorical predictors with more than two levels using treatment or sum coding.
- Introduce the concept of adjusted means when controlling for other predictors.

### Potential examples

- Model exam scores using study hours (continuous) and teaching method (categorical) to illustrate contrasts.
- Show how to interpret coefficients when switching the reference category.

### Potential exercises

- Ask learners to encode a three-level categorical variable manually and verify with software output.
- Provide regression results and have learners translate coefficients into comparisons between categories.

## Combining predictors

### Key topics to emphasise

- Frame constructed terms (products, ratios, smooth functions) as tools to capture curvature or conditional relationships.
- Stress the importance of centring predictors before forming interaction or polynomial terms to reduce multicollinearity.
- Connect constructed terms to model comparison using nested F-tests or information criteria.

### Potential examples

- Compare linear and quadratic fits for age vs. blood pressure to illustrate polynomial curvature.
- Introduce interaction between advertising budget and market type to show varying slopes.

### Potential exercises

- Have learners expand a design matrix to include squared terms and interpret the resulting coefficients.
- Ask learners to perform an F-test comparing a model with and without interaction terms.

### Polynomials

#### Key topics to emphasise

- Emphasise that polynomial terms model smooth curvature and require careful interpretation at different predictor levels.
- Demonstrate how polynomial degree affects model flexibility and risk of overfitting.
- Include a note on the bias–variance trade-off to frame how adding higher-degree terms can reduce bias but increase variance.
- Encourage plotting predicted values to visualise curvature.

#### Potential examples

- Use a dataset with age vs. lung capacity to illustrate diminishing returns via quadratic terms.
- Contrast polynomial regression with piecewise linear fits to show alternative approaches.

#### Potential exercises

- Ask learners to fit linear, quadratic, and cubic models and compare adjusted $R^2$ values.
- Provide predicted curves and have learners match them to the appropriate polynomial degree.

### Interactions

#### Key topics to emphasise

- Interpret interaction coefficients as changes in slopes across levels of another predictor.
- Highlight the necessity of plotting interactions (e.g., interaction plots, marginal effects).
- Discuss the difference between interactions involving continuous vs. categorical variables.
- Note that interactions need not be simple products; they can combine predictors through indicator-triggered slopes, smooth-varying functions, or other structured contrasts.

#### Potential examples

- Fit a model predicting salary from years of experience, gender, and their interaction to discuss equity analyses.
- Show how to compute and interpret simple slopes at specific predictor values.

#### Potential exercises

- Have learners calculate predicted outcomes at combinations of predictor values to interpret interaction effects.
- Ask learners to describe how an interaction would appear in a contour or surface plot.

### Transformation

#### Key topics to emphasise

- Explain when transforming the response or predictors can stabilise variance or linearise relationships.
- Compare log, square-root, and Box–Cox transformations and their interpretational consequences.
- Encourage residual diagnostics to confirm whether transformations improved model fit.

#### Potential examples

- Transform skewed income data with a log transformation and interpret coefficients in percentage terms.
- Illustrate how transforming predictors (e.g., log-weight) can linearise nonlinear relationships with the response.

#### Potential exercises

- Provide residual plots before and after a transformation for learners to evaluate improvement.
- Assign a short activity where learners propose appropriate transformations based on distributional summaries.


