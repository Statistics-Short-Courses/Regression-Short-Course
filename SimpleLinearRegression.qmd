---
format: live-html
engine: knitr

execute:
  echo: false
  warning: false
  message: false

filters:
  - custom-numbered-blocks

custom-numbered-blocks:
  classes:
    Assumption:
      colors: [FFCDD2, F44336]
      boxstyle: foldbox.simple
      collapse: false
    Example:
      colors: [BBDEFB, 2196F3]
      boxstyle: foldbox.simple
      collapse: false
    Exercise:
      colors: [C8E6C9, 4CAF50]
      boxstyle: foldbox.simple
      collapse: false
    Technical-point:
      colors: [C8E6C9, 4CAF50]
      boxstyle: foldbox.simple
      collapse: true
      
crossref:
  chapters: true
---
{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}
{{< include ./_extensions/r-wasm/live/_gradethis.qmd >}}

```{webr}
#| include: false
library(tidyverse)
set_theme(theme_bw())
```

```{r}
#| include: false
library(tidyverse)
n <- 20
linearData <- tibble(
  x=rnorm(n,0,4),
  y=0.5 * x - 3 + rnorm(n,0, 2)
  )
ojs_define(linearData)
```

# Simple Linear Regression
##  Fitting models to data {.unnumbered}
in the previous section, you were given a linear model - we knew the
values of $\beta_0$, $\beta_1$ and even $\sigma$. However, in most
contexts we don't know the true relationship between $X$ and $Y$ in
advance. Instead, we are given *data* and try to *infer* the values of
$\beta$ and $\sigma$ by analysing the data.

Indeed, at the end of the last session we generated data from a linear model. In preparation for this chapter, I simulated data from a similar such linear model - your task here (and in regression generally) is to try an make a (educated) guess about what linear model (summarised by it's values for $\beta$ and $\sigma$) produced this 
data. 



- [ ] Continue

## Estimating parameters

### Hats
lets distinguish our estimates from the 'true', underlying, parameter values by giving them $\hat{\text{hats}}$. so, we have

$$
\hat{\beta_0}: \text{Estimated intercept}
$$
$$
\hat{\beta_1}: \text{Estimated slope}
$$
$$
\hat{\sigma}: \text{Estimated standard deviation}.
$$

We also define $\hat{Y}$ as the predicted value of $Y$ given our estimated model:

$$
\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X
$$
Looking at some data, we try to find the values of our parameters that best 'fit' its distribution. Adjust the sliders below to find a line that you think fits the data:


```{ojs}
//| panel: sidebar
//| echo: false
viewof b0adj_1 = Inputs.range([-5, 5], {step: 1, label: html`${tex`\hat{\beta_0}`}: Estimated Intercept (asjust)`})
viewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\hat{\beta_1}`}: Estimated Slope`})

b0_1 = (-3 - 0.5*b1_1) + b0adj_1

tex.block`\hat{Y} = ${b0_1} + ${b1_1}x`
```

```{ojs}
//| panel: fill
//| echo: false

xRange = d3.extent(linearData.x)

lineData_1 = xRange.map(x => ({x, y: b1_1 * x + b0_1}))

Plot.plot({
  marks: [
    Plot.line(lineData_1, { x: "x", y: "y" }),

    Plot.dot(linearData, { x: "x", y: "y", r: 3 })
  ],
  x: { domain: xRange },
  y: { domain: [-10, 5], label: "y" },
})
```

::: Exercise
What is your best estimate for the values of $\beta_0$ and $\beta_1$ that best fit the given data?

```{webr}
#| exercise: ex_2.1
#| envir: Ex2
#| define:
#|   - beta_0_hat
#|   - beta_1_hat
beta_0_hat <- 1.2
beta_1_hat <- 12
```
:::: {.solution exercise="ex_1"}
```{webr}
#| exercise: ex_2.1
#| solution: true
```
::::
```{webr}
#| exercise: ex_2.1
#| envir: Ex2
#| check: true
#| class: wait
grade_this({
  if (is.numeric(get("beta_0_hat", envir = .envir_result)) && is.numeric(get("beta_1_hat", envir = .envir_result))){pass("Good guess!")}
  fail("both \\$\\beta_1\\$ and beta_0 should be numbers")
})
```

:::
###
Ok great, so we have an estimate for our line-of-best-fit:
```{ojs}
//| echo: false
tex.block`\hat{Y} = ${beta_0_hat} + ${beta_1_hat}\cdot X`
```

but what do we mean by 'best' fit here? How do we evaluate this estimate and maybe compare it to other estimates?

-   [ ] Continue

### Residuals

Residuals are the difference between observed values of $Y$, and the value predicted by our estimated linear predictor $\hat{Y}$. We denote residuals with the letter $e$:
$$
e=Y-\hat{Y} = Y - \hat{\beta_0}+\hat{\beta_1}X. 
$$

Residuals, $e$, are similar to the errors, $\varepsilon$, that we encountered in chapter 1 - but they are distinct. In many situations, we do not know the values of $\beta_0$ an $\beta_1$ - so we cannot calculate the errors, $\varepsilon= Y- E[Y]= Y-\beta_0 + \beta_1 X$. However, we have are our estimates, $\hat{\beta_0}$ and $\hat{\beta_1}$, so we can calculate residuals. 

Indeed - calculating residuals gives us a way to assess how well our estimated model fits the data.  We can think of the residuals as being the 'mismatch' between our estimated linear predictor and each data point. By *minimizing* the size of residuals, we can get a better fit of our line to data.

- [ ] Continue 

### Estimating $\beta$ by minimising residuals

#### Guess the linear predictor (how low can the residuals go)?

Our estimated line now also displays residuals for each data point.
Underneath the model equation is the *Sum of Squared Residuals* (SSR), which adds up all the residuals (the square means that they are all positive). 

$$
SSR=\sum_{i=1}^{n} e_i^2
$$

This gives us a numerical measure of how well the line fits the data - see how low you can get the SSR by adjusting slope and intercept.

$$
e_i = Y_i - \hat{Y}_i
$$

```{ojs}
//| panel: sidebar
//| echo: false
viewof b0adj = Inputs.range([-5, 5], {step: 1, label: html`${tex`\hat{\beta_0}`}: Estimated Intercept`})
viewof b1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\hat{\beta_1}`}: Estimated Slope`})

b0 = -3 + b0adj

residualData = linearData.map(d => {
  const yhat = b1 * d.x + b0;
  const res  = d.y - yhat;
  return {
    ...d,
    yhat,
    residual: res,
    sign: res >= 0 ? "positive" : "negative",
    absRes: Math.abs(res)
  };
})
  
SSRes = d3.sum(residualData, d => d.residual ** 2)

tex.block`\hat{Y} = ${b1}X + ${b0}`

tex.block`\sum_{i}r_i^{2} = ${SSRes.toFixed(2)}`
```

```{ojs}
//| panel: fill
//| echo: false
lineData = xRange.map(x => ({x, y: b1 * x + b0}))

Plot.plot({
  marks: [
    Plot.ruleX(residualData, {x: "x",
      y1: "y",
      y2: "yhat",
      stroke: "sign", 
      strokeOpacity: 0.75,
      strokeWidth: d => 1 + d.absRes}),

    Plot.line(lineData, { x: "x", y: "y" }),

    Plot.dot(linearData, { x: "x", y: "y", r: 3 })
  ],
  x: { domain: xRange },
  y: { domain: [-10,5], label: "y" },
})
```

::: Exercise
Based on this visualisation, what do you think the minimum possible $SSR$ achievable is?
```{webr}
#| exercise: Ex_2.2
#| envir: Ex2
#| define:
#|   - SSR
SSR <- 50
```
:::: {.solution exercise="Ex_2.2"}
```{webr}
#| exercise: Ex_2.2
#| solution: true
```
::::
```{webr}
#| exercise: Ex_2.2
#| check: true
#| class: wait
grade_this({
  if (is.numeric(get("SSR", envir = .envir_result))){
    pass("Good guess!")}
  fail("SSR should be a number!")
})
```
If you want you can update your $\hat{\beta}$ estimates too (otherwise just proceed):

```{webr}
#| exercise: ex_2.3
#| envir: Ex2
beta_0_hat <- ______

beta_1_hat <- ______
```

:::
::: Technical-point
#### Why squares?
The least squares estimator chooses $\hat \beta_0$ and $\hat \beta_1$ so that the sum of squared residuals $\sum_{i=1}^{n} r_i^2$ is as small as possible. But why take squares?
As mentioned, squaring numbers returns a positive result. Therefore the square gives some idea of the 'absolute' value of the residuals (and doesn't allow negative residuals to cancel out positve ones). But why not just take the absolute value?
Because the square function penalises large deviations heavily, least squares prefers lines that keep every point reasonably close to the fitted value rather than letting a few points drift far away.
:::

### Fitting a model using least squares

We call the estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ which minimise the sum of squares the *least squares estimates*. Some nice mathematics tells us that the least squares esimates for a simple linear model are *unique* - that is, there is one set of values for $\hat{\beta_0}$ and $\hat{\beta_1}$ which satisfy this property. Moreover, we don't have to manually adjust our parameters to keep lowering SSR - there is a convenient closed form expression which allows us to compute $\hat{\beta_0}$ and $\hat{\beta_1}$ very efficiently. As a statistical programming language, this is something R does very easily..


#### The `lm()` function

In R the `lm()` function computes the least squares estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ for our simple linear model in a single command, printing both of the fitted coefficients. 

```{webr}
#| input:
#|   - linearData
lm_1 <- lm(y~x,linearData)
lm_1
```

How do these estimates compare with yours? Lets have a look at the line it produces. 

```{webr}
#| envir: Ex2
#| code-fold: true
#| code-summary: "show code"
# we extract the coefficients from the lm object by indexing with `$`
ls_beta_0_hat <- lm_1$coefficients[1]
ls_beta_1_hat <- lm_1$coefficients[2]

ggplot(linearData)+
  aes(x=x,y=y)+
  geom_point()+
  geom_abline(
    intercept=ls_beta_0_hat,
    slope=ls_beta_1_hat)+
    geom_abline(
    intercept=beta_0_hat,
    slope=beta_1_hat)
```

-   [ ] Continue

### Estimating $\sigma$

Once the line has been fitted, we estimate the variance of the (actual) error terms by measuring the spread of the residuals.

We can extract the individual residual values from the fitted `lm` object:

```{webr}
e <- lm_1$residuals
ggplot(e)+
  aes(x=e)+
  geom_hist()
```

Dividing the residual sum of squares by $n-2$ (one degree of freedom lost per fitted coefficient) gives the mean squared error, and taking the square root yields the residual standard deviation (aka. the {{< glossary Residual-Standard-Error >}})
$$
\hat \sigma = \sqrt{\frac{1}{n-2}\sum_{i=1}^{n} r_i^2}.
$$
In R output, this value is often called the Standard error. Interpreting $\hat \sigma$ is often easier on the original $y$ scale: a typical observation falls about $\hat \sigma$ units away from the fitted line.

- [ ] Continue

## Checking assumptions

### Independence of observations

Linear regression relies on the observations being independent. Time series data or spatially linked measurements often violate this assumption. When in doubt, look for study-design clues (for example repeated measurements on the same participant) or plot the residuals in the order they were collected; long runs of positive or negative residuals suggest dependence.

### Linear relationships

The expected response should change linearly with the predictor. Scatterplots of $(x_i, y_i)$ and residual-versus-fitted plots are useful diagnostics. A curved pattern in either plot indicates that we may need a transformation or additional predictors to capture the relationship.

### Constant Variance

Also called homoscedasticity, this assumption requires that the spread of the residuals remains roughly constant for all fitted values. Residual plots that show a funnel shape (narrow for small $\hat y$ but wide for large $\hat y$) point to heteroscedasticity, in which case we might transform $y$, use weighted least squares, or adopt a different variance model.

### Normality of error

Least squares still works without normal errors, but the inference tools we use later (confidence intervals and hypothesis tests) rely on approximate normality of residuals. A normal Q-Q plot provides a quick check: substantial deviations from the diagonal line suggest heavier tails or skewness than the normal distribution.

### Examples of assumption violations
- In this case the assumptions hold - as expected because I told you the data were sampled from an actual linear model. 


## Inference for Simple Linear Regression


- Here we have been given some data sampled from a linear model, do you think the a parameter estimates obtained so far represent the underlying model well? 

- since we have the opportunity, lets take another sample from the same model: 

```{webr}
n <- 20
x <- rnorm(n,0,4)
error <-  rnorm(n,0, 2)
y <- 0.5 * x - 3 + error
linearData_new <- data.frame(x,y)
```

And fit a linear regression via least squares
```{webr}
lm_2 <- lm(y~x,linearData_new)
lm
```
lets compare out two datasets (remember, these come from the same underlying linear model with $\beta_0=-3$ and $\beta_1=0.5$ and their lines of best fit:
```{webr}
ggplot()+
  geom_point(data=linearData, aes())
```
- Repeating this process produces the sampling distribution of our parameters.

- in the real world we wont have the luxury of generating new samples by running R commands, so we have to make do with the data we have. Thankfully, if our assumptions of the underlying model are true then we can expect the same process of convergence to a stable sampling distribution to hold true and our parameter estimates for beta to have a t-distribution. 

### Inference about $\beta$

### Inference about $\sigma$

With the assumptions above in place, the scaled residual sum of squares follows a $\chi^2_{n-2}$ distribution. This allows us to build a confidence interval for $\sigma$ or to test competing claims about the error variance. In practice we use the R output `sigma` together with the degrees of freedom to construct intervals.
### Inference about $\beta_1$

To determine whether the predictor is associated with the response we frame the null hypothesis $H_0: \beta_1 = 0$ and use the t-statistic
$$
t = \frac{\hat \beta_1 - 0}{\operatorname{SE}(\hat \beta_1)}.
$$
The standard error is obtained from the fitted model and the test statistic is compared against a $t_{n-2}$ distribution. R reports the t-statistic, the corresponding p-value, and a confidence interval for $\beta_1$, letting us draw formal conclusions about the strength and direction of the linear association.

## Interpreting \`summary()\` output

The `summary()` function wraps all of these pieces into a single report. Focus on:
- the coefficients table, which lists estimates, standard errors, t-values, and p-values for $\beta_0$ and $\beta_1$;
- the residual standard error and degrees of freedom, which relate to $\hat \sigma$ and the variability around the line;
- the multiple $R^2$, quantifying the proportion of variation in $y$ explained by the predictor;
- the F-statistic, which matches the t-test for $\beta_1$ in the simple regression setting.

Interpreting each component in context helps translate the statistical output into practical insight about the data.

## Using the model for prediction

After a model has been checked and deemed appropriate, we can use it to predict responses for new values of the predictor. In R the `predict()` function returns both fitted values (point predictions) and prediction intervals that account for residual variability. When reporting predictions always state the uncertainty, because individual observations vary more than the fitted mean. Remember that predictions are most reliable for $x$ values within the range of the observed data; extrapolating far beyond the sample can produce misleading results.
