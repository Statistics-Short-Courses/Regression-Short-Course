---
format: 
  live-html:
    toc: true

execute:
  echo: true
  warning: false
  message: false

editor: 
  markdown: 
    wrap: 72
    
embed-resources: true
---
{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}

# Working with linear models

##  Linear relationships between variables

Regression analysis models how a *response variable* ($Y$) depends on one
or more *predictors* (X). Linear regression assumes that there is a linear relationship between $X$ and $Y$ That is, that we can express Y as X  multiplied by and added to some constant values ($\beta$). For example, with one predictor variable we have a 'simple linear regression':

$$Y \approx \beta_0 + \beta_1 X$$.

Here the two constants, $\beta_0$ and $\beta_1$, tell us about the relationship between X and Y. Specifically, $\beta_0$ tells us what Y looks like when $X=0$, and $\beta_1$ tells us how much Y changes when X increases by 1. 


to make this more concrete, lets look at a basic example.  

### Example 1: A simple linear model

You've been offered a job at ~company A~, and want to predict your salary at this company after working there for 10 years. Say we have records for 10 employees at a company, including their time working at the company. These values, in years are:
```{r}
X <- sample(1:15, 30, replace=2)
X
```
Our aim in this example is to predict the salary of each participant $Y_i$. 

We know that everybody starts at this company at a base salary of \$50,000, and every year they stay employed they get a pay rise of - on average - \$5000. 

Using this information, we can construct a simple linear model of salary from employment time. Given an employment time of $X_i$ years, we can represent a employee's expected salary ($\hat{Y}_i$) mathematically as

$$
E[Y]= \$50,000 + \$5,000\cdot X_i.
$$

lets compute these expected values for our 10 employees
```{r}
E_Y <- 50 + 5*X
E_Y
```

and look at a plot of this data
```{r}
library(ggplot2)
ggplot(cbind('Years employed'=X, "Predicted salary($k)"= E_Y), aes(`Years employed`, `Predicted salary($k)`))+ 
  geom_point()
```

The relationship between $X$ and $\hat{Y}$ is perfectly linear - for an employee of $X_i$ years, we can calulate $\hat{Y}_i$ using the linear equation above. Note that this applies not just to our sample of 10 employees, but to any other employee at the company - like yourself. 


For example, an 11th employee has been working at the company for 8 years, his expected salary will therefore be $\hat{Y}_{11}= \$50,000 + \$5,000\cdot8 = \$90,000$.

::: {.webex-check .webex-box}
### Excersise: 
Using R, calculate the expected salary after working for this company for 10 years

```{webr}
#| exercise: example_2

```

```{webr}
#| exercise: example_2
#| check: true
if (identical(.result, 50000+(5000*10))) {
  list(correct = TRUE, message = "Nice work!")
} else {
  list(correct = FALSE, message = "That's incorrect, sorry.")
}
```
:::

Evaluating our linear equation across the range of X, gives us a straigth line with Y-intercept \$50k and slope of \$5k/year.

```{r}
ggplot(cbind('Years employed'=X, "Predicted salary($k)"= E_Y), aes(`Years employed`, `Predicted salary($k)`))+ 
  geom_point()+
  geom_abline(intercept= 50, slope=5)
```

::: {.webex-check .webex-box}
#### Your turn 

A second company also has a job available. Base salary on start is higher here at \$70,000, but payrises are less  - you are told that on average employees working for the company for 5 years have \$20,000 higher salary than when they started. 
We want to construct a simple linear model to estimate Salary at company 2 after X years of employment. 

$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i
$$

 1. What are the coefficients $\beta_0$ and $beta_1$ in this case?


```{r}
#| echo: false
#| results: 'asis'
library(webexercises)
beta0_quiz <- fitb(answer= 70000, num=TRUE)
beta1_quiz <- fitb(answer=4000, num=TRUE)


# When using `{r} ...` language-bracketed inline code, functions like `mqc()` and `torf()` need to be enclosed by `I()`. This tells knitr to treat the output as markdown, thereby avoiding any character escaping of HTML code these functions generate.
```

$\beta_0 =$`{r} I(beta0_quiz)`
$\beta_1 =$`{r} I(beta1_quiz)`

2. complete the r function that predicts Y from an input X value for this linear model
```{webr}
#| exercise: r_function
#| envir: linear_predictor_function
linear_prediction <- function(X){__ + (__*__)}
```

```{webr}
#| exercise: example_2
#| check: true
if (identical(.result, 50000+(5000*10))) {
  list(correct = TRUE, message = "Nice work!")
} else {
  list(correct = FALSE, message = "That's incorrect, sorry.")
}
```
use your function to predict Y for the following collection of X
```{webr}
#| edit: false
X_new <- c(9,4,5,2,6)
```

```{webr}
#| exercise: r_function_call
#| envir: linear_predictor_function



```

```{webr}
#| exercise: r_function_call
#| check: true
if (identical(.result, 50000+(5000*X_new))) {
  list(correct = TRUE, message = "Nice work!")
} else {
  list(correct = FALSE, message = "That's incorrect, sorry.")
}
```

::::


## Random error
But what is $E Y$? it is the predicted salary based on the 'average' rate of pay rise each year. The actual pay rise varies from individual to individual, year to year, based on factors we don't have information about (e.g. job performance, time-off, good luck).

Thankfully, we have data on the *actual* salary of these employees too. Lets have a look:

```{r}
Y <- (50 + 5*X + rnorm(30, mean=0, sd=4)) |> round(1)
Y
```

```{r}
ggplot(cbind('Years'=X, "Salary($k)"= Y), aes(Years, `Salary($k)`))+ 
  geom_point()
```
Overlaying our linear model from before, 

```{r}
ggplot(cbind('Years'=X, "Salary($k)"= Y), aes(Years, `Salary($k)`))+ 
  geom_point()+
  geom_abline(intercept= 50, slope=5)
```
We can see that our predictions were not always (in fact, never exactly) correct, however our simple linear equation does fairly well at characterising the general relationship between the length of employment and (actual) salary. 

The difference between the linear predictor ($\beta_0+\beta_1\cdot X_i$) and the *actual* values of the outcome, $Y_i$ is called the 'error' and represented by $\varepsilon$. i.e. 
$$
\varepsilon_i = Y_i- \beta_0+\beta_1\cdot X_i
$$

lets calculate the errors for our linear model of the actual outcomes $Y_i$:

```{r}
e <-  Y-E_Y
e
```
It can help to look at a plot
```{r}
ggplot(data.frame(e),aes(e))+
  geom_histogram(bins=5)
```
We can see that most errors are close to zero (this is good), however there is of course some variation. 

We want to give $\epsilon$ some concrete form so we can use our model for prediction and inference, so make the assumption (for a number of reasons which we will not go into detail) that $\epsilon$ is a normally distributed random variable with mean,$\mu=0$ and some fixed standard deviation $\sigma$ i.e.
$$
\varepsilon \sim Normal(0,\sigma^2)
$$

```{r}
ggplot(data.frame(e),aes(e))+
  geom_histogram(bins=5)+
  geom_function(fun= \(x)dnorm(x,0,4)*100, xlim=c(-10,10), colour= "red")
```

With this assumption in place we can rearrange the equation for $\epsilon$ to give us the cannonical form of the simple linear regression model:

$$
Y= \beta_0 + \beta_1X +\varepsilon, \quad \varepsilon \sim N(0, \sigma^2)
$$

```{r}
library(purrr)
library(dplyr)
ggplot(cbind('Years employed'=X, "Predicted salary($k)"= E_Y), aes(`Years employed`, `Predicted salary($k)`))+ 
  geom_point()+
  geom_path(
    data = min(X):max(X) |> 
      map(\(x) tibble(x=x,
                      y=seq(min(E_Y),max(E_Y), length.out=200),
                      z=x+dnorm(y, 50+5*x, 4)*10)) |> 
      list_rbind(),
    aes(z, y, group = x),
    linewidth = 0.4, alpha = 0.9) +
  theme_minimal(base_size = 12)
```



