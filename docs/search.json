[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Linear Regression in R",
    "section": "",
    "text": "Preface\nThis notebook provides an introduction (or refresher) to linear regression, a fundamental tool for modeling relationships between variables. We’ll explore the concepts, fit models in R, interpret results, and visualize fits.\nBy the end, you should be able to:\n\nUnderstand the linear regression model and its assumptions.\nFit and interpret simple and multiple regression models in R.\nDiagnose model fit using residual plots and summary statistics.\nUse model output to make predictions and assess uncertainty.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "LinearModels.html",
    "href": "LinearModels.html",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "1.1 A simple linear model\nHere \\(\\varepsilon\\) represents the random variation in \\(Y\\) that is not explained by \\(X\\).\n-This means that \\(Y\\) is not perfectly determined by \\(X\\): even if we know the values of X, the outcome Y can still vary due to random noise.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#a-simple-linear-model",
    "href": "LinearModels.html#a-simple-linear-model",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "Linear prediction\n\nWe now choose \\(f(X)\\) to be a linear function:\n\n\\[\nf(X)= \\beta_0 + \\beta_1\\cdot X\n\\]\n\nPutting aside the random error term for the moment, we can think of the expected value of \\(Y\\), denoted \\(E[Y]\\), as being given by this linear relationship:\n\n\\[\nE[Y]= \\beta_0 + \\beta_1\\cdot X\n\\tag{1.2}\\]\n\nIn other words, we are representing the expected value of \\(Y\\) as a straight line with y-intercept \\(\\beta_0\\) and slope \\(\\beta_1\\):\n\n\n\nviewof b1 = Inputs.range([-2, 2], {step: 0.1, label: \"slope\"})\nviewof b0 = Inputs.range([-10, 10], {step: 1, label: \"intercept\"})\ntex.block`Y = ${b1}X + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = [-10,10]\nlineData = xRange.map(x =&gt; ({x, y: (b1 * x) + b0}))\nPlot.plot({\n  x:{domain: [-10,10], label: \"X\", grid: true},\n  y:{domain: [-10,10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(lineData, { x: \"x\", y: \"y\" })]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, \\(\\beta_0\\) represents the value of \\(Y\\) when \\(X=0\\), and \\(\\beta_1\\) represents the change in Y for a one-unit increase in X.\n\n\nAssumption 2Y and X have a linear relationship\n\n\n\nExample 1: Salary growth over timeYou’ve been offered a job at ~company A~, and want to predict your salary at this company after working there for 10 years. We know that, on average, the starting salary at this company is $50,000, and salaries increase by $5000 per year of employment.\nUsing this information, we can construct a simple linear predictor of salary from employment time. Given an employment time of \\(X=x\\) years, we can represent a employee’s expected salary (\\(E[Y]\\)) as\n\\[\nE[Y]= \\$50,000 + \\$5,000\\cdot x.\n\\]\nAfter 10 years of working at this company (i.e. \\(X=10\\)), we would expect a salary of \\[E[Y]=\\$50,000 + \\$5,000\\times10= \\$100,000\\]\n\n\n\nYour turn\nA second company also has a job available. The starting salary is higher here - $70,000 on average - but payrises are smaller. You are told that, on average, employees working for the company for 5 years earn $20,000 more per year than when they started.\nYou want to use a simple linear prediction to calculate your expected salary (\\(E[Y]\\)) after \\(X\\) years of employment.\n\\[\nE[Y] = \\beta_0 + \\beta_1 X\n\\]\n\nWhat are the coefficients \\(\\beta_0\\) and \\(beta_1\\) in this case?\n\n\\(\\beta_0 =\\) \\(\\beta_1 =\\)\nUsing R, calculate the expected salary after working for this company for 10 years\n\n\n\n\n\n\n\n\n\n\n\n\nComplete the r function that predicts \\(Y\\) from an input \\(X\\) for this linear model (alternatively - give function e.g. ‘this function predicts salary at company 2 with an input X years work’)\n\n\n\n\n\n\n\n\n\n\n\n\nUse this function to predict \\(Y\\) for the following values of \\(X\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nToDoadd plot comparing two (companies) linear salary growth?\n\n\n\n\n\nRandom Errors\n\nIn practice, data rarely fall perfectly on a straight line. Even if the underlying relationship between \\(X\\) and \\(Y\\) is approximately linear, individual observations tend to vary around that line.\nAs mentioned in Section 1.x, to account for this variation we add an error term, \\(\\varepsilon\\), to our model\n\n\\[\nY=\\beta_0 + \\beta_1X + \\varepsilon\n\\]\n\nMean Zero\n\nThe error term represents the *difference between the actual value of \\(Y\\) and the value predicted by the linear predictor, \\(E[Y]\\).\nOn average, we expect these error terms to balance out:\n\n\nAssumption 3The mean of the error term is zero:\n\\[\nE[\\varepsilon]=\\mu=0\n\\]\ni.e. The linear predictor gives the correct value of \\(Y\\) on average\n\n\n\n\nConstant variance\n\nWhile correct on average, we expect there to be some spread of data around the line (this is why we have the error term). The amount of spread is measured by the variance of the errors.\nWe assume that this variance is constant - like \\(mu=0\\), it is the same for all values of \\(X\\) however we dont specify which particular value it takes:\n\n\nAssumption 4The variance of the error term is constant for all values of X: \\[Var(\\varepsilon)=\\sigma^2\\]\n\n\n\n\nNormal distribution\nWhile the assumptions of mean 0 and constant variance describe the center and spread of the errors, they don’t fully specify the shape of their distribution. To model this more completely, we often assume that the errors follow a Normal distribution. This choice conveniently packages those assumptions — zero mean and constant variance — into a single, well-behaved model that is mathematically tractable and aligns with many real-world data patterns.\n\nAssumption 5The error is normally distributed (with mean \\(\\mu=0\\) and variance \\(\\sigma^2\\) . \\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\n\nviewof sigma = Inputs.range([0.2, 5], {\n  value: 1,\n  step: 0.1,\n  label: html`${tex`\\sigma`}: Standard deviation`\n})\n\nnormalDensity = d3.range(-4 * sigma, 4 * sigma, sigma / 50).map(x =&gt; ({\n  x,\n  density: (1 / (Math.sqrt(2 * Math.PI) * sigma)) * Math.exp(-(x ** 2) / (2 * sigma ** 2))\n}))\n\ntex.block`f(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi}\\,${sigma.toFixed(1)}}\\exp\\left(-\\frac{\\varepsilon^{2}}{2(${sigma.toFixed(1)})^{2}}\\right)`\nPlot.plot({\n  height: 280,\n  marginLeft: 48,\n  marginBottom: 40,\n  y: { label: \"Density\" },\n  x: {domain: [-10,10], label: html`Error value (${tex`\\varepsilon`})` },\n  marks: [\n    Plot.areaY(normalDensity, {\n      x: \"x\",\n      y: \"density\",\n      fillOpacity: 0.2,\n      stroke: \"#2a5599\",\n      fill: \"#2a5599\"\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\nFigure 1.1: Interactive normal distribution with adjustable standard deviation.\n\n\n\n\n\nTechnical-Point 1The Central Limit Theorem is a fundamental result of probability theory that provides some extra motivation for this assumption. According to this theorem, sums or averages of many small, independent random effects tend to follow a Normal distribution. Thus, assuming normally distributed errors is both a practical simplification and a reasonable approximation in many situations.\n\n\n\n\n\nThe Simple Linear Model\nPutting these pieces together we are left with\n\\[\nY=\\beta_0+\\beta_1X+\\varepsilon, \\quad{\\varepsilon \\sim N(0,\\sigma^2)}\n\\]\n\nExample 2\n\nSame model as above - given standard deviation\nExample Y values\n\nIllustrate error with normal overay\n\nExcersises:\n\nwhich variance is larger?\n(hard) probability of finding E[Y]+2sd observation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html",
    "href": "FittingSLR.html",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "",
    "text": "2.1 Estimating parameters\nin the previous section, you were given a linear model - we knew the values of \\(\\beta_0\\), \\(\\beta_1\\) and even \\(\\sigma\\). However, in most contexts we don’t know the true relationship between \\(X\\) and \\(Y\\) in advance. Instead, we are given data and try to infer the values of \\(\\beta\\), \\(\\sigma\\) based on this.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#estimating-parameters",
    "href": "FittingSLR.html#estimating-parameters",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "",
    "text": "Residuals\n\n\nEstimating \\(\\beta\\)\nWhich line fits the data best (how low can the RMSE go)?\n\nx = Array.from({length: 50}, d3.randomNormal(180, 50))\nnoise = Array.from({length: 50}, d3.randomNormal(0, 15))\ny = x.map((xi, i) =&gt; 0.5 * xi - 10 + noise[i])\n\nlinearData = x.map((xi, i) =&gt; ({ x: xi, y: y[i] }))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof b0adj = Inputs.range([-50, 50], {step: 1, label: html`${tex`\\beta_0`}: Intercept (adjustment)`})\nviewof b1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\\beta_1`}: Slope`})\n\nb0 = (90 - b1 * 180) + b0adj\n\nresidualData = linearData.map(d =&gt; {\n  const yhat = b1 * d.x + b0;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n\nRMSR = Math.sqrt(d3.sum(residualData, d =&gt; d.residual ** 2) / residualData.length)\n\ntex.block`y = ${b1}x + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\sqrt{\\frac{1}{n}\\sum_{i}r_i^{2}} = ${RMSR.toFixed(2)}`\n\n\n\n\n\n\n\n\n\n\n\nxRange = d3.extent(residualData,d =&gt; d.x)\nlineData = xRange.map(x =&gt; ({x, y: b1 * x + b0}))\n\nPlot.plot({\n  marks: [\n    Plot.ruleX(residualData, {x: \"x\",\n      y1: \"y\",\n      y2: \"yhat\",\n      stroke: \"sign\", \n      strokeOpacity: 0.75,\n      strokeWidth: d =&gt; 1 + d.absRes / 12,\n       title: d =&gt; `x=${d.x.toFixed(1)}\ny=${d.y.toFixed(1)}\nŷ=${d.yhat.toFixed(1)}\nres=${d.residual.toFixed(1)}\nres²=${(d.residual**2).toFixed(1)}`\n    }),\n\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(linearData, { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [0, 180], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a model using least squares - lm()\n\n\n\n\n\n\n\n\n\n\nEstimating \\(\\sigma\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#checking-assumptions",
    "href": "FittingSLR.html#checking-assumptions",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.2 Checking assumptions",
    "text": "2.2 Checking assumptions\n\nIndependence of observations\n\n\nLinear relationships\n\n\nConstant Variance\n\n\nNormality of error",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#inference-for-simple-linear-regression",
    "href": "FittingSLR.html#inference-for-simple-linear-regression",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.3 Inference for Simple Linear Regression",
    "text": "2.3 Inference for Simple Linear Regression\n\nInference about \\(\\sigma\\)\n\n\nInference about \\(\\beta_1\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#interpreting-summary-output",
    "href": "FittingSLR.html#interpreting-summary-output",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.4 Interpreting `summary()` output",
    "text": "2.4 Interpreting `summary()` output",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html",
    "href": "MultipleLinearRegression.html",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "",
    "text": "3.1 Continuous predictors",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#continuous-predictors",
    "href": "MultipleLinearRegression.html#continuous-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "",
    "text": "Multi-collinearity",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#categorical-predictors",
    "href": "MultipleLinearRegression.html#categorical-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "3.2 Categorical predictors",
    "text": "3.2 Categorical predictors",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#multiplying-predictors",
    "href": "MultipleLinearRegression.html#multiplying-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "3.3 Multiplying predictors",
    "text": "3.3 Multiplying predictors\n\nPolynomials\n\n\nInteractions\n\n\nTransformation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#using-the-model-for-prediction",
    "href": "FittingSLR.html#using-the-model-for-prediction",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.5 Using the model for prediction",
    "text": "2.5 Using the model for prediction",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  }
]