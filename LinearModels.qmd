---
format: 
  live-html:
    toc: true

execute:
  echo: true
  warning: false
  message: false

editor: 
  markdown: 
    wrap: 72
    
embed-resources: true

custom-numbered-blocks:
  classes:
    Assumption:
      colors: [a08080, 500000]
      collapse: false
    Example: 
      collapse: false
    ToDo: default
    Technical-Point: default
    
filters:
- custom-numbered-blocks
---

{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}

```{r}
#| include: false
library(ggplot2)
```


# An introduction to linear statistical models

-   A primary goal of statistical modelling is to characterise the relationship between variables, for now lets call them $X$ and $Y$.
-   In this context, we designate one variable, $Y$ as the *outcome* or *dependent variable*, and attempt to construct a *model* describing how it depends on the other variable(s), $X$, which we call *predictors* or *independent variables*. 
-   We can express the idea that $Y$ depends on $X$ mathematically as
$$
Y=f(X)
$$
-   That is, $Y$ is some function of $X$ (we have not yet specified what kind of function). Given values of $X$, we can use this function to *predict* or *explain* corresponding values of $Y$.

- This is similar to *deterministic models* you may know from physics - for example 
 $$E=MC^2$$,
 where $E$ (the outcome), depends on $M$ (a predictor) through the function $f(M)=MC^2$). 
 
- In statistical modelling we take another step and assume that $Y$ is not a purely determinsitic function of $X$, but that it varies somewhat randomly around such a relationship. We capture this by introducing a *random error term *, $\varepsilon$:
$$
Y=f(X)+\varepsilon
$$ {#eq-statistical-model}

Here $\varepsilon$ represents the random variation in $Y$ that is not explained by $X$. 

-This means that $Y$ is not perfectly determined by $X$: even if we know the values of X, the outcome Y can still vary due to random noise. 

::: Assumption 
$Y$ depends only on a deterministic function of $X$ and a random noise component
:::

## A simple linear model

### Linear prediction

-   We now choose $f(X)$ to be a *linear function*:

$$
f(X)= \beta_0 + \beta_1\cdot X
$$

-   Putting aside the random error term for the moment, we can think of the *expected value* of $Y$, denoted $E[Y]$, as being given by this linear relationship:

$$
E[Y]= \beta_0 + \beta_1\cdot X
$$ {#eq-deterministic-linear-model}

- In other words, we are representing the expected value of $Y$ as a *straight line* with y-intercept $\beta_0$ and slope $\beta_1$:

```{ojs}
//| panel: sidebar
//| echo: false
viewof b1 = Inputs.range([-2, 2], {step: 0.1, label: "Slope (β₁)"})
viewof b0 = Inputs.range([-10, 10], {step: 1, label: "Intercept (β₀)"})
tex.block`E[Y] = ${b1}X + ${b0}`
```

```{ojs}
//| panel: fill
//| echo: false
xRange = [-10,10]
lineData = xRange.map(x => ({x, y: (b1 * x) + b0}))
Plot.plot({
  x:{domain: [-10,10], label: "X", grid: true},
  y:{domain: [-10,10], label: "Y", grid: true},
  marks: [
    Plot.line(lineData, { x: "x", y: "y" })]
})

```

-   Here, $\beta_0$ represents the value of $Y$ when $X=0$, and
    $\beta_1$ represents the change in Y for a one-unit increase in X.
    
::: Assumption 
Y and X have a linear relationship
:::

::: Example
#### Salary growth over time

You've been offered a job at ~company A~, and want to predict your salary at this company after working there for 10 years. 
We know that, on average, the *starting salary* at this company is \$50,000, and salaries increase by \$5000 per year of employment. 

Using this information, we can construct a simple linear predictor of salary from employment time. Given an employment time of $X=x$ years, we can represent a employee's expected salary ($E[Y]$) as

$$
E[Y]= \$50,000 + \$5,000\cdot x.
$$

After 10 years of working at this company (i.e. $X=10$), we would expect a salary of 
$$E[Y]=\$50,000 + \$5,000\times10= \$100,000$$

:::

::: {.webex-check .webex-box}
#### Your turn 

A second company also has a job available. The *starting salary* is higher here - \$70,000 on average - but payrises are smaller. You are told that, on average, employees working for the company for 5 years earn \$20,000 more per year than when they started. 

You want to use a simple linear prediction to calculate your expected salary ($E[Y]$) after $X$ years of employment. 

$$
E[Y] = \beta_0 + \beta_1 X
$$

1. What are the coefficients $\beta_0$ and $beta_1$ in this case?

```{r}
#| echo: false
#| results: 'asis'
library(webexercises)
beta0_quiz <- fitb(answer= 70000, num=TRUE)
beta1_quiz <- fitb(answer=4000, num=TRUE)

```

$\beta_0 =$`{r} I(beta0_quiz)`
$\beta_1 =$`{r} I(beta1_quiz)`

Using R, calculate the expected salary after working for this company for 10 years

```{webr}
#| exercise: example_2

```

```{webr}
#| exercise: example_2
#| check: true
if (identical(.result, 70000+(4000*10))) {
  list(correct = TRUE, message = "Nice work!")
} else {
  list(correct = FALSE, message = "That's incorrect, sorry.")
}
```
2. Complete the r function that predicts $Y$ from an input $X$ for this linear model 
(alternatively - give function e.g. 'this function predicts salary at company 2 with an input X years work')
```{webr}
#| exercise: r_function
#| envir: linear_predictor_function
linear_prediction <- function(X){__ + (__*__)}
```

```{webr}
#| exercise: example_2
#| check: true
if (identical(.result, 50000+(5000*10))) {
  list(correct = TRUE, message = "Nice work!")
} else {
  list(correct = FALSE, message = "That's incorrect, sorry.")
}
```
Use this function to predict $Y$ for the following values of $X$
```{webr}
#| edit: false
X <- c(9,4,5,2,6)
```

```{webr}
#| exercise: r_function_call
#| envir: linear_predictor_function


```

```{webr}
#| exercise: r_function_call
#| check: true
if (identical(.result, 50000+(5000*X))) {
  list(correct = TRUE, message = "Nice work!")
} else {
  list(correct = FALSE, message = "That's incorrect, sorry.")
}
```

:::: ToDo
add plot comparing two (companies) linear salary growth? 
::::

:::


### Random Errors

-   In practice, data rarely fall perfectly on a straight line. Even if the underlying relationship between $X$ and $Y$ is approximately linear, individual observations tend to vary around that line.

-   As mentioned in Section 1.x,  to account for this variation we add an *error term*, $\varepsilon$, to our model 

$$
Y=\beta_0 + \beta_1X + \varepsilon
$$

#### Mean Zero
- The error term represents the *difference between the actual value of $Y$ and the value predicted by the linear predictor, $E[Y]$.
- On average, we expect these error terms to balance out:

::: Assumption 
The mean of the error term is zero:

$$
E[\varepsilon]=\mu=0
$$

i.e. The linear predictor gives the correct value of $Y$ on average
:::


#### Constant variance

- While correct on average, we expect there to be some *spread* of data around the line (this is why we have the error term). The amount of spread is measured by the *variance* of the errors.
- We assume that this variance is *constant* - like $mu=0$, it is the same for all values of $X$ however we dont specify which particular value it takes:

::: Assumption 
The variance of the error term is constant for all values of X:
$$Var(\varepsilon)=\sigma^2$$
:::

#### Normal distribution 

While the assumptions of mean 0 and constant variance describe the center and spread of the errors, they don’t fully specify the shape of their distribution. To model this more completely, we often assume that the errors follow a Normal distribution.

::: Assumption
The error is normally distributed (with mean $\mu=0$ and variance $\sigma^2$ .
$$\varepsilon \sim \mathcal{N}(0, \sigma^2)$$
:::

```{ojs}
//| echo: false
//| fig-cap: "Interactive normal distribution with adjustable standard deviation."
viewof sigma = Inputs.range([0.2, 5], {
  value: 1,
  step: 0.1,
  label: html`${tex`\sigma`}: Standard deviation`
})

normalDensity = d3.range(-4 * sigma, 4 * sigma, sigma / 50).map(x => ({
  x,
  density: (1 / (Math.sqrt(2 * Math.PI) * sigma)) * Math.exp(-(x ** 2) / (2 * sigma ** 2))
}))

tex.block`f(\varepsilon) = \frac{1}{\sqrt{2\pi}\,${sigma.toFixed(1)}}\exp\left(-\frac{\varepsilon^{2}}{2(${sigma.toFixed(1)})^{2}}\right)`

Plot.plot({
  height: 280,
  marginLeft: 48,
  marginBottom: 40,
  y: { label: "Density" },
  x: {domain: [-10,10], label: "ε" },
  marks: [
    Plot.areaY(normalDensity, {
      x: "x",
      y: "density",
      fillOpacity: 0.2,
      stroke: "#2a5599",
      fill: "#2a5599"
    })
  ]
})
```

::: Technical-Point
The Central Limit Theorem is a fundamental result of probability theory that provides some extra motivation for this assumption. According to this theorem, sums or averages of many small, independent random effects tend to follow a Normal distribution. Thus, assuming normally distributed errors is both a practical simplification and a reasonable approximation in many situations.
:::

### The Simple Linear Model

Putting these pieces together we are left with:

$$
Y=\beta_0+\beta_1X+\varepsilon, \quad{\varepsilon \sim N(0,\sigma^2)}
$$

This 'simple linear model' is the starting place for conducting linear regression - in which whe 'fit' (i.e. estimate the values of $\beta_0$, $\beta_1$, and $\sigma^2$) from data. 

```{ojs}
//| panel: sidebar
//| echo: false

viewof b1_2 = Inputs.range([-2, 2], {step: 0.1, label: "Slope (β₁)"})
viewof b0_2 = Inputs.range([-10, 10], {step: 1, label: "Intercept (β₀)"})
viewof sigma2 = Inputs.range([0.2, 5], {step: 0.1, value: 1, label: "Std. deviation (σ)"})

tex.block`Y = {b0_2} + ${b1_2}X + \varepsilon, \quad \varepsilon \sim \text{Normal}($ 0, ${sigma2}^2)`
```

```{ojs}
//| panel: fill
//| echo: false
//| width: 700
 xValues = Array.from({length: 101}, (_, i) => -10 + i * 0.2);
 yValues = Array.from({length: 101}, (_, i) => -10 + i * 0.2);

 normalDensity2 = (y, mean, sd) =>
  (1 / (sd * Math.sqrt(2 * Math.PI))) * Math.exp(-0.5 * ((y - mean) / sd) ** 2);

 densityGrid = xValues.flatMap(x => {
  const mu = b0_2 + b1_2 * x;
  return yValues.map(y => ({
    x, y,
    density: normalDensity2(y, mu, sigma2)   // <- use sigma (not sigma2)
  }));
});

Plot.plot({
  x: {domain: [-10, 10], label: "X", grid: true},
  y: {domain: [-10, 10], label: "Y", grid: true},
  color: {label: "Density",
    type: "sequential",
    scheme: "Blues"},
  marks: [
    Plot.raster(densityGrid, {x: "x", y: "y", fill: "density", interpolate: "nearest"})
  ]
});

```
::: Example
-   Same model as above - given standard deviation
-   Example Y values
    -   Illustrate error with normal overay
-   Excersises:
    -   which variance is larger?
    -   (hard) probability of finding E\[Y\]+2sd observation
:::
