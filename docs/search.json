[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Linear Regression in R",
    "section": "",
    "text": "Preface\nThis notebook provides an introduction (or refresher) to linear regression, a fundamental tool for modeling relationships between variables. We’ll explore the concepts, fit models in R, interpret results, and visualize fits.\nBy the end, you should be able to:\n\nUnderstand the linear regression model and its assumptions.\nFit and interpret simple and multiple regression models in R.\nDiagnose model fit using residual plots and summary statistics.\nUse model output to make predictions and assess uncertainty.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "LinearModels.html",
    "href": "LinearModels.html",
    "title": "1  Working with linear models",
    "section": "",
    "text": "1.1 Linear relationships between variables\nRegression analysis models how a response variable (\\(Y\\)) depends on one or more predictors (X). Linear regression assumes that there is a linear relationship between \\(X\\) and \\(Y\\) That is, that we can express Y as X multiplied by and added to some constant values (\\(\\beta\\)). For example, with one predictor variable we have a ‘simple linear regression’:\n\\[Y \\approx \\beta_0 + \\beta_1 X\\].\nHere the two constants, \\(\\beta_0\\) and \\(\\beta_1\\), tell us about the relationship between X and Y. Specifically, \\(\\beta_0\\) tells us what Y looks like when \\(X=0\\), and \\(\\beta_1\\) tells us how much Y changes when X increases by 1.\nto make this more concrete, lets look at a basic example.\nEvaluating our linear equation across the range of X, gives us a straigth line with Y-intercept $50k and slope of $5k/year.\nggplot(cbind('Years employed'=X, \"Predicted salary($k)\"= E_Y), aes(`Years employed`, `Predicted salary($k)`))+ \n  geom_point()+\n  geom_abline(intercept= 50, slope=5)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Working with linear models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#linear-relationships-between-variables",
    "href": "LinearModels.html#linear-relationships-between-variables",
    "title": "1  Working with linear models",
    "section": "",
    "text": "Example 1: A simple linear model\nYou’ve been offered a job at ~company A~, and want to predict your salary at this company after working there for 10 years. Say we have records for 10 employees at a company, including their time working at the company. These values, in years are:\n\nX &lt;- sample(1:15, 30, replace=2)\nX\n\n [1] 14  1  4  3  7  2 14  6  5  1  3  8  2  8  1  1  8  7  6 12  4 15  7  5  5\n[26]  7 12 11  9  2\n\n\nOur aim in this example is to predict the salary of each participant \\(Y_i\\).\nWe know that everybody starts at this company at a base salary of $50,000, and every year they stay employed they get a pay rise of - on average - $5000.\nUsing this information, we can construct a simple linear model of salary from employment time. Given an employment time of \\(X_i\\) years, we can represent a employee’s expected salary (\\(\\hat{Y}_i\\)) mathematically as\n\\[\nE[Y]= \\$50,000 + \\$5,000\\cdot X_i.\n\\]\nlets compute these expected values for our 10 employees\n\nE_Y &lt;- 50 + 5*X\nE_Y\n\n [1] 120  55  70  65  85  60 120  80  75  55  65  90  60  90  55  55  90  85  80\n[20] 110  70 125  85  75  75  85 110 105  95  60\n\n\nand look at a plot of this data\n\nlibrary(ggplot2)\nggplot(cbind('Years employed'=X, \"Predicted salary($k)\"= E_Y), aes(`Years employed`, `Predicted salary($k)`))+ \n  geom_point()\n\n\n\n\n\n\n\n\nThe relationship between \\(X\\) and \\(\\hat{Y}\\) is perfectly linear - for an employee of \\(X_i\\) years, we can calulate \\(\\hat{Y}_i\\) using the linear equation above. Note that this applies not just to our sample of 10 employees, but to any other employee at the company - like yourself.\nFor example, an 11th employee has been working at the company for 8 years, his expected salary will therefore be \\(\\hat{Y}_{11}= \\$50,000 + \\$5,000\\cdot8 = \\$90,000\\).\n\n\nExcersise:\nUsing R, calculate the expected salary after working for this company for 10 years\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\nA second company also has a job available. Base salary on start is higher here at $70,000, but payrises are less - you are told that on average employees working for the company for 5 years have $20,000 higher salary than when they started. We want to construct a simple linear model to estimate Salary at company 2 after X years of employment.\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]\n\nWhat are the coefficients \\(\\beta_0\\) and \\(beta_1\\) in this case?\n\n\\(\\beta_0 =\\) \\(\\beta_1 =\\)\n\ncomplete the r function that predicts Y from an input X value for this linear model\n\n\n\n\n\n\n\n\n\n\n\n\nuse your function to predict Y for the following collection of X\n\nX_new &lt;- c(9,4,5,2,6)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Working with linear models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#random-error",
    "href": "LinearModels.html#random-error",
    "title": "1  Working with linear models",
    "section": "1.2 Random error",
    "text": "1.2 Random error\nBut what is \\(E Y\\)? it is the predicted salary based on the ‘average’ rate of pay rise each year. The actual pay rise varies from individual to individual, year to year, based on factors we don’t have information about (e.g. job performance, time-off, good luck).\nThankfully, we have data on the actual salary of these employees too. Lets have a look:\n\nY &lt;- (50 + 5*X + rnorm(30, mean=0, sd=4)) |&gt; round(1)\nY\n\n [1] 125.4  59.2  70.9  62.0  80.0  58.6 118.6  79.6  70.3  54.3  55.6  90.0\n[13]  59.1  95.3  57.7  51.4  94.7  81.1  77.0 113.9  72.4 124.1  89.5  69.7\n[25]  77.3  82.4 108.2 110.3  96.7  57.2\n\n\n\nggplot(cbind('Years'=X, \"Salary($k)\"= Y), aes(Years, `Salary($k)`))+ \n  geom_point()\n\n\n\n\n\n\n\n\nOverlaying our linear model from before,\n\nggplot(cbind('Years'=X, \"Salary($k)\"= Y), aes(Years, `Salary($k)`))+ \n  geom_point()+\n  geom_abline(intercept= 50, slope=5)\n\n\n\n\n\n\n\n\nWe can see that our predictions were not always (in fact, never exactly) correct, however our simple linear equation does fairly well at characterising the general relationship between the length of employment and (actual) salary.\nThe difference between the linear predictor (\\(\\beta_0+\\beta_1\\cdot X_i\\)) and the actual values of the outcome, \\(Y_i\\) is called the ‘error’ and represented by \\(\\varepsilon\\). i.e.  \\[\n\\varepsilon_i = Y_i- \\beta_0+\\beta_1\\cdot X_i\n\\]\nlets calculate the errors for our linear model of the actual outcomes \\(Y_i\\):\n\ne &lt;-  Y-E_Y\ne\n\n [1]  5.4  4.2  0.9 -3.0 -5.0 -1.4 -1.4 -0.4 -4.7 -0.7 -9.4  0.0 -0.9  5.3  2.7\n[16] -3.6  4.7 -3.9 -3.0  3.9  2.4 -0.9  4.5 -5.3  2.3 -2.6 -1.8  5.3  1.7 -2.8\n\n\nIt can help to look at a plot\n\nggplot(data.frame(e),aes(e))+\n  geom_histogram(bins=5)\n\n\n\n\n\n\n\n\nWe can see that most errors are close to zero (this is good), however there is of course some variation.\nWe want to give \\(\\epsilon\\) some concrete form so we can use our model for prediction and inference, so make the assumption (for a number of reasons which we will not go into detail) that \\(\\epsilon\\) is a normally distributed random variable with mean,\\(\\mu=0\\) and some fixed standard deviation \\(\\sigma\\) i.e. \\[\n\\varepsilon \\sim Normal(0,\\sigma^2)\n\\]\n\nggplot(data.frame(e),aes(e))+\n  geom_histogram(bins=5)+\n  geom_function(fun= \\(x)dnorm(x,0,4)*100, xlim=c(-10,10), colour= \"red\")\n\n\n\n\n\n\n\n\nWith this assumption in place we can rearrange the equation for \\(\\epsilon\\) to give us the cannonical form of the simple linear regression model:\n\\[\nY= \\beta_0 + \\beta_1X +\\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2)\n\\]\n\nlibrary(purrr)\nlibrary(dplyr)\nggplot(cbind('Years employed'=X, \"Predicted salary($k)\"= E_Y), aes(`Years employed`, `Predicted salary($k)`))+ \n  geom_point()+\n  geom_path(\n    data = min(X):max(X) |&gt; \n      map(\\(x) tibble(x=x,\n                      y=seq(min(E_Y),max(E_Y), length.out=200),\n                      z=x+dnorm(y, 50+5*x, 4)*10)) |&gt; \n      list_rbind(),\n    aes(z, y, group = x),\n    linewidth = 0.4, alpha = 0.9) +\n  theme_minimal(base_size = 12)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Working with linear models</span>"
    ]
  },
  {
    "objectID": "Fitting.html",
    "href": "Fitting.html",
    "title": "2  Fitting a linear model to data with lm()",
    "section": "",
    "text": "2.1 Estimating parameters",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data with lm()</span>"
    ]
  },
  {
    "objectID": "Fitting.html#which-line-fits-the-data-best",
    "href": "Fitting.html#which-line-fits-the-data-best",
    "title": "2  Fitting a linear model to data with lm()",
    "section": "2.2 2. Which line fits the data best?",
    "text": "2.2 2. Which line fits the data best?\n\n(how low can the RMSE go?)\n\n\nviewof b1 = Inputs.range([-2, 2], {step: 0.1, label: \"slope\"})\nviewof b0adj = Inputs.range([-20, 20], {step: 1, label: \"intercept adustment\"})\nb0 = (90 - b1 * 180) + b0adj\n\nresidualData = data.map(d =&gt; {\n  const yhat = b1 * d.x + b0;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n\nRMSR = Math.sqrt(d3.sum(residualData, d =&gt; d.residual ** 2) / residualData.length)\n\ntex.block`y = ${b1}x + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\sqrt{\\frac{1}{n}\\sum_{i}r_i^{2}} = ${RMSR.toFixed(2)}`\n\n\n\n\n\n\n\n\n\n\n\nxRange = d3.extent(residualData,d =&gt; d.x)\nlineData = xRange.map(x =&gt; ({x, y: b1 * x + b0}))\n\nPlot.plot({\n  marks: [\n    Plot.ruleX(residualData, {x: \"x\",\n      y1: \"y\",\n      y2: \"yhat\",\n      stroke: \"sign\", \n      strokeOpacity: 0.75,\n      strokeWidth: d =&gt; 1 + d.absRes / 12,\n       title: d =&gt; `x=${d.x.toFixed(1)}\ny=${d.y.toFixed(1)}\nŷ=${d.yhat.toFixed(1)}\nres=${d.residual.toFixed(1)}\nres²=${(d.residual**2).toFixed(1)}`\n    }),\n\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(data, { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [0, 180], label: \"y\" },\n})",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data with lm()</span>"
    ]
  },
  {
    "objectID": "Fitting.html#the-ols-estimate",
    "href": "Fitting.html#the-ols-estimate",
    "title": "2  Fitting a linear model to data with lm()",
    "section": "2.3 3. The OLS estimate",
    "text": "2.3 3. The OLS estimate",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data with lm()</span>"
    ]
  },
  {
    "objectID": "Fitting.html#estimating-parameters",
    "href": "Fitting.html#estimating-parameters",
    "title": "2  Fitting a linear model to data with lm()",
    "section": "",
    "text": "Estimating \\(\\beta\\)\nWhich line fits the data best (how low can the RMSE go)?\n\n\nviewof b1 = Inputs.range([-2, 2], {step: 0.1, label: \"slope\"})\nviewof b0adj = Inputs.range([-20, 20], {step: 1, label: \"intercept adustment\"})\nb0 = (90 - b1 * 180) + b0adj\n\nresidualData = data.map(d =&gt; {\n  const yhat = b1 * d.x + b0;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n\nRMSR = Math.sqrt(d3.sum(residualData, d =&gt; d.residual ** 2) / residualData.length)\n\ntex.block`y = ${b1}x + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\sqrt{\\frac{1}{n}\\sum_{i}r_i^{2}} = ${RMSR.toFixed(2)}`\n\n\n\n\n\n\n\n\n\n\n\nxRange = d3.extent(residualData,d =&gt; d.x)\nlineData = xRange.map(x =&gt; ({x, y: b1 * x + b0}))\n\nPlot.plot({\n  marks: [\n    Plot.ruleX(residualData, {x: \"x\",\n      y1: \"y\",\n      y2: \"yhat\",\n      stroke: \"sign\", \n      strokeOpacity: 0.75,\n      strokeWidth: d =&gt; 1 + d.absRes / 12,\n       title: d =&gt; `x=${d.x.toFixed(1)}\ny=${d.y.toFixed(1)}\nŷ=${d.yhat.toFixed(1)}\nres=${d.residual.toFixed(1)}\nres²=${(d.residual**2).toFixed(1)}`\n    }),\n\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(data, { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [0, 180], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating \\(\\sigma\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data with lm()</span>"
    ]
  },
  {
    "objectID": "Fitting.html#fitting-a-model-using-least-squares---lm",
    "href": "Fitting.html#fitting-a-model-using-least-squares---lm",
    "title": "2  Fitting a linear model to data with lm()",
    "section": "2.2 Fitting a model using least squares - lm()",
    "text": "2.2 Fitting a model using least squares - lm()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data with lm()</span>"
    ]
  },
  {
    "objectID": "Fitting.html#interpreting-lm-summary-output",
    "href": "Fitting.html#interpreting-lm-summary-output",
    "title": "2  Fitting a linear model to data with lm()",
    "section": "2.3 Interpreting lm() |> summary() output",
    "text": "2.3 Interpreting lm() |&gt; summary() output",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data with lm()</span>"
    ]
  },
  {
    "objectID": "Fitting.html#inference-for-simple-linear-regression",
    "href": "Fitting.html#inference-for-simple-linear-regression",
    "title": "2  Fitting a linear model to data with lm()",
    "section": "2.4 Inference for simple linear regression",
    "text": "2.4 Inference for simple linear regression",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data with lm()</span>"
    ]
  },
  {
    "objectID": "Fitting.html#interpreting-summary-output",
    "href": "Fitting.html#interpreting-summary-output",
    "title": "2  Fitting a linear model to data with lm()",
    "section": "2.3 Interpreting summary() output",
    "text": "2.3 Interpreting summary() output",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting a linear model to data with lm()</span>"
    ]
  }
]