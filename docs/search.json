[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Linear Regression in R",
    "section": "",
    "text": "Preface\nThis notebook provides an introduction (or refresher) to linear regression, a fundamental tool for modeling relationships between variables. We’ll explore the concepts, fit models in R, interpret results, and visualize fits.\nBy the end, you should be able to:\n\nUnderstand the linear regression model and its assumptions.\nFit and interpret simple and multiple regression models in R.\nDiagnose model fit using residual plots and summary statistics.\nUse model output to make predictions and assess uncertainty.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "LinearModels.html",
    "href": "LinearModels.html",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "1.1 Statistical models\nHere \\(\\varepsilon\\) represents the random variation in \\(Y\\) that is not explained by \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#statistical-models",
    "href": "LinearModels.html#statistical-models",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "A primary goal of statistical modelling is to characterise the relationship between variables, for now lets call them \\(X\\) and \\(Y\\).\nIn this context, we designate one variable, \\(Y\\) as the outcome or dependent variable, and attempt to construct a model describing how it depends on the other variable(s), \\(X\\), which we call predictors or independent variables.\nWe can express the idea that \\(Y\\) depends on \\(X\\) mathematically as \\[\nY=f(X)\n\\]\nThat is, \\(Y\\) is some function of \\(X\\) (we have not yet specified what kind of function). Given values of \\(X\\), we can use this function to predict or explain corresponding values of \\(Y\\).\nThis is similar to deterministic models you may know from physics - for example \\[E=MC^2\\], where \\(E\\) (the outcome), depends on \\(M\\) (a predictor) through the function \\(f(M)=MC^2\\)).\nIn statistical modelling we take another step and assume that \\(Y\\) is not a purely determinsitic function of \\(X\\), but that it varies somewhat randomly around such a relationship. We capture this by introducing a random error term , \\(\\varepsilon\\): \\[\nY=f(X)+\\varepsilon\n\\tag{1.1}\\]\n\n\n\nThis means that \\(Y\\) is not perfectly determined by \\(X\\): even if we know the values of X, the outcome Y can still vary due to random noise.\n\n\nAssumption 1\\(Y\\) depends only on a deterministic function of \\(X\\) and a random noise component\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#linear-prediction",
    "href": "LinearModels.html#linear-prediction",
    "title": "1  An introduction to linear statistical models",
    "section": "1.2 Linear prediction",
    "text": "1.2 Linear prediction\n\nWe now choose \\(f(X)\\) to be a linear function:\n\n\\[\nf(X)= \\beta_0 + \\beta_1\\cdot X\n\\]\n\nPutting aside the random error term for the moment, we can think of the expected value of \\(Y\\), denoted \\(E[Y]\\), as being given by this linear relationship:\n\n\\[\nE[Y]= \\beta_0 + \\beta_1\\cdot X\n\\tag{1.2}\\]\n\nIn other words, we are representing the expected value of \\(Y\\) as a straight line with y-intercept \\(\\beta_0\\) and slope \\(\\beta_1\\):\n\n\n\nviewof b1 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β₁)\"})\nviewof b0 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (β₀)\"})\ntex.block`E[Y] = ${b1}X + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = [-10,10]\nlineData = xRange.map(x =&gt; ({x, y: (b1 * x) + b0}))\nPlot.plot({\n  x:{domain: [-10,10], label: \"X\", grid: true},\n  y:{domain: [-10,10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(lineData, { x: \"x\", y: \"y\" })]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\nFigure 1.1: Plot of the linear function E[Y] = β₀+ β₁X\n\n\n\n\n\n\n\n\nHere, \\(\\beta_0\\) represents the expected value of \\(Y\\) (\\(E[Y]\\)) when \\(X=0\\), and \\(\\beta_1\\) represents the change in \\(E[Y]\\) for a one-unit increase in \\(X\\).\n\n\nAssumption 2Y and X have a linear relationship\n\n\n\nExample 1: Salary growth over timeYou’ve been offered a job at ~company A~, and want to predict your salary at this company after working there for 10 years. We know that, on average, the starting salary at this company is $50,000, and salaries increase by $5000 per year of employment.\nUsing this information, we can construct a simple linear predictor of salary from employment time. Given an employment time of \\(X=x\\) years, we can represent a employee’s expected salary (\\(E[Y]\\)) as\n\\[\nE[Y]= \\$50,000 + \\$5,000\\cdot x.\n\\]\n\n\nshow code\nggplot()+\n  geom_abline(intercept=5e4, slope=5e3, colour='#2196F3')+\n  lims(x=c(0,15),y=c(4e4,13e4))+\n  labs(x=\"Years Employment\", y= \"Expecteted Salary\")\n\n\n\n\n\n\n\n\n\nAfter 10 years of working at this company (i.e. \\(X=10\\)), we would expect a salary of \\[E[Y]=\\$50,000 + \\$5,000\\times10= \\$100,000\\]\n\n\n\nExercise 1: A competing offer\nA second company also has a job available. The starting salary ishigher here - $70,000 on average - but payrises are smaller. You are told that, on average, employees working for the company for 6 years earn $18,000 more per year than when they started.\nYou want to use a simple linear prediction to calculate your expectedsalary (\\(E[Y]\\)) after \\(X\\) years of employment.\n\\[\nE[Y] = \\beta_0 + \\beta_1 X\n\\]\n\nChoosing parameters\nWhat are the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in this case?\nAssign the relevant values to variables in R\n\n\n\n\n\n\n\n\n\n\n\nbeta_0 &lt;- 70000\nbeta_1 &lt;- 3000\nbeta_0 &lt;- 70000\nbeta_1 &lt;- 3000\n\n\n\n\n\n\n\n\nLinear prediction\nGreat, so our linear predictor of the expected salary looks something like: \\[\nE[Y]= \\$70,000 + \\$3,000\\cdot X\n\\]\n\n\nshow code\nggplot()+\n  geom_abline(aes(intercept=7e4, slope=3e3, colour=\"1\"))+\n  geom_abline(aes(intercept=5e4, slope=5e3, colour=\"Company 2\"))+\n  lims(x=c(0,15),y=c(4e4,13e4))+\n  labs(x=\"Years Employment\",\n       y= \"Expecteted Salary ($)\",\n       colour=\"Company\")+\n  scale_color_manual(values= c(\"1\"=\"#4CAF50\",\"2\"='#2196F3'))\n\n\n\n\n\nLinear trend of salary and years employed at two different companies\n\n\n\n\nUsing the parameter variables you just defined, calculate the expected salary after working for this companyfor 10 years\n\n\n\n\n\n\n\n\n\n\n\nE_Y &lt;- beta_0 + (beta_1*10)\nE_Y &lt;- beta_0 + (beta_1*10)\n\n\n\n\n\n\n\n\nUsing R functions\nGreat, so our expected salary after working at the company for 10 years is\n\n\n\n\n\n\n\n\n\\[\nE[Y]= \\$70,000+\\$3,000\\times10 = \\$100,000\n\\] Sounds alright.\nNow to finish this section lets turn this linear prediction into our own R function that takes an input \\(X\\) and returns the \\(E[Y]\\)\n\n\n\n\n\n\n\n\nUse this function to predict \\(Y\\) for the following values of \\(X\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsimple_linear_prediction(X)\nsimple_linear_prediction(X)\n\n\n\n\n\n\n\n\nGood work!\nNext we’ll add the second main component to our linear predictor to make it a linear statistical model\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#random-errors",
    "href": "LinearModels.html#random-errors",
    "title": "1  An introduction to linear statistical models",
    "section": "1.3 Random Errors",
    "text": "1.3 Random Errors\n\nIn practice, data rarely fall perfectly on a straight line. Even if the underlying relationship between \\(X\\) and \\(Y\\) is approximately linear, individual observations tend to vary around that line.\nAs mentioned in Section 1.x, to account for this variation we add an error term, \\(\\varepsilon\\), to our model\n\n\\[\nY=\\beta_0 + \\beta_1X + \\varepsilon\n\\]\n\nMean Zero\n\nThe error term represents the *difference between the actual value of \\(Y\\) and the value predicted by the linear predictor, \\(E[Y]\\).\nOn average, we expect these error terms to balance out:\n\n\nAssumption 3The mean of the error term is zero:\n\\[\nE[\\varepsilon]=\\mu=0\n\\]\ni.e. The linear predictor gives the correct value of \\(Y\\) on average\n\n\n\n\nConstant variance\n\nWhile correct on average, we expect there to be some spread of data around the line (this is why we have the error term). The amount of spread is measured by the variance of the errors.\nWe assume that this variance is constant - like \\(mu=0\\), it is the same for all values of \\(X\\) however we dont specify which particular value it takes:\n\n\nAssumption 4The variance of the error term is constant for all values of X: \\[Var(\\varepsilon)=\\sigma^2\\]\n\n\n\n\nNormal distribution\nWhile the assumptions of mean 0 and constant variance describe the center and spread of the errors, they don’t fully specify the shape of their distribution. To model this more completely, we often assume that the errors follow a Normal distribution.\n\nAssumption 5The error is normally distributed (with mean \\(\\mu=0\\) and variance \\(\\sigma^2\\) . \\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\n\n\nviewof mu = Inputs.range([-5, 5], {\n  value: 0,\n  step: 0.1,\n  label: `Mean (μ):`\n})\n\nviewof sigma = Inputs.range([0.2, 5], {\n  value: 1,\n  step: 0.1,\n  label: 'Standard deviation (σ):'\n})\n\nSQRT2PI=Math.sqrt(2 * Math.PI)\n\nnormalDensity = (x, mean, sd) =&gt;\n  (1 / (sd * SQRT2PI)) * Math.exp(-0.5 * ((x - mean) / sd) ** 2);\n  \ndensityGrid_1 = d3.range((-4 * sigma)+mu, mu+(4 * sigma), sigma / 50).map(x =&gt; ({\n  x,\n  density: normalDensity(x,mu,sigma)\n}));\n\ntex.block`\\varepsilon \\sim \\text{Normal}(${mu}, ${sigma}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 280,\n  marginLeft: 48,\n  marginBottom: 40,\n  y: { label: \"Density\" },\n  x: {domain: [-10,10], label: \"ε\" },\n  marks: [\n    Plot.areaY(densityGrid_1, {\n      x: \"x\",\n      y: \"density\",\n      fillOpacity: 0.2,\n      stroke: \"#2a5599\",\n      fill: \"#2a5599\"\n    })\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1.2: Normal distribution with adjustable mean and standard deviation.\n\n\n\n\n\n\n\n\nTechnical-point 1: Central limit theorem\n\n\n\nExample 2: Variation in salaryLets return to our simple linear model of salary at ~company A~,\n\\[\nE[Salary] = 50,000 + 5,000\\times Years\n\\]\nThis expresses how salary tends to increase with experience (i.e. on averaage). But in practice, not every employee with the same number of years earns the same amount — some earn a bit more, some a bit less. Suppose we know that most employees — about two-thirds of them — earn within roughly $4,000 of the average salary for their experience level. In other words, if the average salary after five years is $75,000, then about two-thirds of employees earn between $71,000 and $79,000, and almost everyone (about 95%) earns between $67,000 and $83,000.\nWe can capture this variability with a random error term, \\(\\varepsilon\\), assumed to follow a Normal distribution with mean 0 and standard deviation \\(\\sigma = 4,000\\).\n\\[\nSalary = 50,000 + 5,000\\times Years + \\varepsilon, \\quad{\\varepsilon \\sim \\mathcal{N}(0,4000^2)}\n\\]\nThis means that for a given number of years \\(X\\): - The expected salary is \\(50,000 + 5,000\\cdot X\\) - Actual salaries will vary around that average, typically within about ±$4,000\nFor example, after 5 years (X=5): \\[\nE[Y]=\\$50,000 + \\$5,000 \\times 5 = \\$75,000\n\\]\nThe distribution of salaries for employees with 5 years’ experience is\n\\[\nY\\sim \\mathcal{N}(75,000, 4,000^2)\n\\]\nSince we have a probability distribution over \\(Y\\), we can use R to evaluate the probability of any given salary after X years at the company.\nFor example, we want to know if employed at company A, what is the probabity after working there for 10 years I will have a salary of at least $110,000?\nFirst lets calculate the average salary after 10 years\n\n50000+(5000*10)\n\n[1] 1e+05\n\n\n$100,000.\nNow\n\npnorm(11e4,1e5, 4e3, lower.tail = FALSE)\n\n[1] 0.006209665\n\n\nThe following diagram tells us roughly the probability of observing a \\(Y\\) value in the given range:\n\n\nmu_salary = 75000\nsigma_salary = 4000\n\nviewof k = Inputs.range([0, 3], {step: 0.1, value: 1, label: \"Half-width k (so interval is μ ± k·σ)\"})\nviewof offset_z = Inputs.range([-5, 5], {step: 0.1, value: 0, label: \"Centre offset (in σ units)\"})\n\ncentre = mu_salary + offset_z * sigma_salary\na = centre - k * sigma_salary\nb = centre + k * sigma_salary\n\n\n// --- Numerical helpers ---\nerf = x =&gt; {\n  const a1 = 0.254829592, a2 = -0.284496736, a3 = 1.421413741,\n  a4 = -1.453152027, a5 = 1.061405429, p = 0.3275911;\n  const sign = Math.sign(x) || 1;\n  x = Math.abs(x);\n  const t = 1 / (1 + p * x);\n  const y = 1 - (((((a5*t + a4)*t + a3)*t + a2)*t + a1)*t) * Math.exp(-x*x);\n  return sign * y;\n}\nnormalCDF = (x, mean, sd) =&gt; 0.5 * (1 + erf((x - mean) / (sd * Math.SQRT2)))\n\n// Probability mass between a and b\nprob = Math.max(0, Math.min(1, normalCDF(b, mu_salary, sigma_salary) - normalCDF(a, mu_salary, sigma_salary)))\n\n// Density grid and shaded interval\ndensityGrid_salary = d3.range(mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary, sigma_salary/100)\n.map(y =&gt; ({ y, density: normalDensity(y, mu_salary, sigma_salary) }))\n\nshaded = densityGrid_salary.filter(d =&gt; d.y &gt;= a && d.y &lt;= b)\n\n// Display text summary\ntex.block`P(${Math.round(a).toLocaleString()} \\le Y \\le ${Math.round(b).toLocaleString()}) = ${prob.toFixed(3)} \\;\\;(\\approx ${(prob*100).toFixed(1)}\\%)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 300,\n  marginLeft: 56,\n  marginBottom: 40,\n  x: { label: \"Salary ($)\", grid: true, domain: [mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary] },\n  y: { label: \"Density\",  },\n  marks: [\n    // Full curve (light)\n    Plot.areaY(densityGrid_salary, {x:\"y\", y:\"density\", fill:\"#2a5599\", fillOpacity:0.12, stroke:\"#2a5599\"}),\n    // Shaded probability region\n    Plot.areaY(shaded, {x:\"y\", y:\"density\", fill:\"#FFD54F\", fillOpacity:0.35}),\n    // Vertical rules\n    Plot.ruleX([mu_salary], {stroke: \"black\", strokeDash: [4,4]}),\n    Plot.ruleX([a], {y1:0, y2:normalDensity(a, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    Plot.ruleX([b], {y1:0, y2:normalDensity(b, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    // Baseline\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1.3: Probability of salary falling within a chosen interval.\n\n\n\n\n\n\n\nHere \\(\\varepsilon\\) represents random deviations from the expected (average) salary for a given number of years X. - Same model as above - given standard deviation - Example Y values - Illustrate error with normal overay - Excersises: - which variance is larger? - (hard) probability of finding E[Y]+2sd observation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#a-simple-linear-model-in-r",
    "href": "LinearModels.html#a-simple-linear-model-in-r",
    "title": "1  An introduction to linear statistical models",
    "section": "1.5 A simple Linear model in R",
    "text": "1.5 A simple Linear model in R\n\nfinish this section and lead into the next on fitting models to data.\nsimulate observations of Y from a specified linear model\n\n\nto begin, we start with a collection of X values (we might imagine we measure these values in the wild)\n\n\nn &lt;- 100\nX &lt;- runif(n=100, min=0, max= 50)\nhead(X)\n\n[1] 21.781556 22.482677 45.483555 32.030714 44.456408  4.117591\n\n\n\nThis code randomly chooses n = 100 values uniformy at random from the interval \\([0,50]\\).\n\n\nDefine the model\n\nNext, we construct our simple linear model\n\nbeta_0 &lt;- 4\nbeta_1 &lt;- 1.2\nsigma &lt;- 4\n\nsimple_linear_model &lt;- function(X, beta_0, beta_1, sigma) {\n  mu &lt;- beta_0 + (beta_1 * X) \n  mu + rnorm(length(X), mean = 0, sd = sigma)\n}\n\n\nThis is function takes X values and returns the specified linear function with normally distributed random noise added.\nNow we can simulate observations of \\(Y\\) given our list of \\(X\\) values and out linear model:\n\n\nY &lt;- simple_linear_model(X, beta_0, beta_1, sigma)\nhead(Y)\n\n[1] 33.422844 30.768083 60.300646 39.717100 57.324722  8.501843\n\n\nlets look at the joint distribution of X and Y:\n\nlibrary(ggplot2)\n\ndf &lt;- data.frame(X = X, Y = Y)\n\nggplot(df, aes(X, Y)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(intercept = beta_0, slope = beta_1, linetype = \"dashed\") +\n  labs(x = \"X\", y = \"Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 1.5: Data generated from the simple linear model \\(Y=4+1.2\\times X + \\varepsilon\\), with \\(\\varepsilon\\sim N(0,16)\\). Dashed line shows E[Y|X] = β0 + β1X\n\n\n\n\n\nHeres the same code running in webR - try adjusting some of the parameters (e.g. the number of samples) and running to plot a different random sample!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html",
    "href": "FittingSLR.html",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "",
    "text": "2.1 Estimating parameters\nin the previous section, you were given a linear model - we knew the values of \\(\\beta_0\\), \\(\\beta_1\\) and even \\(\\sigma\\). However, in most contexts we don’t know the true relationship between \\(X\\) and \\(Y\\) in advance. Instead, we are given data and try to infer the values of \\(\\beta\\) and \\(\\sigma\\) by analysing the data.\nIndeed, at the end of the last session we generated data from a linear model. In preparation for this chapter, I simulated data from a similar such linear model - your task here (and in regression generally) is to try an make a (educated) guess about what linear model (summarised by it’s values for \\(\\beta\\) and \\(\\sigma\\)) produced this data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#estimating-parameters",
    "href": "FittingSLR.html#estimating-parameters",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "",
    "text": "Hats\nlets distinguish our estimates from the ‘true’, underlying, parameter values by giving them \\(\\hat{\\text{hats}}\\). so, we have\n\\[\n\\hat{\\beta_0}: \\text{Estimated intercept}\n\\] \\[\n\\hat{\\beta_1}: \\text{Estimated slope}\n\\] \\[\n\\hat{\\sigma}: \\text{Estimated standard deviation}.\n\\]\nWe also define \\(\\hat{Y}\\) as the predicted value of \\(Y\\) given our estimated model:\n\\[\n\\hat{Y}=\\hat{\\beta_0}+\\hat{\\beta_1}X\n\\] Looking at some data, we try to find the values of our parameters that best ‘fit’ its distribution. Adjust the sliders below to find a line that you think fits the data:\n\nx = Array.from({length: 10}, d3.randomNormal(0,4 ))\nnoise = Array.from({length: 50}, d3.randomNormal(0, 2))\ny = x.map((xi, i) =&gt; 0.5 * xi - 3 + noise[i])\n\nxRange = d3.extent(x)\n\n\nlinearData = x.map((xi, i) =&gt; ({ x: xi, y: y[i] }))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof b0adj_1 = Inputs.range([-5, 5], {step: 1, label: html`${tex`\\hat{\\beta_0}`}: Estimated Intercept (asjust)`})\nviewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\\hat{\\beta_1}`}: Estimated Slope`})\n\nb0_1 = (-3 - 0.5*b1_1) + b0adj_1\n\ntex.block`\\hat{Y} = ${b0_1} + ${b1_1}x`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlineData_1 = xRange.map(x =&gt; ({x, y: b1_1 * x + b0_1}))\n\nPlot.plot({\n  marks: [\n    Plot.line(lineData_1, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(linearData, { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [-10, 5], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\nWhat is your best estimate for the values of \\(\\beta_0\\) and \\(\\beta_1\\) that best fit the given data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOk great, so we have an estimate for our line-of-best-fit:\n\ntex.block`\\hat{Y} = ${beta_0_hat} + ${beta_1_hat}\\cdot X`\n\n\n\n\n\n\nbut what do we mean by ‘best’ fit here? How do we evaluate this estimate and maybe compare it to other estimates?\n\nContinue\n\n\n\nResiduals\nResiduals are the difference between observed values of \\(Y\\), and the value predicted by our estimated linear predictor \\(\\hat{Y}\\). We denote residuals with the letter \\(e\\): \\[\ne=Y-\\hat{Y} = Y - \\hat{\\beta_0}+\\hat{\\beta_1}X.\n\\]\nResiduals, \\(e\\), are similar to the errors, \\(\\varepsilon\\), that we encountered in chapter 1 - but they are distinct. In many situations, we do not know the values of \\(\\beta_0\\) an \\(\\beta_1\\) - so we cannot calculate the errors, \\(\\varepsilon= Y- E[Y]= Y-\\beta_0 + \\beta_1 X\\). However, we have are our estimates, \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\), so we can calculate residuals.\nIndeed - calculating residuals gives us a way to assess how well our estimated model fits the data. We can think of the residuals as being the ‘mismatch’ between our estimated linear predictor and each data point. By minimizing the size of residuals, we can get a better fit of our line to data.\n\nContinue\n\n\n\nEstimating \\(\\beta\\) by minimising residuals\n\nGuess the linear predictor (how low can the residuals go)?\nOur estimated line now also displays residuals for each data point. Underneath the model equation is the Sum of Squared Residuals (SSR), which adds up all the residuals (the square means that they are all positive).\n\\[\nSSR=\\sum_{i=1}^{n} e_i^2\n\\]\nThis gives us a numerical measure of how well the line fits the data - see how low you can get the SSR by adjusting slope and intercept.\n\\[\ne_i = Y_i - \\hat{Y}_i\n\\]\n\n\nviewof b0adj = Inputs.range([-5, 5], {step: 1, label: html`${tex`\\hat{\\beta_0}`}: Estimated Intercept`})\nviewof b1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\\hat{\\beta_1}`}: Estimated Slope`})\n\nb0 = -3 + b0adj\n\nresidualData = linearData.map(d =&gt; {\n  const yhat = b1 * d.x + b0;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n  \nSSRes = d3.sum(residualData, d =&gt; d.residual ** 2)\n\ntex.block`\\hat{Y} = ${b1}X + ${b0}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\sum_{i}r_i^{2} = ${SSRes.toFixed(2)}`\n\n\n\n\n\n\n\n\n\n\n\nlineData = xRange.map(x =&gt; ({x, y: b1 * x + b0}))\n\nPlot.plot({\n  marks: [\n    Plot.ruleX(residualData, {x: \"x\",\n      y1: \"y\",\n      y2: \"yhat\",\n      stroke: \"sign\", \n      strokeOpacity: 0.75,\n      strokeWidth: d =&gt; 1 + d.absRes}),\n\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(linearData, { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [-10,5], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\nBased on this visualisation, what do you think the minimum possible \\(SSR\\) achievable is?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want you can update your \\(\\hat{\\beta}\\) estimates too (otherwise just proceed):\n\n\n\n\n\n\n\n\n\n\n\nTechnical-point 1: Why squares?The least squares estimator chooses \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) so that the sum of squared residuals \\(\\sum_{i=1}^{n} r_i^2\\) is as small as possible. But why take squares? As mentioned, squaring numbers returns a positive result. Therefore the square gives some idea of the ‘absolute’ value of the residuals (and doesn’t allow negative residuals to cancel out positve ones). But why not just take the absolute value? Because the square function penalises large deviations heavily, least squares prefers lines that keep every point reasonably close to the fitted value rather than letting a few points drift far away.\n\n\n\n\n\nFitting a model using least squares\nWe call the estimates \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) which minimise the sum of squares the least squares estimates. Some nice mathematics tells us that the least squares esimates for a simple linear model are unique - that is, there is one set of values for \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) which satisfy this property. Moreover, we don’t have to manually adjust our parameters to keep lowering SSR - there is a convenient closed form expression which allows us to compute \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) very efficiently. As a statistical programming language, this is something R does very easily..\n\nThe lm() function\nIn R the lm() function computes the least squares estimates \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) for our simple linear model in a single command, printing both of the fitted coefficients.\n\n\n\n\n\n\n\n\nHow do these estimates compare with yours? Lets have a look at the line it produces.\n\n\n\n\n\n\n\n\n\nContinue\n\n\n\n\nEstimating \\(\\sigma\\)\nOnce the line has been fitted, we estimate the variance of the (actual) error terms by measuring the spread of the residuals.\nDividing the residual sum of squares by \\(n-2\\) (one degree of freedom lost per fitted coefficient) gives the mean squared error, and taking the square root yields the residual standard deviation (aka. the Residual-Standard-Error) \\[\n\\hat \\sigma = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^{n} r_i^2}.\n\\] In R output, this value is often called the. Interpreting \\(\\hat \\sigma\\) is often easier on the original \\(y\\) scale: a typical observation falls about \\(\\hat \\sigma\\) units away from the fitted line.\n\n\nEstimating by the spread of residuals",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#checking-assumptions",
    "href": "FittingSLR.html#checking-assumptions",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.2 Checking assumptions",
    "text": "2.2 Checking assumptions\n\nIndependence of observations\nLinear regression relies on the observations being independent. Time series data or spatially linked measurements often violate this assumption. When in doubt, look for study-design clues (for example repeated measurements on the same participant) or plot the residuals in the order they were collected; long runs of positive or negative residuals suggest dependence.\n\n\nLinear relationships\nThe expected response should change linearly with the predictor. Scatterplots of \\((x_i, y_i)\\) and residual-versus-fitted plots are useful diagnostics. A curved pattern in either plot indicates that we may need a transformation or additional predictors to capture the relationship.\n\n\nConstant Variance\nAlso called homoscedasticity, this assumption requires that the spread of the residuals remains roughly constant for all fitted values. Residual plots that show a funnel shape (narrow for small \\(\\hat y\\) but wide for large \\(\\hat y\\)) point to heteroscedasticity, in which case we might transform \\(y\\), use weighted least squares, or adopt a different variance model.\n\n\nNormality of error\nLeast squares still works without normal errors, but the inference tools we use later (confidence intervals and hypothesis tests) rely on approximate normality of residuals. A normal Q-Q plot provides a quick check: substantial deviations from the diagonal line suggest heavier tails or skewness than the normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#inference-for-simple-linear-regression",
    "href": "FittingSLR.html#inference-for-simple-linear-regression",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.3 Inference for Simple Linear Regression",
    "text": "2.3 Inference for Simple Linear Regression\n\nInference about \\(\\sigma\\)\nWith the assumptions above in place, the scaled residual sum of squares follows a \\(\\chi^2_{n-2}\\) distribution. This allows us to build a confidence interval for \\(\\sigma\\) or to test competing claims about the error variance. In practice we use the R output sigma together with the degrees of freedom to construct intervals. ### Inference about \\(\\beta_1\\)\nTo determine whether the predictor is associated with the response we frame the null hypothesis \\(H_0: \\beta_1 = 0\\) and use the t-statistic \\[\nt = \\frac{\\hat \\beta_1 - 0}{\\operatorname{SE}(\\hat \\beta_1)}.\n\\] The standard error is obtained from the fitted model and the test statistic is compared against a \\(t_{n-2}\\) distribution. R reports the t-statistic, the corresponding p-value, and a confidence interval for \\(\\beta_1\\), letting us draw formal conclusions about the strength and direction of the linear association.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#interpreting-summary-output",
    "href": "FittingSLR.html#interpreting-summary-output",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.4 Interpreting `summary()` output",
    "text": "2.4 Interpreting `summary()` output\nThe summary() function wraps all of these pieces into a single report. Focus on: - the coefficients table, which lists estimates, standard errors, t-values, and p-values for \\(\\beta_0\\) and \\(\\beta_1\\); - the residual standard error and degrees of freedom, which relate to \\(\\hat \\sigma\\) and the variability around the line; - the multiple \\(R^2\\), quantifying the proportion of variation in \\(y\\) explained by the predictor; - the F-statistic, which matches the t-test for \\(\\beta_1\\) in the simple regression setting.\nInterpreting each component in context helps translate the statistical output into practical insight about the data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "FittingSLR.html#using-the-model-for-prediction",
    "href": "FittingSLR.html#using-the-model-for-prediction",
    "title": "2  Simple Linear Regression: fitting a model to data",
    "section": "2.5 Using the model for prediction",
    "text": "2.5 Using the model for prediction\nAfter a model has been checked and deemed appropriate, we can use it to predict responses for new values of the predictor. In R the predict() function returns both fitted values (point predictions) and prediction intervals that account for residual variability. When reporting predictions always state the uncertainty, because individual observations vary more than the fitted mean. Remember that predictions are most reliable for \\(x\\) values within the range of the observed data; extrapolating far beyond the sample can produce misleading results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression: fitting a model to data</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html",
    "href": "MultipleLinearRegression.html",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "",
    "text": "3.1 Continuous predictors",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#continuous-predictors",
    "href": "MultipleLinearRegression.html#continuous-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "",
    "text": "Multi-collinearity",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#categorical-predictors",
    "href": "MultipleLinearRegression.html#categorical-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "3.2 Categorical predictors",
    "text": "3.2 Categorical predictors",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "MultipleLinearRegression.html#multiplying-predictors",
    "href": "MultipleLinearRegression.html#multiplying-predictors",
    "title": "3  Multiple Linear Regression: Building more complex models",
    "section": "3.3 Multiplying predictors",
    "text": "3.3 Multiplying predictors\n\nPolynomials\n\n\nInteractions\n\n\nTransformation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression: Building more complex models</span>"
    ]
  },
  {
    "objectID": "RegressionCase.html",
    "href": "RegressionCase.html",
    "title": "4  Regression case study",
    "section": "",
    "text": "Exercise 1a new test",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression case study</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "5  Glossary",
    "section": "",
    "text": "Term  Definition",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#section",
    "href": "LinearModels.html#section",
    "title": "1  An introduction to linear statistical models",
    "section": "",
    "text": "Good work. Next we’ll add the second main component to our linear predictor to make it a linear statistical model",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  },
  {
    "objectID": "LinearModels.html#the-simple-linear-model",
    "href": "LinearModels.html#the-simple-linear-model",
    "title": "1  An introduction to linear statistical models",
    "section": "1.4 The Simple Linear Model",
    "text": "1.4 The Simple Linear Model\nPutting these pieces together we are left with:\n\\[\nY=\\beta_0+\\beta_1X+\\varepsilon, \\quad{\\varepsilon \\sim N(0,\\sigma^2)}\n\\]\nThis ‘simple linear model’ is the starting place for conducting linear regression - in which we ‘fit’ (i.e. estimate the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\)) from data.\n\n\nviewof b1_2 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β₁)\"})\nviewof b0_2 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (β₀)\"})\nviewof sigma_2 = Inputs.range([0.2, 5], {step: 0.1, value: 2.5, label: \"Std. deviation (σ)\"})\n\nviewof n_cs = Inputs.range([10, 50], {step: 1, value: 50, label: \"Number of cross sections visualised\"})\n\ntex.block`Y = ${b0_2} + ${b1_2}X + \\varepsilon, \\quad \\varepsilon \\sim \\text{Normal}(0, ${sigma_2}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxMin = -10;\nxMax = 10;\nstep = (xMax - xMin) / (n_cs - 1);\nxSampleValues = d3.range(xMin, xMax + step/2, step);  \n\nySectionValues = d3.range(-10, 10.001, 0.1)\nwidthScale = Math.min(1.8, sigma_2 * 0.9)\n\ndensityCurveData = xSampleValues.flatMap(xVal =&gt; {\n  const mu = b0_2 + b1_2 * xVal;\n  const peakDensity = normalDensity(mu, mu, sigma_2);\n\n  const rightSide = ySectionValues.map(y =&gt; {\n    const density = normalDensity(y, mu, sigma_2);\n    const width = (density / peakDensity) * widthScale;\n    return {x: xVal + width, y, group: xVal};\n  });\n\n  return rightSide\n});\n\ncrossSectionTrendLine = xSampleValues.map(x =&gt; ({\n  x,\n  y: b0_2 + b1_2 * x\n}))\n\nPlot.plot({\n  x: {domain: [-10, 10], label: \"X\", grid: true},\n  y: {domain: [-10, 10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(densityCurveData, {\n      x: \"x\",\n      y: \"y\",\n      z: \"group\",\n      stroke: \"#2a5599\",\n      strokeWidth: 1.5,\n      curve: \"basis\"\n    }),\n    Plot.line(crossSectionTrendLine, {x: \"x\", y: \"y\", stroke: \"black\", strokeWidth: 2})\n  ]\n});\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\nFigure 1.4: Cross-sections of the simple linear model normal error density.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to linear statistical models</span>"
    ]
  }
]